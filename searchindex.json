{"categories":[],"posts":[{"content":"git基础知识 .git目录探索 tree -L 1 .git/\n.git/ ├── COMMIT_EDITMSG — 最近一次commit的msg\n├── HEAD — 当前头指针\n├── ORIG_HEAD — 远端头指针\n├── config — 项目git配置\n├── description — GitWeb使用\n├── hooks — client或server钩子脚本\n├── index — 暂存区索引\n├── info — 保存项目全局需要排除的ignored patterns（很神奇的用法是：如果需要不在.gitignore文件中追踪，但需要忽略的文件，可以通过修改这个文件来实现）\n├── logs — 非初始化目录，记录一切修改HEAD的操作\n├── objects — 相当于git的数据库，使用sha-1校验和作为文件名，文件内容使用zip压缩\n└── refs — 维护分支和tag的引用关系\ngit中的对象(object: blob, tree, commit) 及sha-1校验和的由来 blob 项目目录下除了被忽略的文件和.git目录下的文件，其余的文件就是一个blob对象。\n➜ test-git git:(main) echo 'hello git' \u0026gt; hello ➜ test-git git:(main) ✗ git add hello ➜ test-git git:(main) ✗ find .git/objects -type f .git/objects/8d/0e41234f24b6da002d962a26c2495ea16a425f 可以看到，在git add 以后，文件即保存在git存储里。\n8d/0e41234f24b6da002d962a26c2495ea16a425f这一串sha-1是如何生成的呢？git暴露出了一个sub cli可以生成hash：\n➜ test-git git:(main) ✗ echo 'hello git' | git hash-object -w --stdin 8d0e41234f24b6da002d962a26c2495ea16a425f hash-object命令就是对输入的字符计算sha-1校验和，也可以在自己的电脑上用熟悉的语言打印一下是否如此（这个操作git有夹藏私货：需要格式化文件，如下）。\n我们强行查看一下这个对象文件里究竟有什么\n➜ test-git git:(main) ✗ cat .git/objects/8d/0e41234f24b6da002d962a26c2495ea16a425f xK��OR04`�H���WH�,�6A�% 不出所料是一串乱码。使用zlib-reader阅读一下里面究竟有什么\n➜ test-git git:(main) ✗ zlib-reader .git/objects/8d/0e41234f24b6da002d962a26c2495ea16a425f \u0026quot;blob 10\\x00hello git\\n\u0026quot; 查看资料可以抽象出文件内容的格式：\nx00\n使用git提供的方式格式化阅读：\n➜ test-git git:(main) git cat-file -p 8d0e41234f24b6da002d962a26c2495ea16a425f hello git ➜ test-git git:(main) git cat-file -t 8d0e41234f24b6da002d962a26c2495ea16a425f blob tree 刚才只是在项目顶级目录创建了文件hello，现在尝试建立子目录：\n➜ test-git git:(main) ✗ mkdir topic ➜ test-git git:(main) ✗ echo 'hello topic' \u0026gt; topic/hello-topic ➜ test-git git:(main) ✗ tree -L 2 . . ├── hello └── topic └── hello-topic 1 directory, 2 files ➜ test-git git:(main) ✗ git add topic/hello-topic ➜ test-git git:(main) ✗ git write-tree 313d4544c5e56d50d88efc84043df5b7fe6fef69 ➜ test-git git:(main) ✗ find .git/objects -type f .git/objects/e4/1f889718a3d72c8538078bd07ac77334bb32ee .git/objects/28/aeecdf471111ef079cbd8704f6db14f39a6a3a .git/objects/31/3d4544c5e56d50d88efc84043df5b7fe6fef69 .git/objects/8d/0e41234f24b6da002d962a26c2495ea16a425f 在未commit之前，是不会构建tree对象的，所以这里使用write-tree手动创建tree\n查看他的格式化内容：\n➜ test-git git:(main) ✗ git cat-file -p 313d4544c5e56d50d88efc84043df5b7fe6fef69 100644 blob 8d0e41234f24b6da002d962a26c2495ea16a425f\thello 040000 tree e41f889718a3d72c8538078bd07ac77334bb32ee\ttopic 抽象出内容的格式：\n其sha-1的计算也是对文件未压缩前的内容进行hash\n官网中的图例：\n如图所示，tree对象确实是一棵树，且叶子节点为具体的文件，叶子必不能是tree，这也是为什么git不能追踪空目录的原因。\ncommit 每一次的commit也是一个git object，属于commit类型，看看他有什么内容：\n➜ test-git git:(main) ✗ git commit -m 'first commit' [main (root-commit) fa4cc5d] first commit 2 files changed, 2 insertions(+) create mode 100644 hello create mode 100644 topic/hello-topic ➜ test-git git:(main) git cat-file -p fa4cc5d tree 313d4544c5e56d50d88efc84043df5b7fe6fef69 author yoogo \u0026lt;yoogoc@163.com\u0026gt; 1666081939 +0800 committer yoogo \u0026lt;yoogoc@163.com\u0026gt; 1666081939 +0800 first commit 抽象出文件内容的格式：\ntree author \u0026laquo;email\u0026raquo; committer \u0026laquo;email\u0026raquo; 其sha-1的计算也是对未压缩前的文件内容进行hash，因为有时间戳的关系，同样的changes，在不同的时间计算出的sha-1是不同的。\n我们看到tree 313d4544c5e56d50d88efc84043df5b7fe6fef69这就是树的根节点，官网图例：\n在这个图中，可以看到：\nfirst commit提交了test.txt文件\nsecond commit 提交了new.txt,并且修改了test.txt\nthird commit提交bak/test.txt，此次提交并未新增blob对象，因为new.txt，test.txt并未修改，所以得到的sha-1是相同的，提交的bak/test.txt内容与first commit的test.txt文件是相同的，所以计算出的sha-1也是相同的，所以都无需创建blob对象，只需构建tree即可。\n事实上，所有的git操作都是围绕着这三个对象，所以理解git object是很重要的。\n参考资料 https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain\nhttps://thoughtbot.com/blog/rebuilding-git-in-ruby\nhttp://gitlet.maryrosecook.com/docs/gitlet.html https://benhoyt.com/writings/pygit/\nhttps://github.com/go-git/go-git\n一些抖机灵的操作 git commit —-amend 背景：接到需求，编码，git提交，发现一个小问题 这时提交到git可能的方案：对这个小问题新建一个commit。 但这种貌似不是很优雅，这样会使整个git log非常难看，过了n天，自己也记不得当时为什么在极短的时间差提交了两个commit，一个feat: xxxx，一个fix: xxxxx。 这时可以使用标题的操作，会把当前的更改替换到头指针的commit，新的提交具有与当前提交有相同的parent和author，相当于git reset --soft HEAD^, do something, git commit -c ORIG_HEAD\ngit reflog 搭配 git reset 背景：在刚才操作的基础上，发现我的fix是多余的，只是脑子抽风而已 这个时候可能会git reset --soft HEAD^, revert recent changes, git commit -c ORIG_HEAD 这时可以用git reflog看看我究竟都对.git/HEAD做了什么，然后可以发现我的抽风发生在HEAD@{0},要是有一种办法帮我复原到HEAD@{1}就好了： git reset HEAD@{1}\ngit cat-file 上文已经用过很多次了，用来解析git sha-1值中的内容\ngit apply 在某些情况下，我不想commit，但是我还想与其他小伙伴分享我的代码，手动方式： 复制git diff的内容，保存并发送文件changes.diff，然后小伙伴逐行比对，手工变更， 这个时候就可以：git apply /changes.diff，会自动应用变更\n更多cli参见git —help -a,git —help -g\ngit commit建议 Atomic Commit 一个commit只做一件事（有点像数据库中事务的原子性）。例如，有两个不同的fix，应该提交两个commit，而不是fix a and fix b。为什么：\n倘若你的fix在本地有生效，然而在staging或者prod并没有生效，假如在两个fix在一个commit，你可能的解决办法是：注释掉其中一个fix再提交一个commit或者amend到上个commit。但无论怎样都是无法先上线其中一个fix的； 假如两个fix在分别的commit上，那我们就可以git cp一个正常的fix先上线，再回过头修另外一个。 （换成不同的场景也是类似）\n从而带来的好处：\n让团队的其他人或者n天后的自己能尽快熟悉代码 方便出错时回滚 Don\u0026rsquo;t Commit Half-Done Work 一件事在同一人的同一阶段不要分为多个commit，这样会使commit message不堪入目，而且具有迷惑性，不符合直觉。实际的影响可能会在合并上游代码发生冲突，发生在你的commit part 1，这时你并一定不能够解决问题，因为他是个半成品，可能无法debug。\nWrite Good Commit Messages 要写一个概括性且准确的message，一定要保证准确，可以牺牲概括性。一个message可以分为如下几个部分：\n变更的动机是什么？fix？feat？…… 与之前的实现有什么不同？ ","id":0,"section":"posts","summary":"git基础知识 .git目录探索 tree -L 1 .git/ .git/ ├── COMMIT_EDITMSG — 最近一次commit的msg ├── HEAD — 当前头指针 ├── ORIG_HEAD — 远端头指针 ├── config — 项目git配","tags":["git"],"title":"git基础知识","uri":"https://blog.yoogo.top/2022/10/git%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","year":"2022"},{"content":"本次只学习一下pod间的网络是怎么流通的，暂时不去管pod内多容器是如何共享localhost的。\n不同pod之间有两种形式进行访问：同一主机，不同主机。\n同一主机下的通讯还是很简单的，cni虚拟网卡就可以搞定了。这里着重讨论在不同主机下的场景。\n为什么会有pod间通讯的问题 在k8s中，以每个pod为一个原子，会获得一个唯一的ip地址，在同一台主机上，我们有很多种方法来实现不同pod间的通讯，例如虚拟网卡，iptables，ebpf等等。但是在不同主机上，我们就需要引入一个有状态的组件来维持网络，或者说用软件来定义网络（SDN）。\nexample分析 假设有虚拟机vm1，vm2，vm1中有pod1，pod2，vm2中有pod3，pod4：\npod1 -\u0026gt; pod2 \u0026mdash;\u0026mdash; 本地直接路由\npod1 -\u0026gt; pod3 \u0026mdash;\u0026mdash; 因为pod的ip地址都是集群内网地址，不能直接通过网卡发送数据包，所以一种经典的方式就是加一层额外的封包（客户端）解包（服务端）的操作，使得pod间丝滑无感的使用集群地址。\n这里采用flannel的vxlan模式进行分析。\nflanneld会watch所有k8s node，当有增加或者删除事件到达时，会程序触发增加/删除 flannel 对应的arp flannel0是一个vxlan虚拟设备，会自动封包/解包 集群网络的数据包 todo detail。。。。。\n","id":1,"section":"posts","summary":"本次只学习一下pod间的网络是怎么流通的，暂时不去管pod内多容器是如何共享localhost的。 不同pod之间有两种形式进行访问：同一主机","tags":["k8s","go","network"],"title":"k8s网络模型之pod间通讯","uri":"https://blog.yoogo.top/2022/07/k8s%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B9%8Bpod%E9%97%B4%E9%80%9A%E8%AE%AF/","year":"2022"},{"content":"operator模式 Operator 是 Kubernetes 的扩展软件，它利用 定制资源 管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制器 方面。\n官网介绍\n简单来说，operator是一种通过实现crd和相应的controller来达到拓展k8s的方法。\n实现及应用operator的大致流程：\n定义crd：可以理解为oop中定义一个类 实现controller：watch crd，对crd不同的生命周期做个性化处理，也就是实现operator核心的业务逻辑 部署controller：可以部署在集群内部，也可以在外部 定义crd实例：可以理解为oop中实例化一个类 frp frp 是一个专注于内网穿透的高性能的反向代理应用，支持 TCP、UDP、HTTP、HTTPS 等多种协议。可以将内网服务以安全、便捷的方式通过具有公网 IP 节点的中转暴露到公网。\nfrp在k8s中的应用 在正常情况下，外部访问k8s有几种方式：\nservice的lb和nodeport类型可以暴露服务 使用ingress暴露服务 对于个人项目或者其他某种场景，这个集群并不能直接被外部访问，也就是说没有公网ip的情况如何对外服务。首先frp是满足需求的，只需要有一台公网的云服务器。如果没有frpc-operator的情况，那可能需要如下步骤去穿透服务：\n部署frpc deployment 将frpc config置于configmap内 每一次暴露或调整新的节点时，就需要调整configmap，删除pod，才能更新完成。\n这一步可以进行优化，我们引入一个config-as-code边车容器： 他会监控指定的configmap，当调整配置以后，边车容器自动调用frpc的reload接口，实现不用删除pod即可更新。\n需要注意的是，此边车容器需要configmap的一些权限。\n为了学习operator，可以将这个操作集封装成为frpc-operator frpc-operator frpc-operator中定义了两种资源：\nclient 每一个client就是一个frpc deployment（事实上这里有点问题，如果多个副本会引起proxy重复，应该使用statefulset，将proxy平均分配至每个副本）。示例：\napiVersion: frpc.yoogo.top/v1 kind: Client metadata: name: client-sample spec: // 大致与frpc.ini中的配置差不多，只是支持的字段比较贫瘠 common: server_addr: 127.0.0.1 server_port: 7000 token: value: passwd Proxy apiVersion: frpc.yoogo.top/v1 kind: Proxy metadata: name: proxy-sample spec: client: client-sample // 必须指定client 名字 local_addr: 127.0.0.1 // 本地地址 local_port: \u0026quot;80\u0026quot; // 本地端口 tcp: remote_port: \u0026quot;20080\u0026quot; //服务端口 ","id":2,"section":"posts","summary":"operator模式 Operator 是 Kubernetes 的扩展软件，它利用 定制资源 管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制器 方面。 官网介绍 简单来说，operator","tags":["k8s","go"],"title":"初探k8s-operator及实现frpc-operator","uri":"https://blog.yoogo.top/2022/07/%E5%88%9D%E6%8E%A2k8s-operator/","year":"2022"},{"content":"ent runtime 通过简单的示例分析ent的运行时，目前只关心字段，暂不关心Edges\n创建schema，生成ent代码 package schema import ( \u0026quot;time\u0026quot; \u0026quot;entgo.io/ent\u0026quot; \u0026quot;entgo.io/ent/schema/field\u0026quot; ) // Product holds the schema definition for the Product entity. type Product struct { ent.Schema } // Fields of the Product. func (Product) Fields() []ent.Field { return []ent.Field{ field.Int64(\u0026quot;id\u0026quot;), field.String(\u0026quot;name\u0026quot;), field.String(\u0026quot;pic\u0026quot;), field.String(\u0026quot;code\u0026quot;), field.Bool(\u0026quot;is_publish\u0026quot;), field.Int32(\u0026quot;sort\u0026quot;), field.Time(\u0026quot;created_at\u0026quot;).Immutable().Default(time.Now), field.Time(\u0026quot;updated_at\u0026quot;).Default(time.Now).UpdateDefault(time.Now), } } // Edges of the Product. func (Product) Edges() []ent.Edge { return nil } 初始化driver ent 对database/sql包进行了一次封装，把database/sql#DB结构体抽象到了ExecQuerier接口中\n// entgo.io/ent/dialect/driver.go type ExecQuerier interface { ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error) QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error) } // Conn implements dialect.ExecQuerier given ExecQuerier. type Conn struct { ExecQuerier } // Driver is a dialect.Driver implementation for SQL based databases. // 这个结构体实际上是dialect.Driver的实现 type Driver struct { Conn // 这个字段是为了在buildersql的时候针对不同的数据库采用不同的语法或分隔符，比如pg的quote是\u0026quot;,而mysql的是` dialect string } func Open(driver, source string) (*Driver, error) { db, err := sql.Open(driver, source) if err != nil { return nil, err } return \u0026amp;Driver{Conn{db}, driver}, nil } 初始化client 调用ent/client.go:NewClient func NewClient(opts ...Option) *Client { // 创建配置对象 cfg := config{log: log.Println, hooks: \u0026amp;hooks{}} // 通过Option方式传入自定义参数 cfg.options(opts...) // 创建Client结构体 client := \u0026amp;Client{config: cfg} // 初始化Client client.init() return client } 整体流程还是非常简单的，接下来看下config都保存了什么内容\ntype config struct { // 执行db请求的驱动器 driver dialect.Driver // 打出debug日志 debug bool // debug模式使用的日志方法 log func(...interface{}) // 每个mutations执行时的hook hooks *hooks } type hooks struct { // product表执行时的hook Product []ent.Hook } 这个config内容不多，主要字段是driver和hook, 继续看下client结构体都保存了什么\ntype Client struct { // 配置 config // 用于迁移db的schema结构体 Schema *migrate.Schema // 与builders交互的client，我们这里只有一个ent schema，所以只有一个，下同 Product *ProductClient } client自身的初始化 func (c *Client) init() { // 根据驱动器生成用于db迁移的Schema结构体 c.Schema = migrate.NewSchema(c.driver) // 初始化ProductClient c.Product = NewProductClient(c.config) } // ent/migrate/migrate.go // 初始化Schema type Schema struct { drv dialect.Driver } func NewSchema(drv dialect.Driver) *Schema { return \u0026amp;Schema{drv: drv} } // 初始化ProductClient type ProductClient struct { config } func NewProductClient(c config) *ProductClient { return \u0026amp;ProductClient{config: c} } 就这样，client初始化已经完成\n事务的处理 ent使用client.Tx(ctx)或client.BeginTx(ctx, opts)开始一个事务，其区别在数BeginTx可以指定事务隔离级别\nfunc (c *Client) Tx(ctx context.Context) (*Tx, error) { if _, ok := c.driver.(*txDriver); ok { return nil, fmt.Errorf(\u0026quot;ent: cannot start a transaction within a transaction\u0026quot;) } tx, err := newTx(ctx, c.driver) if err != nil { return nil, fmt.Errorf(\u0026quot;ent: starting a transaction: %w\u0026quot;, err) } cfg := c.config cfg.driver = tx return \u0026amp;Tx{ ctx: ctx, config: cfg, Product: NewProductClient(cfg), }, nil } func (c *Client) BeginTx(ctx context.Context, opts *sql.TxOptions) (*Tx, error) { if _, ok := c.driver.(*txDriver); ok { return nil, fmt.Errorf(\u0026quot;ent: cannot start a transaction within a transaction\u0026quot;) } tx, err := c.driver.(interface { BeginTx(context.Context, *sql.TxOptions) (dialect.Tx, error) }).BeginTx(ctx, opts) if err != nil { return nil, fmt.Errorf(\u0026quot;ent: starting a transaction: %w\u0026quot;, err) } cfg := c.config cfg.driver = \u0026amp;txDriver{tx: tx, drv: c.driver} return \u0026amp;Tx{ ctx: ctx, config: cfg, Product: NewProductClient(cfg), }, nil } 可以看到，两种方式开启一个事务和初始化client流程几乎相同。\n通过ProductClient创建一个builder // Create returns a create builder for Product. func (c *ProductClient) Create() *ProductCreate { mutation := newProductMutation(c.config, OpCreate) return \u0026amp;ProductCreate{config: c.config, hooks: c.Hooks(), mutation: mutation} } // CreateBulk returns a builder for creating a bulk of Product entities. func (c *ProductClient) CreateBulk(builders ...*ProductCreate) *ProductCreateBulk { return \u0026amp;ProductCreateBulk{config: c.config, builders: builders} } // Update returns an update builder for Product. func (c *ProductClient) Update() *ProductUpdate { mutation := newProductMutation(c.config, OpUpdate) return \u0026amp;ProductUpdate{config: c.config, hooks: c.Hooks(), mutation: mutation} } // UpdateOne returns an update builder for the given entity. func (c *ProductClient) UpdateOne(pr *Product) *ProductUpdateOne { mutation := newProductMutation(c.config, OpUpdateOne, withProduct(pr)) return \u0026amp;ProductUpdateOne{config: c.config, hooks: c.Hooks(), mutation: mutation} } // UpdateOneID returns an update builder for the given id. func (c *ProductClient) UpdateOneID(id int64) *ProductUpdateOne { mutation := newProductMutation(c.config, OpUpdateOne, withProductID(id)) return \u0026amp;ProductUpdateOne{config: c.config, hooks: c.Hooks(), mutation: mutation} } // Delete returns a delete builder for Product. func (c *ProductClient) Delete() *ProductDelete { mutation := newProductMutation(c.config, OpDelete) return \u0026amp;ProductDelete{config: c.config, hooks: c.Hooks(), mutation: mutation} } // DeleteOne returns a delete builder for the given entity. func (c *ProductClient) DeleteOne(pr *Product) *ProductDeleteOne { return c.DeleteOneID(pr.ID) } // DeleteOneID returns a delete builder for the given id. func (c *ProductClient) DeleteOneID(id int64) *ProductDeleteOne { builder := c.Delete().Where(product.ID(id)) builder.mutation.id = \u0026amp;id builder.mutation.op = OpDeleteOne return \u0026amp;ProductDeleteOne{builder} } // Query returns a query builder for Product. func (c *ProductClient) Query() *ProductQuery { return \u0026amp;ProductQuery{ config: c.config, } } // Get returns a Product entity by its id. func (c *ProductClient) Get(ctx context.Context, id int64) (*Product, error) { return c.Query().Where(product.ID(id)).Only(ctx) } // GetX is like Get, but panics if an error occurs. func (c *ProductClient) GetX(ctx context.Context, id int64) *Product { obj, err := c.Get(ctx, id) if err != nil { panic(err) } return obj } product会有几种不同的builder，可以按照以下方式分类：\n是否批量：批量不会创建自己的mutation，但是会将传入的builder切片保存到自己的结构体里。 是否支持hook：增改删相关可以执行hook，而查询相关方法是不能带hook的。 基础builder和高级builder：我将（Create，Update，Delete，Query）成为基础builder，其余为高级builder。 是否会panic：方法后带X的会panic。 每一个基础builder都会生成一个通过newProductMutation创建的mutation，先看下mutation结构：\n// 可以把一个mutation分成几个部分来对待： type ProductMutation struct { // 1. 配置，与client相同 config // 2. 当前操作类型 op Op // 3. 节点类型（固定字符串Product） typ string // 4. 指针类型的field id *int64 name *string pic *string code *string is_publish *bool sort *int32 addsort *int32 created_at *time.Time updated_at *time.Time // 需要清空的字段 clearedFields map[string]struct{} // 与oldValue有关 done bool // 旧值 oldValue func(context.Context) (*Product, error) // 谓语，也可以说是where条件 predicates []predicate.Product } 现在再看ProductCreate这个builder，还是先看结构体：\ntype ProductCreate struct { config mutation *ProductMutation hooks []Hook } 结构体依然朴实无华，还是看见常用方法，我们在create一个实体通常有如下操作：\nclient.Product.Create().SetName(\u0026quot;product1\u0026quot;).Save(ctx) create builder使用SetXX()或方法来设置值\nfunc (pc *ProductCreate) SetName(s string) *ProductCreate { pc.mutation.SetName(s) return pc } ... func (pc *ProductCreate) SetCreatedAt(t time.Time) *ProductCreate { pc.mutation.SetCreatedAt(t) return pc } func (pc *ProductCreate) SetNillableCreatedAt(t *time.Time) *ProductCreate { if t != nil { pc.SetCreatedAt(*t) } return pc } 这里可能会有疑问：为什么有些字段会有Nillable setter，而有一些却没有，这里我们可以通过阅读源码中的模板得到(setter.tmpl):\n{{ if and (not $f.Type.Nillable) (or $f.Optional $f.Default) (not (and $updater $f.UpdateDefault)) }} Type.Nillable字段的意思大概可以理解为该实体字段是否是指针类型，那这个判断整体可以理解为：\n如果 字段不是指针类型，且（字段是不是必填 或 字段有默认值）且 不能是（更新操作中 该字段有更新默认值）\n更新操作的ProductUpdate ，ProductUpdateOne，ProductDelete和ProductDeleteOne中Setter方法与ProductCreate中类似，这里就不展开啰嗦了。\nbuilder构建完成，核心的执行方法 从ProductCreate Save开始：\nfunc (pc *ProductCreate) Save(ctx context.Context) (*Product, error) { var ( err error node *Product ) // 根据Schema中定义Default方法对未赋值的字段进行赋默认值 pc.defaults() // 根据有无hook将执行逻辑分为两个部分，但其最终都会调用sqlSave(ctx)方法 if len(pc.hooks) == 0 { // 对字段进行检查，每个字段可能会有两种检查，第一是对不是optional的字段检查是否已经赋值 // 第二种是对这个字段自定义的Validators进行检查 if err = pc.check(); err != nil { return nil, err } // 真正有价值的东西，后面单独分析 node, err = pc.sqlSave(ctx) } else { // 构造 Mutator ，使hook能通过slice形式进行递归调用，比如：Use(f, g, h)` 等于 `product.Hooks(f(g(h()))) var mut Mutator = MutateFunc(func(ctx context.Context, m Mutation) (Value, error) { mutation, ok := m.(*ProductMutation) if !ok { return nil, fmt.Errorf(\u0026quot;unexpected mutation type %T\u0026quot;, m) } if err = pc.check(); err != nil { return nil, err } pc.mutation = mutation if node, err = pc.sqlSave(ctx); err != nil { return nil, err } mutation.id = \u0026amp;node.ID mutation.done = true return node, err }) for i := len(pc.hooks) - 1; i \u0026gt;= 0; i-- { if pc.hooks[i] == nil { return nil, fmt.Errorf(\u0026quot;ent: uninitialized hook (forgotten import ent/runtime?)\u0026quot;) } mut = pc.hooks[i](mut) } if _, err := mut.Mutate(ctx, pc.mutation); err != nil { return nil, err } } return node, err } 执行sqlSave：\nfunc (pc *ProductCreate) sqlSave(ctx context.Context) (*Product, error) { // 构造Product对象和sqlgraph.CreateSpec _node, _spec := pc.createSpec() // 构造sql并执行 if err := sqlgraph.CreateNode(ctx, pc.driver, _spec); err != nil { if sqlgraph.IsConstraintError(err) { err = \u0026amp;ConstraintError{err.Error(), err} } return nil, err } // 更新id if _spec.ID.Value != _node.ID { id := _spec.ID.Value.(int64) _node.ID = int64(id) } return _node, nil } 构造Spec（流程依旧朴实无华，代码即注释，不啰嗦了）:\nfunc (pc *ProductCreate) createSpec() (*Product, *sqlgraph.CreateSpec) { var ( _node = \u0026amp;Product{config: pc.config} _spec = \u0026amp;sqlgraph.CreateSpec{ Table: product.Table, ID: \u0026amp;sqlgraph.FieldSpec{ Type: field.TypeInt64, Column: product.FieldID, }, } ) if id, ok := pc.mutation.ID(); ok { _node.ID = id _spec.ID.Value = id } if value, ok := pc.mutation.Name(); ok { _spec.Fields = append(_spec.Fields, \u0026amp;sqlgraph.FieldSpec{ Type: field.TypeString, Value: value, Column: product.FieldName, }) _node.Name = value } ... return _node, _spec } 万事俱备只欠东风 终于从codegen生产的代码跳到了ent包内\nfunc CreateNode(ctx context.Context, drv dialect.Driver, spec *CreateSpec) error { // 所有的builder都会用这个通用的graph来构造和执行sql gr := graph{tx: drv, builder: sql.Dialect(drv.Dialect())} // creator 是为了create操作抽象成的结构体 cr := \u0026amp;creator{CreateSpec: spec, graph: gr} // 构造和执行 return cr.node(ctx, drv) } 构造和执行,忽略了edges\nfunc (c *creator) node(ctx context.Context, drv dialect.Driver) error { var ( ... // 构造真正的InsertBuilder insert = c.builder.Insert(c.Table).Schema(c.Schema).Default() ) // 填充字段值 if err := c.setTableColumns(insert, edges); err != nil { return err } // 根据edges判断是否为多行sql，如果是则开启事务。 // 无论怎样都会把creator的tx替换，如果开启事务：则结构体为https://github.com/ent/ent/blob/cb6e0e063dbc88664a0377e7fde4796b593f9469/dialect/sql/driver.go#Tx // 如果不开启：则结构体为https://github.com/ent/ent/blob/cb6e0e063dbc88664a0377e7fde4796b593f9469/dialect/dialect.go#NopTx tx, err := c.mayTx(ctx, drv, edges) if err != nil { return err } if err := func() error { // 拼接和执行sql if err := c.insert(ctx, insert); err != nil { return err } ... }(); err != nil { return rollback(tx, err) } return tx.Commit() } 拼接和执行sql：\nfunc (c *creator) insert(ctx context.Context, insert *sql.InsertBuilder) error { if opts := c.CreateSpec.OnConflict; len(opts) \u0026gt; 0 { insert.OnConflict(opts...) c.ensureLastInsertID(insert) } // 如果外部提供id值 if c.ID.Value != nil { insert.Set(c.ID.Column, c.ID.Value) // In case of \u0026quot;ON CONFLICT\u0026quot;, the record may exists in the // database, and we need to get back the database id field. if len(c.CreateSpec.OnConflict) == 0 { // 拼接sql，构造参数数组，这个query方法只是不停地拼字符串，就不展开啰嗦了 query, args := insert.Query() // 执行sql，这个是各种驱动器实现的，就不深入分析了 return c.tx.Exec(ctx, query, args, nil) } } return c.insertLastID(ctx, insert.Returning(c.ID.Column)) } 不提供id值的情况，需要从数据库取回id并赋值回CreateSpec\n","id":3,"section":"posts","summary":"ent runtime 通过简单的示例分析ent的运行时，目前只关心字段，暂不关心Edges 创建schema，生成ent代码 package schema import ( \u0026quot;time\u0026quot; \u0026quot;entgo.io/ent\u0026quot; \u0026quot;entgo.io/ent/schema/field\u0026quot; ) // Product holds the schema definition for the Product entity.","tags":["go"],"title":"ent分析-runtime-create","uri":"https://blog.yoogo.top/2022/04/ent-runtime-create/","year":"2022"},{"content":" 代码基于commit:d199a7c26797e29ffbd5651673301bc936f99029\ninit-初始化schema 目的 生成数据表/go entity 的schema文件，用于定义表结构/实体模型\n源码分析 cmd入口在cmd/internal/base/base.go:66 func InitCmd() *cobra.Command { var target string cmd := \u0026amp;cobra.Command{ Use: \u0026quot;init [flags] [schemas]\u0026quot;, Short: \u0026quot;initialize an environment with zero or more schemas\u0026quot;, Example: examples( \u0026quot;ent init Example\u0026quot;, \u0026quot;ent init --target entv1/schema User Group\u0026quot;, ), // 命令行参数校验是否开始于大写 Args: func(_ *cobra.Command, names []string) error { for _, name := range names { if !unicode.IsUpper(rune(name[0])) { return errors.New(\u0026quot;schema names must begin with uppercase\u0026quot;) } } return nil }, // 命令入口, 支持生成多schema Run: func(cmd *cobra.Command, names []string) { if err := initEnv(target, names); err != nil { log.Fatalln(fmt.Errorf(\u0026quot;ent/init: %w\u0026quot;, err)) } }, } // 生成路径默认当前路径下ent/schema目录 cmd.Flags().StringVar(\u0026amp;target, \u0026quot;target\u0026quot;, defaultSchema, \u0026quot;target directory for schemas\u0026quot;) return cmd } 执行入口cmd/internal/base/base.go:182 func initEnv(target string, names []string) error { // 创建目录 if err := createDir(target); err != nil { return fmt.Errorf(\u0026quot;create dir %s: %w\u0026quot;, target, err) } for _, name := range names { // 校验name if err := gen.ValidSchemaName(name); err != nil { return fmt.Errorf(\u0026quot;init schema %s: %w\u0026quot;, name, err) } b := bytes.NewBuffer(nil) // 模板渲染 if err := tmpl.Execute(b, name); err != nil { return fmt.Errorf(\u0026quot;executing template %s: %w\u0026quot;, name, err) } // 写入目标目录 newFileTarget := filepath.Join(target, strings.ToLower(name+\u0026quot;.go\u0026quot;)) if err := os.WriteFile(newFileTarget, b.Bytes(), 0644); err != nil { return fmt.Errorf(\u0026quot;writing file %s: %w\u0026quot;, newFileTarget, err) } } return nil } 校验name：\nfunc ValidSchemaName(name string) error { // Schema package is lower-cased (see Type.Package). pkg := strings.ToLower(name) // 是否为go关键字 if token.Lookup(pkg).IsKeyword() { return fmt.Errorf(\u0026quot;schema lowercase name conflicts with Go keyword %q\u0026quot;, pkg) } // 是否为go原生类型 if types.Universe.Lookup(pkg) != nil { return fmt.Errorf(\u0026quot;schema lowercase name conflicts with Go predeclared identifier %q\u0026quot;, pkg) } // 是否为ent关键字 if _, ok := globalIdent[pkg]; ok { return fmt.Errorf(\u0026quot;schema lowercase name conflicts ent predeclared identifier %q\u0026quot;, pkg) } if _, ok := globalIdent[name]; ok { return fmt.Errorf(\u0026quot;schema name conflicts with ent predeclared identifier %q\u0026quot;, name) } return nil } generate-根据schema生成go code 目的 根据schema生成predicate，crud及ent基础代码（如context、config、runtime、mutation、client等）\n源码分析 cmd入口：cmd/internal/base/base.go:117 func GenerateCmd(postRun ...func(*gen.Config)) *cobra.Command { var ( // Header: codegen头部信息 // Target: 目标目录 cfg gen.Config // 存储驱动:默认sql,可选gremlin storage string // additional features features []string // 外部模板,支持的格式:dir=xxx,file=xxx,glob=xxx templates []string // 代码生成的id类型,默认int idtype = IDType(field.TypeInt) cmd = \u0026amp;cobra.Command{ Use: \u0026quot;generate [flags] path\u0026quot;, Short: \u0026quot;generate go code for the schema directory\u0026quot;, Example: examples( \u0026quot;ent generate ./ent/schema\u0026quot;, \u0026quot;ent generate github.com/a8m/x\u0026quot;, ), Args: cobra.ExactArgs(1), Run: func(cmd *cobra.Command, path []string) { // 加载驱动和features opts := []entc.Option{ entc.Storage(storage), entc.FeatureNames(features...), } // 将模板映射为Option结构体 for _, tmpl := range templates { typ := \u0026quot;dir\u0026quot; if parts := strings.SplitN(tmpl, \u0026quot;=\u0026quot;, 2); len(parts) \u0026gt; 1 { typ, tmpl = parts[0], parts[1] } switch typ { case \u0026quot;dir\u0026quot;: opts = append(opts, entc.TemplateDir(tmpl)) case \u0026quot;file\u0026quot;: opts = append(opts, entc.TemplateFiles(tmpl)) case \u0026quot;glob\u0026quot;: opts = append(opts, entc.TemplateGlob(tmpl)) default: log.Fatalln(\u0026quot;unsupported template type\u0026quot;, typ) } } // If the target directory is not inferred from // the schema path, resolve its package path. // 如果目标目录不在schema目录,需要找到一个合适的包路径 if cfg.Target != \u0026quot;\u0026quot; { pkgPath, err := PkgPath(DefaultConfig, cfg.Target) if err != nil { log.Fatalln(err) } cfg.Package = pkgPath } cfg.IDType = \u0026amp;field.TypeInfo{Type: field.Type(idtype)} // 执行gen if err := entc.Generate(path[0], \u0026amp;cfg, opts...); err != nil { log.Fatalln(err) } // 生成后的hook for _, fn := range postRun { fn(\u0026amp;cfg) } }, } ) cmd.Flags().Var(\u0026amp;idtype, \u0026quot;idtype\u0026quot;, \u0026quot;type of the id field\u0026quot;) cmd.Flags().StringVar(\u0026amp;storage, \u0026quot;storage\u0026quot;, \u0026quot;sql\u0026quot;, \u0026quot;storage driver to support in codegen\u0026quot;) cmd.Flags().StringVar(\u0026amp;cfg.Header, \u0026quot;header\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;override codegen header\u0026quot;) cmd.Flags().StringVar(\u0026amp;cfg.Target, \u0026quot;target\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;target directory for codegen\u0026quot;) cmd.Flags().StringSliceVarP(\u0026amp;features, \u0026quot;feature\u0026quot;, \u0026quot;\u0026quot;, nil, \u0026quot;extend codegen with additional features\u0026quot;) cmd.Flags().StringSliceVarP(\u0026amp;templates, \u0026quot;template\u0026quot;, \u0026quot;\u0026quot;, nil, \u0026quot;external templates to execute\u0026quot;) return cmd } 执行入口entc/entc.go:53： func Generate(schemaPath string, cfg *gen.Config, options ...Option) (err error) { // 默认输出目录为schema同级 if cfg.Target == \u0026quot;\u0026quot; { abs, err := filepath.Abs(schemaPath) if err != nil { return err } // default target-path for codegen is one dir above // the schema. cfg.Target = filepath.Dir(abs) } for _, opt := range options { if err := opt(cfg); err != nil { return err } } // 默认存储为sql if cfg.Storage == nil { driver, err := gen.NewStorage(\u0026quot;sql\u0026quot;) if err != nil { return err } cfg.Storage = driver } // 确保包路径不会循环依赖 undo, err := gen.PrepareEnv(cfg) if err != nil { return err } defer func() { if err != nil { _ = undo() } }() // 准备就绪,进入生成器方法 return generate(schemaPath, cfg) } 生成入口entc/entc.go:284： func generate(schemaPath string, cfg *gen.Config) error { // 构造Graph graph, err := LoadGraph(schemaPath, cfg) if err != nil { // 如果开启了快照feature,则尝试恢复 if err := mayRecover(err, schemaPath, cfg); err != nil { return err } // 尝试重新构造Graph if graph, err = LoadGraph(schemaPath, cfg); err != nil { return err } } // 格式化包名('-' =\u0026gt; '_') if err := normalizePkg(cfg); err != nil { return err } return graph.Gen() } 构造Graphentc/entc.go:27: func LoadGraph(schemaPath string, cfg *gen.Config) (*gen.Graph, error) { // 加载映射后的schema结构体 spec, err := (\u0026amp;load.Config{Path: schemaPath}).Load() if err != nil { return nil, err } cfg.Schema = spec.PkgPath if cfg.Package == \u0026quot;\u0026quot; { // default package-path for codegen is one package // before the schema package (`\u0026lt;project\u0026gt;/ent/schema`). // 默认包名 cfg.Package = path.Dir(spec.PkgPath) } // 从schema中构造Graph return gen.NewGraph(cfg, spec.Schemas...) } 从schema路径加载Schema结构体entc/load/load.go:49： func (c *Config) Load() (*SchemaSpec, error) { // Config初始化只有Path,这里的load()方法会加载出所有schema的name, // 并返回schema包路径,用于buildTmpl渲染import // load()方法使用了很多的ast知识，暂时忽略细节 pkgPath, err := c.load() if err != nil { return nil, fmt.Errorf(\u0026quot;entc/load: load schema dir: %w\u0026quot;, err) } if len(c.Names) == 0 { return nil, fmt.Errorf(\u0026quot;entc/load: no schema found in: %s\u0026quot;, c.Path) } var b bytes.Buffer // 构造可执行文件 err = buildTmpl.ExecuteTemplate(\u0026amp;b, \u0026quot;main\u0026quot;, struct { *Config Package string }{c, pkgPath}) if err != nil { return nil, fmt.Errorf(\u0026quot;entc/load: execute template: %w\u0026quot;, err) } buf, err := format.Source(b.Bytes()) if err != nil { return nil, fmt.Errorf(\u0026quot;entc/load: format template: %w\u0026quot;, err) } if err := os.MkdirAll(\u0026quot;.entc\u0026quot;, os.ModePerm); err != nil { return nil, err } target := fmt.Sprintf(\u0026quot;.entc/%s.go\u0026quot;, filename(pkgPath)) if err := os.WriteFile(target, buf, 0644); err != nil { return nil, fmt.Errorf(\u0026quot;entc/load: write file %s: %w\u0026quot;, target, err) } defer os.RemoveAll(\u0026quot;.entc\u0026quot;) // 执行映射好的main文件,用于将schema下的schema definition文件序列化为[]byte,输出到std, // 然后entc主进程收集main文件(target)的stdout，这里依赖go环境 out, err := run(target) if err != nil { return nil, err } spec := \u0026amp;SchemaSpec{PkgPath: pkgPath} // 将schema反序列化至spec结构体 for _, line := range strings.Split(out, \u0026quot;\\n\u0026quot;) { schema, err := UnmarshalSchema([]byte(line)) if err != nil { return nil, fmt.Errorf(\u0026quot;entc/load: unmarshal schema %s: %w\u0026quot;, line, err) } spec.Schemas = append(spec.Schemas, schema) } return spec, nil } 从Schema中构造Graphentc/gen/graph.go:126: func NewGraph(c *Config, schemas ...*load.Schema) (g *Graph, err error) { defer catch(\u0026amp;err) g = \u0026amp;Graph{c, make([]*Type, 0, len(schemas)), schemas} // 构造节点 for i := range schemas { g.addNode(schemas[i]) } // 构造边,并添加到相应node for i := range schemas { g.addEdges(schemas[i]) } // 解析关联 for _, t := range g.Nodes { check(resolve(t), \u0026quot;resolve %q relations\u0026quot;, t.Name) } for _, t := range g.Nodes { check(t.setupFKs(), \u0026quot;set %q foreign-keys\u0026quot;, t.Name) } // 构造索引 for i := range schemas { g.addIndexes(schemas[i]) } // 指定graph默认值,当前只有id type g.defaults() return } 6.1. 构造节点：\n// entc/gen/graph.go:126 func (g *Graph) addNode(schema *load.Schema) { // schema翻译为Type/Node/Ent t, err := NewType(g.Config, schema) check(err, \u0026quot;create type %s\u0026quot;, schema.Name) g.Nodes = append(g.Nodes, t) } // entc/gen/type.go:191 func NewType(c *Config, schema *load.Schema) (*Type, error) { idType := c.IDType if idType == nil { idType = defaultIDType } typ := \u0026amp;Type{ Config: c, ID: \u0026amp;Field{ cfg: c, Name: \u0026quot;id\u0026quot;, def: \u0026amp;load.Field{ Name: \u0026quot;id\u0026quot;, }, Type: idType, StructTag: structTag(\u0026quot;id\u0026quot;, \u0026quot;\u0026quot;), }, schema: schema, Name: schema.Name, Annotations: schema.Annotations, Fields: make([]*Field, 0, len(schema.Fields)), fields: make(map[string]*Field, len(schema.Fields)), foreignKeys: make(map[string]struct{}), } if err := ValidSchemaName(typ.Name); err != nil { return nil, err } for _, f := range schema.Fields { tf := \u0026amp;Field{ cfg: c, def: f, Name: f.Name, Type: f.Info, Unique: f.Unique, Position: f.Position, Nillable: f.Nillable, Optional: f.Optional, Default: f.Default, UpdateDefault: f.UpdateDefault, Immutable: f.Immutable, StructTag: structTag(f.Name, f.Tag), Validators: f.Validators, UserDefined: true, Annotations: f.Annotations, } if err := typ.checkField(tf, f); err != nil { return nil, err } // User defined id field. // 如果从schema中单独定义id字段，那么则id字段取用户自定义的 if tf.Name == typ.ID.Name { typ.ID = tf } else { typ.Fields = append(typ.Fields, tf) typ.fields[f.Name] = tf } } return typ, nil } 6.2. 构造边,并添加到相应nodeentc/gen/graph.go:259:\nfunc (g *Graph) addEdges(schema *load.Schema) { // 根据schema获取Type结构体 t, _ := g.typ(schema.Name) seen := make(map[string]struct{}, len(schema.Edges)) for _, e := range schema.Edges { typ, ok := g.typ(e.Type) // edge的type必须在graph的nodes里 expect(ok, \u0026quot;type %q does not exist for edge\u0026quot;, e.Type) _, ok = t.fields[e.Name] // edge的名字不能与字段名相同 expect(!ok, \u0026quot;%s schema can't contain field and edge with the same name %q\u0026quot;, schema.Name, e.Name) _, ok = seen[e.Name] // 不能定义多个名字一样的edge expect(!ok, \u0026quot;%s schema contains multiple %q edges\u0026quot;, schema.Name, e.Name) seen[e.Name] = struct{}{} switch { // Assoc only. // 正向关联,比如: edge.To(\u0026quot;card\u0026quot;, Card.Type).Unique() case !e.Inverse: t.Edges = append(t.Edges, \u0026amp;Edge{ def: e, Type: typ, Name: e.Name, Owner: t, Unique: e.Unique, Optional: !e.Required, StructTag: structTag(e.Name, e.Tag), Annotations: e.Annotations, }) // Inverse only. // 反向关联,比如: edge.From(\u0026quot;owner\u0026quot;, User.Type).Ref(\u0026quot;card\u0026quot;) case e.Inverse \u0026amp;\u0026amp; e.Ref == nil: // 必须要指定RefName expect(e.RefName != \u0026quot;\u0026quot;, \u0026quot;missing reference name for inverse edge: %s.%s\u0026quot;, t.Name, e.Name) t.Edges = append(t.Edges, \u0026amp;Edge{ def: e, Type: typ, Name: e.Name, Owner: typ, Inverse: e.RefName, Unique: e.Unique, Optional: !e.Required, StructTag: structTag(e.Name, e.Tag), Annotations: e.Annotations, }) // Inverse and assoc. // 仅同类型的edge会进入这里,比如: // edge.To(\u0026quot;following\u0026quot;, User.Type).From(\u0026quot;followers\u0026quot;) case e.Inverse: // e.Ref 指正向边 ref := e.Ref // 必须不指定Ref名字 expect(e.RefName == \u0026quot;\u0026quot;, \u0026quot;reference name is derived from the assoc name: %s.%s \u0026lt;-\u0026gt; %s.%s\u0026quot;, t.Name, ref.Name, t.Name, e.Name) // 必须同类型 expect(ref.Type == t.Name, \u0026quot;assoc-inverse edge allowed only as o2o relation of the same type\u0026quot;) // 因为是用一条语句表达了正反两条边,所以需要把两条边都加入切片 t.Edges = append(t.Edges, \u0026amp;Edge{ def: e, Type: typ, Name: e.Name, Owner: t, Inverse: ref.Name, Unique: e.Unique, Optional: !e.Required, StructTag: structTag(e.Name, e.Tag), Annotations: e.Annotations, }, \u0026amp;Edge{ def: ref, Type: typ, Owner: t, Name: ref.Name, Unique: ref.Unique, Optional: !ref.Required, StructTag: structTag(ref.Name, ref.Tag), Annotations: ref.Annotations, }) default: panic(graphError{\u0026quot;edge must be either an assoc or inverse edge\u0026quot;}) } } } 6.3 解析关联entc/gen/graph.go:365：\nfunc resolve(t *Type) error { for _, e := range t.Edges { switch { // 反向边 case e.IsInverse(): // 必须从e的node下的edges中找到相反的edge结构体 ref, ok := e.Type.HasAssoc(e.Inverse) if !ok { return fmt.Errorf(\u0026quot;edge %q is missing for inverse edge: %s.%s(%s)\u0026quot;, e.Inverse, t.Name, e.Name, e.Type.Name) } // 两条边不能同时设为required if !e.Optional \u0026amp;\u0026amp; !ref.Optional { return fmt.Errorf(\u0026quot;edges cannot be required in both directions: %s.%s \u0026lt;-\u0026gt; %s.%s\u0026quot;, t.Name, e.Name, e.Type.Name, ref.Name) } // 检查类型一致性 if ref.Type != t { return fmt.Errorf(\u0026quot;mismatch type for back-ref %q of %s.%s \u0026lt;-\u0026gt; %s.%s\u0026quot;, e.Inverse, t.Name, e.Name, e.Type.Name, ref.Name) } // 设置edge的ref e.Ref, ref.Ref = ref, e // table 指的是需要加外键的表名 table := t.Table() // Name the foreign-key column in a format that wouldn't change even if an inverse // edge is dropped (or added). The format is: \u0026quot;\u0026lt;Edge-Owner\u0026gt;_\u0026lt;Edge-Name\u0026gt;\u0026quot;. // 以固定格式命名,即时反向也不会变 column := fmt.Sprintf(\u0026quot;%s_%s\u0026quot;, e.Type.Label(), snake(ref.Name)) // 确定关联类型 switch a, b := ref.Unique, e.Unique; { // If the relation column is in the inverse side/table. The rule is simple, if assoc is O2M, // then inverse is M2O and the relation is in its table. case a \u0026amp;\u0026amp; b: e.Rel.Type, ref.Rel.Type = O2O, O2O case !a \u0026amp;\u0026amp; b: e.Rel.Type, ref.Rel.Type = M2O, O2M // If the relation column is in the assoc side. case a \u0026amp;\u0026amp; !b: e.Rel.Type, ref.Rel.Type = O2M, M2O table = e.Type.Table() case !a \u0026amp;\u0026amp; !b: e.Rel.Type, ref.Rel.Type = M2M, M2M table = e.Type.Label() + \u0026quot;_\u0026quot; + ref.Name c1, c2 := ref.Owner.Label()+\u0026quot;_id\u0026quot;, ref.Type.Label()+\u0026quot;_id\u0026quot; // If the relation is from the same type: User has Friends ([]User). // give the second column a different name (the relation name). if c1 == c2 { c2 = rules.Singularize(e.Name) + \u0026quot;_id\u0026quot; } e.Rel.Columns = []string{c1, c2} ref.Rel.Columns = []string{c1, c2} } e.Rel.Table, ref.Rel.Table = table, table if !e.M2M() { e.Rel.Columns = []string{column} ref.Rel.Columns = []string{column} } // Assoc with uninitialized relation. case !e.IsInverse() \u0026amp;\u0026amp; e.Rel.Type == Unk: switch { case !e.Unique \u0026amp;\u0026amp; e.Type == t: e.Rel.Type = M2M e.Bidi = true e.Rel.Table = t.Label() + \u0026quot;_\u0026quot; + e.Name e.Rel.Columns = []string{e.Owner.Label() + \u0026quot;_id\u0026quot;, rules.Singularize(e.Name) + \u0026quot;_id\u0026quot;} case e.Unique \u0026amp;\u0026amp; e.Type == t: e.Rel.Type = O2O e.Bidi = true e.Rel.Table = t.Table() case e.Unique: e.Rel.Type = M2O e.Rel.Table = t.Table() default: e.Rel.Type = O2M e.Rel.Table = e.Type.Table() } if !e.M2M() { e.Rel.Columns = []string{fmt.Sprintf(\u0026quot;%s_%s\u0026quot;, t.Label(), snake(e.Name))} } } } return nil } 根据Graph生成真正文件entc/gen/graph.go:179： func (g *Graph) Gen() error { // 构造生成器 var gen Generator = GenerateFunc(generate) // 叠加hook for i := len(g.Hooks) - 1; i \u0026gt;= 0; i-- { gen = g.Hooks[i](gen) } return gen.Generate(g) } // generate is the default Generator implementation. func generate(g *Graph) error { var ( assets assets external []GraphTemplate ) // 获取root templates和全局拓展模板 templates, external = g.templates() // 从node开始遍历 for _, n := range g.Nodes { assets.dirs = append(assets.dirs, filepath.Join(g.Config.Target, n.Package())) // 逐个模板生成 for _, tmpl := range Templates { b := bytes.NewBuffer(nil) if err := templates.ExecuteTemplate(b, tmpl.Name, n); err != nil { return fmt.Errorf(\u0026quot;execute template %q: %w\u0026quot;, tmpl.Name, err) } assets.files = append(assets.files, file{ path: filepath.Join(g.Config.Target, tmpl.Format(n)), content: b.Bytes(), }) } } // 全局模板和全局拓展模板生成 for _, tmpl := range append(GraphTemplates, external...) { if tmpl.Skip != nil \u0026amp;\u0026amp; tmpl.Skip(g) { continue } if dir := filepath.Dir(tmpl.Format); dir != \u0026quot;.\u0026quot; { assets.dirs = append(assets.dirs, filepath.Join(g.Config.Target, dir)) } b := bytes.NewBuffer(nil) if err := templates.ExecuteTemplate(b, tmpl.Name, g); err != nil { return fmt.Errorf(\u0026quot;execute template %q: %w\u0026quot;, tmpl.Name, err) } assets.files = append(assets.files, file{ path: filepath.Join(g.Config.Target, tmpl.Format), content: b.Bytes(), }) } // 删掉不需要的Features相关文件 for _, f := range AllFeatures { if f.cleanup == nil || g.featureEnabled(f) { continue } if err := f.cleanup(g.Config); err != nil { return fmt.Errorf(\u0026quot;cleanup %q feature assets: %w\u0026quot;, f.Name, err) } } // Write and format assets only if template execution // finished successfully. if err := assets.write(); err != nil { return err } // We can't run \u0026quot;imports\u0026quot; on files when the state is not completed. // Because, \u0026quot;goimports\u0026quot; will drop undefined package. Therefore, it's // suspended to the end of the writing. return assets.format() } 至此，entc完成了代码生成的全部工作，由此可以看出，entc并未真正有关于orm的操作，所以可以把entc称为ent前端。entc generate的流程大致可以有如下表示：\n将我们写好的schema转化为entc所需要的graph结构 确定此次生成器需要的模板 逐个生成orm文件 ","id":4,"section":"posts","summary":"代码基于commit:d199a7c26797e29ffbd5651673301bc936f99029 init-初始化schema 目的 生成数","tags":["go"],"title":"ent分析-ent codegen","uri":"https://blog.yoogo.top/2021/10/ent%E5%88%86%E6%9E%90/","year":"2021"},{"content":"ingress是如何进行工作的 从官网盗图：\ngraph LR; client([1.客户端])-. 2.Ingress-管理的 \u0026lt;br\u0026gt;负载均衡器 .-\u0026gt;ingress[3.Ingress]; ingress--\u0026gt;|4.路由规则|service[5.Service]; subgraph cluster ingress; service--\u0026gt;pod1[6.1.Pod]; service--\u0026gt;pod2[6.2.Pod]; end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#1a2433,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#1a2433; class ingress,service,pod1,pod2 k8s; class client plain; class cluster cluster; 解析 假设客户端需要通过访问http://example.com来访问位于 pod1的资源\n客户端键入http://example.com,dns服务器解析出ip地址59.41.164.65 通过L4层的负载均衡将流量分发至ingress controller，即图中2，3项 ingress controller解析路由规则，转发至期望的service service将请求负载至具体pod 说明 与作为 kube-controller-manager 可执行文件的一部分运行的其他类型的控制器不同， Ingress 控制器不是随集群自动启动的。\ningress-nginx内部是如何工作的 graph LR; subgraph 限流queue; worker([syncIngress]) end worker([syncIngress])-. 获取所有ingress并\u0026lt;br/\u0026gt;筛选及排序 .-\u0026gt;gen[生成nginx.conf]; gen -. reload .-\u0026gt; nginx classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#1a2433; classDef queue fill:#fff,stroke:#bbb,stroke-width:2px,color:#1a2433; class worker queue; 想要做的事情 ingress-nginx-controller的目的是组装一个配置文件（nginx.conf），在配置文件发生任何更改后需要重新加载 NGINX\n如何监控数据变化 k8s controller 使用synchronization loop pattern来检查controller中状态是否已更新或是否需要更新。\nInformer/SharedInformer 监视 ingress当前状态的变化，并将事件发送到queue，然后由worker弹出事件进行处理(syncIngress()方法)。\n怎样去更新数据变化 由于没办法知道一个特定（Ingresses, Services, Endpoints, Secrets, and Configmaps）的变化是否会影响到最终的配置文件。\n每次更改时，我们都必须根据集群的状态从头开始重建新模型，并将其与当前模型进行比较。如果新模型与当前模型相同，那么我们将避免生成新的 nginx 配置并触发重新加载。\n如果diff仅与Endpoints相关，使用 HTTP POST 请求将新的Endpoints list发送到在 Nginx 内运行的 Lua 处理程序，并再次避免生成新的 nginx 配置并触发重新加载。\n如果diff不止Endpoints，那我们就要重新组装nginx配置了。\n如何组装nginx配置 构建nginx配置是极其耗时的操作，所以使用同步循环，通过syncQueue，既可以不丢失change，又可以摆脱sync.Mutex噩梦，来强制同步循环的单次执行。同步循环的开始和结束之间创建一个时间窗口，允许丢弃不必要的更新。\n构建模型的操作：\n按CreationTimestamp字段对Ingress 规则进行排序，即旧规则在前。 如果在多个 Ingress 中定义了同一主机的相同路径，则最旧的规则获胜。 如果多个 Ingress 包含同一主机的 TLS 部分，则最旧的规则获胜。 如果多个 Ingress 定义了影响 Server 块配置的注释，则最旧的规则获胜。 创建 NGINX 服务器列表（每个主机名） 创建 NGINX 上游列表 如果多个 Ingress 为同一主机定义了不同的路径，则 Ingress 控制器将合并这些定义。 注释应用于 Ingress 中的所有路径。 多个 Ingress 可以定义不同的注释。这些定义在 Ingress 之间不共享。 需要重新加载的场景 创建新的Ingress。 现有 Ingress添加TLS 。 Ingress 注释的变化不仅仅影响上游配置。例如load-balance注释不需要重新加载。 从 Ingress 添加/删除path。 删除Ingress, Service, Secret。 一些来自 Ingress 的缺失引用对象可用，例如 Service 或 Secret。 更新了一个 Secret。 ","id":5,"section":"posts","summary":"ingress是如何进行工作的 从官网盗图： graph LR; client([1.客户端])-. 2.Ingress-管理的 \u0026lt;br\u0026gt;负载均衡器 .-\u0026gt;ingress[3.Ingress]; i","tags":["k8s"],"title":"ingress-nginx是如何工作的","uri":"https://blog.yoogo.top/2021/06/ingress-nginx%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/","year":"2021"},{"content":"为什么要用coredns，而不是kube-dns https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/\n官网的这篇文章给出了很详细的理由，而且k8s官方也推荐用\nk8s基于dns的服务发现 k8s dns 规范\nhttps://github.com/kubernetes/dns/blob/master/docs/specification.md\n将CoreDNS的service的clusterIP设置到kubelet-config中的clusterDNS，这样，在每个pod启动的时候，kubelet会自动的生成容器的/etc/resolv.conf, 例如：\nnameserver 169.254.20.10 search istio-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5 解释：\nnameserver：就是coredns的service的clusterIP search： 用来补全hostname，假设有名为kiali的service，则在当前命名空间下，可以通过kiali访问到服务，其他命名空间可以通过kiali.istio-system访问 options：如果查询的域名包含的点“.”，不到5个，那么进行DNS查找，将使用非完全限定名称（或者叫绝对域名），如果你查询的域名包含点数大于等于5，那么DNS查询，默认会使用绝对域名进行查询 CoreDNS基于插件的服务 一个简单的Corefile：\n.:54 { log errors kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 endpoint http://localhost:8001 } } 解释：启动一个端口为54的dns服务，解析所有host，并启动插件：log，errors，kubernetes\nk8s插件的dsl参照官网\nhttps://coredns.io/plugins/kubernetes/\nCoreDNS kubernetes插件处理流程 1. 入口 plugin/kubernetes/handler.go\nServeDNS方法为插件入口方法，经过具体处理后，将应答消息写入dns.ResponseWriter，然后返回成功\n2. 执行步骤 ServeDNS中，构造state后执行按类型执行响应的解析方法 switch state.QType() { case dns.TypeA: records, err = plugin.A(ctx, \u0026amp;k, zone, state, nil, plugin.Options{}) case dns.TypeAAAA: records, err = plugin.AAAA(ctx, \u0026amp;k, zone, state, nil, plugin.Options{}) case dns.TypeTXT: records, err = plugin.TXT(ctx, \u0026amp;k, zone, state, nil, plugin.Options{}) case dns.TypeCNAME: records, err = plugin.CNAME(ctx, \u0026amp;k, zone, state, plugin.Options{}) case dns.TypePTR: records, err = plugin.PTR(ctx, \u0026amp;k, zone, state, plugin.Options{}) case dns.TypeMX: records, extra, err = plugin.MX(ctx, \u0026amp;k, zone, state, plugin.Options{}) case dns.TypeSRV: records, extra, err = plugin.SRV(ctx, \u0026amp;k, zone, state, plugin.Options{}) case dns.TypeSOA: if qname == zone { records, err = plugin.SOA(ctx, \u0026amp;k, zone, state, plugin.Options{}) } case dns.TypeAXFR, dns.TypeIXFR: return dns.RcodeRefused, nil case dns.TypeNS: if state.Name() == zone { records, extra, err = plugin.NS(ctx, \u0026amp;k, zone, state, plugin.Options{}) break } fallthrough default: // Do a fake A lookup, so we can distinguish between NODATA and NXDOMAIN fake := state.NewWithQuestion(state.QName(), dns.TypeA) fake.Zone = state.Zone _, err = plugin.A(ctx, \u0026amp;k, zone, fake, nil, plugin.Options{}) } 以dns.TypeA为例，看下plugin.A的处理流程 func A(ctx context.Context, b ServiceBackend, zone string, state request.Request, previousRecords []dns.RR, opt Options) (records []dns.RR, err error) { //检查特殊的 apex.dns 目录，寻找将以 A 或 AAAA 形式返回的记录。 services, err := checkForApex(ctx, b, zone, state, opt) if err != nil { return nil, err } dup := make(map[string]struct{}) // 轮询找到的所有记录 for _, serv := range services { what, ip := serv.HostType() switch what { case dns.TypeCNAME: if Name(state.Name()).Matches(dns.Fqdn(serv.Host)) { // x CNAME x is a direct loop, don't add those continue } newRecord := serv.NewCNAME(state.QName(), serv.Host) if len(previousRecords) \u0026gt; 7 { // don't add it, and just continue continue } if dnsutil.DuplicateCNAME(newRecord, previousRecords) { continue } if dns.IsSubDomain(zone, dns.Fqdn(serv.Host)) { state1 := state.NewWithQuestion(serv.Host, state.QType()) state1.Zone = zone nextRecords, err := A(ctx, b, zone, state1, append(previousRecords, newRecord), opt) if err == nil { // Not only have we found something we should add the CNAME and the IP addresses. if len(nextRecords) \u0026gt; 0 { records = append(records, newRecord) records = append(records, nextRecords...) } } continue } // This means we can not complete the CNAME, try to look else where. target := newRecord.Target // Lookup m1, e1 := b.Lookup(ctx, state, target, state.QType()) if e1 != nil { continue } // Len(m1.Answer) \u0026gt; 0 here is well? records = append(records, newRecord) records = append(records, m1.Answer...) continue case dns.TypeA: // a类型的记录根据host去重 if _, ok := dup[serv.Host]; !ok { dup[serv.Host] = struct{}{} records = append(records, serv.NewA(state.QName(), ip)) } case dns.TypeAAAA: // nada } } return records, nil } 查询符合条件的service func checkForApex(ctx context.Context, b ServiceBackend, zone string, state request.Request, opt Options) ([]msg.Service, error) { if state.Name() != zone { return b.Services(ctx, state, false, opt) } // 如果区名本身被解析，我们伪造查询以解析一个特殊条目，这相当于NS解析。 old := state.QName() state.Clear() state.Req.Question[0].Name = dnsutil.Join(\u0026quot;apex.dns\u0026quot;, zone) services, err := b.Services(ctx, state, false, opt) if err == nil { state.Req.Question[0].Name = old return services, err } state.Req.Question[0].Name = old return b.Services(ctx, state, false, opt) } 真正与k8s api-server交互的是Records方法： func (k *Kubernetes) Records(ctx context.Context, state request.Request, exact bool) ([]msg.Service, error) { r, e := parseRequest(state.Name(), state.Zone) if e != nil { return nil, e } if r.podOrSvc == \u0026quot;\u0026quot; { return nil, nil } if dnsutil.IsReverse(state.Name()) \u0026gt; 0 { return nil, errNoItems } if !wildcard(r.namespace) \u0026amp;\u0026amp; !k.namespaceExposed(r.namespace) { return nil, errNsNotExposed } // 在此之前都是做校验，下面才是真正的核心方法：判断当前请求的是pod地址还是service，并请求api-server获取对应kv if r.podOrSvc == Pod { pods, err := k.findPods(r, state.Zone) return pods, err } services, err := k.findServices(r, state.Zone) return services, err } 至此已完成k8s插件解析逻辑，整体流程还是非常清晰的，我表达的可能过分简化，如果想更详细的了解，还是看看代码\n调试k8s插件 本地开启kubectl proxy,将api-server代理至本地8001端口下 设置corefile中k8s插件的endpoint 调试模式启动coredns 使用dig请求解析，因为我们本地的resolv.conf是没有search的，所以需要解析完整的地址： dig -p 54 xxx.xx.svc.cluster.local ","id":6,"section":"posts","summary":"为什么要用coredns，而不是kube-dns https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/ 官网的这篇文章给出了很详细的理由，而且k8s官方也推荐用 k8s基于dns的服务发现 k8s dns 规范","tags":["k8s","dns","coredns","go"],"title":"coredns-如何实现k8s服务发现","uri":"https://blog.yoogo.top/2021/06/coredns-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0k8s%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","year":"2021"},{"content":"什么是linux namespace 命名空间是Linux内核的一个功能，它对内核资源进行分区，使一组进程看到一组资源，而另一组进程看到另一组资源。该功能的作用是为一组资源和进程提供相同的命名空间，但这些命名空间指的是不同的资源。资源可能存在于多个空间。这类资源的例子有：进程ID、主机名、用户ID、文件名，以及一些与网络访问和进程间通信相关的名称。\nhttps://en.wikipedia.org/wiki/Linux_namespaces\n现有种类 1. UTS\u0026ndash;CLONE_NEWUTS UTS Namespace 主要用来隔离 nodename 和 domainname 两个系统标识。在 UTS Namespace 里面 ， 每个 Namespace 允许有自己的 hostname\n2. Interprocess Communication (ipc)\u0026ndash;CLONE_NEWIPC IPC Namespace 用来隔离 System V IPC 和 POSIX message queues。 每一个 IPC Namespace 都有自己的 System V IPC 和 POSIX message queue。\n3. Process ID (pid)\u0026ndash;CLONE_NEWPID PID Namespace是用来隔离进程 ID的。同样一个进程在不同的 PIDNamespace里可以拥 有不同的 PID。这样就可以理解， 在 container里面， 使用 ps-ef经常会发现， 在容器 内， 前台运行的那个进程 PID 是 l， 但是在容器外，使用 ps -ef会发现同样的进程却有不同的PID， 这就是 PIDNamespace做的事情。\n实验时要注意，要用 echo $$来打印进程id，而不是ps，因为ps是读取/proc中的内容来显示，但此时/proc还是宿主机的内容\n4. Mount (mnt)\u0026ndash;CLONE_NEWNS MountNamespace用来隔离各个进程看到的挂载点视图。在不同Namespace的进程中， 看 到的文件系统层次是不一样的。在 Mount Namespace 中调用 mount()和 umount()仅仅只会影响 当前 Namespace 内的文件系统 ，而对全局的文件系统是没有影响的。\nMount Namespace 是 Linux 第 一个实现 的 Namespace 类型 ， 因此，它的系统调用参数 是 NEWNS ( New Namespace 的缩 写)。设计者没有预料到会有其他的命名空间。\n5. User ID (user) \u0026ndash;CLONE_NEWUSER User N amespace 主要是隔离用户 的用户组 ID。 也就是说 ， 一个进程的 User ID 和 Group ID在UserNamespace内外可以是不同的。 比较常用的是，在宿主机上以一个非root用户运行 创建一个 User Namespace， 然后在 User Namespace 里面却映射成 root 用户。这意味着 ， 这个 进程在 User Namespace 里面有 root权限，但是在 User Namespace 外面却没有 root 的权限。从 Linux Kernel 3.8开始， 非root进程也可以创建UserNamespace， 并且此用户在Namespace里 面可以被映射成 root， 且在 Namespace 内有 root权限。\n6. Network (net)\u0026ndash;CLONE_NEWNET Network Namespace 是用来隔离网络设备、 IP地址端口 等网络械的 Namespace。 Network Namespace 可以让每个容器拥有自己独立的(虚拟的)网络设备，而且容器内的应用可以绑定 到自己的端口，每个 Namespace 内的端口都不会互相冲突。在宿主机上搭建网桥后，就能很方 便地实现容器之间的通信，而且不同容器上的应用可以使用相同的端口 。\n7. Control group (cgroup) Namespace\u0026ndash;CLONE_NEWCGROUP cgroup命名空间类型隐藏了进程作为成员的控制组的身份。在这样的命名空间中的进程，在检查任何进程属于哪个控制组时，会看到一个实际上是相对于创建时设置的控制组的路径，隐藏其真实的控制组位置和身份\n8. Time Namespace\u0026ndash; CLONE_NEWTIME 时间命名空间允许进程以类似于UTS命名空间的方式看到不同的系统时间。\n提案种类 syslog namespace ","id":7,"section":"posts","summary":"什么是linux namespace 命名空间是Linux内核的一个功能，它对内核资源进行分区，使一组进程看到一组资源，而另一组进程看到另一组资源。该功能的作用","tags":["linux"],"title":"Linux Namespace","uri":"https://blog.yoogo.top/2021/05/linux-namespace/","year":"2021"},{"content":"按照官网检查required https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n此次使用ubuntu2004 kvm作为主机\n检查iptables # 检查是否加载模块br_netfilter lsmod | grep br_netfilter # 开启 sudo modprobe br_netfilter iptables 能够正确地查看桥接流量：\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab apt remove -y ufw lxd lxd-client lxcfs lxc-common 安装 runtime 目前可用的运行时：\n运行时 域套接字 Docker /var/run/dockershim.sock containerd /run/containerd/containerd.sock CRI-O /var/run/crio/crio.sock 如果同时检测到 Docker 和 containerd，则优先选择 Docker\n这里使用containerd：\nsudo apt-get update # 会依赖安装runc sudo apt-get install containerd containerd config default | sudo tee /etc/containerd/config.toml 结合 runc 使用 systemd cgroup 驱动，在 /etc/containerd/config.toml 中设置\n... [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;] ... sandbox_image = \u0026quot;registry.aliyuncs.com/google_containers/pause:3.5\u0026quot; ... [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc] ... [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry] ... [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.mirrors] [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://30l6yhjq.mirror.aliyuncs.com\u0026quot;] 设置启动文件\nwget https://github.com/containerd/containerd/archive/v1.5.7.tar.gz tar xvf v1.5.7.tar.gz \u0026amp;\u0026amp; cd containerd-1.5.7 cp containerd.service /etc/systemd/system/ 启动\nsystemctl daemon-reload systemctl enable containerd.service systemctl start containerd.service systemctl status containerd.service 安装 kubeadm、kubelet 和 kubectl 更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包： sudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install -y apt-transport-https ca-certificates curl 添加apt仓库\na. 可以访问外网的情况：\nsudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \u0026quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list b. 不能访问外网，使用阿里源：\ncurl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main EOF 开源社源（我用404）：\ncurl -s https://mirror.azure.cn/kubernetes/packages/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://mirror.azure.cn/kubernetes/packages/apt/ kubernetes-xenial main EOF 更新apt\nsudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 创建kubeadm config\nkind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta3 # kubernetesVersion: v1.22.3 imageRepository: registry.aliyuncs.com/google_containers controlPlaneEndpoint: '192.168.1.141:6443' apiServer: extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s networking: serviceSubnet: '10.96.0.0/12' # default podSubnet: \u0026quot;10.244.0.0/16\u0026quot; dnsDomain: 'cluster.local' # default controllerManager: {} etcd: local: dataDir: /var/lib/etcd certificatesDir: /etc/kubernetes/pki --- kind: InitConfiguration apiVersion: kubeadm.k8s.io/v1beta3 nodeRegistration: kubeletExtraArgs: pod-infra-container-image: 'registry.aliyuncs.com/google_containers/pause:3.5' network-plugin: cni v: '9' --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd #cgroupDriver: cgroupfs 初始化 kubeadm init --config kubeadm-init-config.yaml -v 20 等待初始化完成,完成后检查k8s状态 # 获取组件状态 kubectl get cs 因为kubeadm会默认不开启scheduler和controller-manager的http接口，因此可能会Unhealthy，解决：注释掉/etc/kubernetes/manifests/kube-scheduler.yaml,/etc/kubernetes/manifests/kube-controller-manager.yaml 中port=0字段，重启kubelet\n# 获取节点状态 kubectl get node kubectl describe nodes 此时节点是NotReady状态的，原因可以通过kubectl describe nodes中Conditions的得到：\nConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 30 Oct 2021 01:19:42 +0000 Fri, 29 Oct 2021 16:21:57 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 30 Oct 2021 01:19:42 +0000 Fri, 29 Oct 2021 16:21:57 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 30 Oct 2021 01:19:42 +0000 Fri, 29 Oct 2021 16:21:57 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready False Sat, 30 Oct 2021 01:19:42 +0000 Fri, 29 Oct 2021 16:21:57 +0000 KubeletNotReady container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized 因为没有cni插件，所以会NotReady\n安装最先进的cni插件-cilium curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} cilium install # 等待安装完成 cilium status kubectl get node ","id":8,"section":"posts","summary":"按照官网检查required https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 此次使用ubuntu2004 kvm作为主机 检查iptables # 检查是否加载模块br_netfilter lsmod | grep","tags":["k8s"],"title":"kubeadm部署k8s","uri":"https://blog.yoogo.top/2021/04/kubeadm%E9%83%A8%E7%BD%B2k8s/","year":"2021"},{"content":"\n跨域身份管理系统 SCIM 2，用于管理身份的开放API现已完成，并在IETF下发布。\n概述 跨域身份管理系统(SCIM)规范旨在使基于云的应用和服务中的用户身份管理更加容易。该规范套件寻求建立在现有模式和部署的经验基础上，特别强调开发和集成的简单性，同时应用现有的认证、授权和隐私模型。其目的是通过提供一个通用的用户模式和扩展模型，以及提供使用标准协议交换该模式的绑定文档，来降低用户管理操作的成本和复杂性。实质上：让用户快速、廉价、轻松地将用户移入、移出和在云中移动。\n本概览页上的信息不具有规范性。\n模型 SCIM 2.0建立在一个对象模型上，其中资源是共同点，所有SCIM对象都是由它派生出来的。它有id、externalId和meta作为属性，RFC7643定义了User、Group和EnterpriseUser，扩展了共同属性。\nUser 示例 这是一个如何将用户数据编码为JSON中的SCIM对象的例子。\n虽然这个例子不包含完整的可用属性集，但请注意可以用来创建SCIM对象的不同数据类型。简单类型，如id、用户名等的字符串。复杂类型，即有子属性的属性，如姓名和地址。多值类型，如电子邮件、电话号码、地址等。\n{ \u0026quot;schemas\u0026quot;: [\u0026quot;urn:ietf:params:scim:schemas:core:2.0:User\u0026quot;], \u0026quot;id\u0026quot;:\u0026quot;2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;externalId\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;meta\u0026quot;:{ \u0026quot;resourceType\u0026quot;: \u0026quot;User\u0026quot;, \u0026quot;created\u0026quot;:\u0026quot;2011-08-01T18:29:49.793Z\u0026quot;, \u0026quot;lastModified\u0026quot;:\u0026quot;2011-08-01T18:29:49.793Z\u0026quot;, \u0026quot;location\u0026quot;:\u0026quot;https://example.com/v2/Users/2819c223...\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;W\\/\\\u0026quot;f250dd84f0671c3\\\u0026quot;\u0026quot; }, \u0026quot;name\u0026quot;:{ \u0026quot;formatted\u0026quot;: \u0026quot;Mr. Dwight K Schrute, III\u0026quot;, \u0026quot;familyName\u0026quot;: \u0026quot;Schrute\u0026quot;, \u0026quot;givenName\u0026quot;: \u0026quot;Dwight\u0026quot;, \u0026quot;middleName\u0026quot;: \u0026quot;Kurt\u0026quot;, \u0026quot;honorificPrefix\u0026quot;: \u0026quot;Mr.\u0026quot;, \u0026quot;honorificSuffix\u0026quot;: \u0026quot;III\u0026quot; }, \u0026quot;userName\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;phoneNumbers\u0026quot;:[ { \u0026quot;value\u0026quot;:\u0026quot;555-555-8377\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;work\u0026quot; } ], \u0026quot;emails\u0026quot;:[ { \u0026quot;value\u0026quot;:\u0026quot;dschrute@example.com\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;work\u0026quot;, \u0026quot;primary\u0026quot;: true } ] } Group 示例 除了用户，SCIM还包括组的定义。组用于模拟所提供资源的组织结构。组可以包含用户或其他组。\n{ \u0026quot;schemas\u0026quot;: [\u0026quot;urn:ietf:params:scim:schemas:core:2.0:Group\u0026quot;], \u0026quot;id\u0026quot;:\u0026quot;e9e30dba-f08f-4109-8486-d5c6a331660a\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Sales Reps\u0026quot;, \u0026quot;members\u0026quot;:[ { \u0026quot;value\u0026quot;: \u0026quot;2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;$ref\u0026quot;: \u0026quot;https://example.com/v2/Users/2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;display\u0026quot;: \u0026quot;Dwight Schrute\u0026quot; }, { \u0026quot;value\u0026quot;: \u0026quot;902c246b-6245-4190-8e05-00816be7344a\u0026quot;, \u0026quot;$ref\u0026quot;: \u0026quot;https://example.com/v2/Users/902c246b-6245-4190-8e05-00816be7344a\u0026quot;, \u0026quot;display\u0026quot;: \u0026quot;Jim Halpert\u0026quot; } ], \u0026quot;meta\u0026quot;: { \u0026quot;resourceType\u0026quot;: \u0026quot;Group\u0026quot;, \u0026quot;created\u0026quot;: \u0026quot;2010-01-23T04:56:22Z\u0026quot;, \u0026quot;lastModified\u0026quot;: \u0026quot;2011-05-13T04:42:34Z\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;W\\/\\\u0026quot;3694e05e9dff592\\\u0026quot;\u0026quot;, \u0026quot;location\u0026quot;: \u0026quot;https://example.com/v2/Groups/e9e30dba-f08f-4109-8486-d5c6a331660a\u0026quot; } } rest操作 对于资源的操作，SCIM提供了一个REST API，具有丰富但简单的操作集，支持从对特定用户的特定属性进行修补到进行大规模的批量更新。\nCreate: POST https://example.com/{v}/{resource} Read: GET https://example.com/{v}/{resource}/{id} Replace: PUT https://example.com/{v}/{resource}/{id} Delete: DELETE https://example.com/{v}/{resource}/{id} Update: PATCH https://example.com/{v}/{resource}/{id} Search: GET https://example.com/{v}/{resource}?ﬁlter={attribute}{op}{value}\u0026amp;sortBy={attributeName}\u0026amp;sortOrder={ascending|descending} Bulk: POST https://example.com/{v}/Bulk Discovery 为了简化互操作性，SCIM提供了三个端点来发现支持的功能和具体的属性细节。\nGET /ServiceProviderConfig 规范遵守、认证方案、数据模型。 GET /ResourceTypes 用于发现可用资源类型的端点。 GET /Schemas 介绍资源和属性扩展。 Create Request 要创建一个资源，请向资源的相应端点发送一个HTTP POST请求。在下面的例子中，我们看到的是一个用户的创建。\n在这个例子和后面的例子中可以看到，URL包含一个版本号，这样SCIM API的不同版本可以共存。可用版本可以通过 ServiceProviderConﬁg 端点动态发现。\nPOST /v2/Users HTTP/1.1 Accept: application/json Authorization: Bearer h480djs93hd8 Host: example.com Content-Length: ... Content-Type: application/json { \u0026quot;schemas\u0026quot;:[\u0026quot;urn:ietf:params:scim:schemas:core:2.0:User\u0026quot;], \u0026quot;externalId\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;userName\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;name\u0026quot;:{ \u0026quot;familyName\u0026quot;:\u0026quot;Schrute\u0026quot;, \u0026quot;givenName\u0026quot;:\u0026quot;Dwight\u0026quot; } } Create Response 响应中包含创建的资源和HTTP代码201，表示资源创建成功。需要注意的是，返回的用户包含的数据比发布的多，id和meta数据已经被服务商添加到了一个完整的User资源中。位置头表示在后续请求中可以在哪里获取资源。\nHTTP/1.1 201 Created Content-Type: application/scim+json Location: https://example.com/v2/Users/2819c223-7f76-453a-919d-413861904646 ETag: W/\u0026quot;e180ee84f0671b1\u0026quot; { \u0026quot;schemas\u0026quot;:[\u0026quot;urn:ietf:params:scim:schemas:core:2.0:User\u0026quot;], \u0026quot;id\u0026quot;:\u0026quot;2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;externalId\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;meta\u0026quot;:{ \u0026quot;resourceType\u0026quot;:\u0026quot;User\u0026quot;, \u0026quot;created\u0026quot;:\u0026quot;2011-08-01T21:32:44.882Z\u0026quot;, \u0026quot;lastModified\u0026quot;:\u0026quot;2011-08-01T21:32:44.882Z\u0026quot;, \u0026quot;location\u0026quot;: \u0026quot;https://example.com/v2/Users/2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;W\\/\\\u0026quot;e180ee84f0671b1\\\u0026quot;\u0026quot; }, \u0026quot;name\u0026quot;:{ \u0026quot;familyName\u0026quot;:\u0026quot;Schrute\u0026quot;, \u0026quot;givenName\u0026quot;:\u0026quot;Dwight\u0026quot; }, \u0026quot;userName\u0026quot;:\u0026quot;dschrute\u0026quot; } Get Request 获取资源是通过向所需的资源端点发送HTTP GET请求来完成的，如本例。\nGET /v2/Users/2819c223-7f76-453a-919d-413861904646 HTTP/1.1 Host: example.com Accept: application/scim+json Authorization: Bearer h480djs93hd8 Get Response GET的响应包含资源。在后续请求中，Etag头可用于防止资源的并发修改。\nHTTP/1.1 200 OK HTTP/1.1 Content-Type: application/scim+json Location: https://example.com/v2/Users/2819c223-7f76-453a-919d-413861904646 ETag: W/\u0026quot;f250dd84f0671c3\u0026quot; { \u0026quot;schemas\u0026quot;: [\u0026quot;urn:ietf:params:scim:schemas:core:2.0:User\u0026quot;], \u0026quot;id\u0026quot;:\u0026quot;2819c223-7f76-453a-919d-413861904646\u0026quot;, \u0026quot;externalId\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;meta\u0026quot;:{ \u0026quot;resourceType\u0026quot;: \u0026quot;User\u0026quot;, \u0026quot;created\u0026quot;:\u0026quot;2011-08-01T18:29:49.793Z\u0026quot;, \u0026quot;lastModified\u0026quot;:\u0026quot;2011-08-01T18:29:49.793Z\u0026quot;, \u0026quot;location\u0026quot;:\u0026quot;https://example.com/v2/Users/2819c223...\u0026quot;, \u0026quot;version\u0026quot;:\u0026quot;W\\/\\\u0026quot;f250dd84f0671c3\\\u0026quot;\u0026quot; }, \u0026quot;name\u0026quot;:{ \u0026quot;formatted\u0026quot;: \u0026quot;Mr. Dwight K Schrute, III\u0026quot;, \u0026quot;familyName\u0026quot;: \u0026quot;Schrute\u0026quot;, \u0026quot;givenName\u0026quot;: \u0026quot;Dwight\u0026quot;, \u0026quot;middleName\u0026quot;: \u0026quot;Kurt\u0026quot;, \u0026quot;honorificPrefix\u0026quot;: \u0026quot;Mr.\u0026quot;, \u0026quot;honorificSuffix\u0026quot;: \u0026quot;III\u0026quot; }, \u0026quot;userName\u0026quot;:\u0026quot;dschrute\u0026quot;, \u0026quot;phoneNumbers\u0026quot;:[ { \u0026quot;value\u0026quot;:\u0026quot;555-555-8377\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;work\u0026quot; } ], \u0026quot;emails\u0026quot;:[ { \u0026quot;value\u0026quot;:\u0026quot;dschrute@example.com\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;work\u0026quot;, \u0026quot;primary\u0026quot;: true } ] } Filter Request 除了获取单个资源外，还可以通过查询资源端点来获取资源集，而无需特定资源的id。通常情况下，获取请求会包含一个应用于资源的过滤器。SCIM 支持等价、包含、开始与等过滤操作。除了过滤响应外，还可以要求服务提供商对响应中的资源进行排序。\n除了过滤响应外，还可以要求服务提供商对响应中的资源进行排序，返回资源的特定属性，以及只返回资源的子集。\nhttps://example.com/{resource}?ﬁlter={attribute} {op} {value} \u0026amp; sortBy={attributeName}\u0026amp;sortOrder={ascending|descending}\u0026amp;attributes={attributes} https://example.com/Users?ﬁlter=title pr and userType eq “Employee”\u0026amp;sortBy=title\u0026amp;sortOrder=ascending\u0026amp;attributes=title,username Filter Response 响应是一个匹配资源的列表。\n{ \u0026quot;schemas\u0026quot;:[\u0026quot;urn:ietf:params:scim:api:messages:2.0:ListResponse\u0026quot;], \u0026quot;totalResults\u0026quot;:2, \u0026quot;Resources\u0026quot;:[ { \u0026quot;id\u0026quot;:\u0026quot;c3a26dd3-27a0-4dec-a2ac-ce211e105f97\u0026quot;, \u0026quot;title\u0026quot;:\u0026quot;Assistant VP\u0026quot;, \u0026quot;userName\u0026quot;:\u0026quot;dschrute\u0026quot; }, { \u0026quot;id\u0026quot;:\u0026quot;a4a25dd3-17a0-4dac-a2ac-ce211e125f57\u0026quot;, \u0026quot;title\u0026quot;:\u0026quot;VP\u0026quot;, \u0026quot;userName\u0026quot;:\u0026quot;jsmith\u0026quot; } ] } 规范 SCIM 2.0 SCIM 2.0于2015年9月在IETF下以RFC7642、RFC7643和RFC7644的形式发布。\nRFC7643 - SCIM：核心模式。 核心模式提供了一个平台中立的模式和扩展模型来表示用户和组。\nRFC7644 - SCIM：协议 SCIM协议是一个应用级的REST协议，用于在网络上提供和管理身份数据。\nRFC7642 - SCIM：定义、概述、概念和要求。 本文档列出了跨域身份管理系统（SCIM）的用户场景和使用案例。\n相关文档和扩展。\n草稿-ansari-scim-soft-delete(软删除) 本文档指定了一个配置文件，用于处理 SCIM 服务提供商的用户软删除。\n草案-猎取-SCIM-通知 在SCIM环境中，多方可能会要求更改资源。随着时间的推移，感兴趣的用户可能希望被告知SCIM服务提供商正在发生的资源状态变化。本规范定义了一个中心通知服务，可用于向感兴趣的注册用户发布和分发事件。\n草稿-猎取-SCIM-口令-管理服务 本规范定义了一组密码和账户状态扩展，用于管理密码和密码使用（如失败）以及其他相关会话数据。该规范定义了新的ResourceTypes，可以实现对密码和账户恢复功能的管理。\n草案-wahl-scim-jit-profile 本文件规定了跨域身份管理协议系统（SCIM）的配置文件。实现SAML或OpenID Connect等协议的服务器通过这些协议接收用户身份，并经常对其进行缓存，SCIM的这个配置文件定义了身份提供者如何将用户账户的变化通知SCIM服务器。\nSCIM和vCard映射 该文档定义了SCIM和vCard之间的映射。\n草案-Grizzle-SCIM-PAM-EXT 本文档包含SCIM 2.0对特权访问管理的扩展，其中包括对核心用户和组对象的扩展，以及标准特权访问管理结构的新资源类型和模式。该扩展旨在为PAM软件和客户端之间提供更强的互操作性、PAM概念的通用语言以及可进一步扩展以支持更复杂的PAM要求的基线。\n","id":9,"section":"posts","summary":"跨域身份管理系统 SCIM 2，用于管理身份的开放API现已完成，并在IETF下发布。 概述 跨域身份管理系统(SCIM)规范旨在使基于云的应用和服务中的","tags":["iam"],"title":"scim 翻译","uri":"https://blog.yoogo.top/2021/04/scim%E6%A6%82%E8%BF%B0/","year":"2021"},{"content":"helm 安装 helm upgrade -i jenkins jenkins/jenkins -n jenkins \\ --set controller.ingress.enabled=true \\ --set controller.ingress.path=/ \\ --set controller.ingress.hostName=jenkins.yoogo.cc \\ --set agent.privileged=true \\ --set agent.volumes[0].type=HostPath \\ --set agent.volumes[0].hostPath=/var/run/docker.sock \\ --set agent.volumes[0].mountPath=/var/run/docker.sock --set agent.image=registry.cn-hangzhou.aliyuncs.com/yoogo-tools/jenkins-agent \\ --set agent.tag=4.7-3 ","id":10,"section":"posts","summary":"helm 安装 helm upgrade -i jenkins jenkins/jenkins -n jenkins \\ --set controller.ingress.enabled=true \\ --set controller.ingress.path=/ \\ --set controller.ingress.hostName=jenkins.yoogo.cc \\ --set agent.privileged=true \\ --set agent.volumes[0].type=HostPath \\ --set agent.volumes[0].hostPath=/var/run/docker.sock \\ --set agent.volumes[0].mountPath=/var/run/docker.sock --set agent.image=registry.cn-hangzhou.aliyuncs.com/yoogo-tools/jenkins-agent \\ --set agent.tag=4.7-3","tags":["devops"],"title":"jenkins","uri":"https://blog.yoogo.top/2021/04/jenkins/","year":"2021"},{"content":"1. load 入口文件位于 coremain/run.go#Run，在真正执行Run之前有一些init操作, 暂可称为Load阶段\n1. core/dnsserver/register.go#init func init() { flag.StringVar(\u0026amp;Port, serverType+\u0026quot;.port\u0026quot;, DefaultPort, \u0026quot;Default port\u0026quot;) caddy.RegisterServerType(serverType, caddy.ServerType{ Directives: func() []string { return Directives }, DefaultInput: func() caddy.Input { return caddy.CaddyfileInput{ Filepath: \u0026quot;Corefile\u0026quot;, Contents: []byte(\u0026quot;.:\u0026quot; + Port + \u0026quot; {\\nwhoami\\nlog\\n}\\n\u0026quot;), ServerTypeName: serverType, } }, NewContext: newContext, }) } 流程分析：\n从命令行参数dns.port获取Port,默认为 53 将serverType=\u0026ldquo;dns\u0026quot;注册至caddy，并配置插件链和默认配置文件Corefile及Context 插件链默认为core/dnsserver/zdirectives.go#Directives，注意插件链是顺序执行的 默认配置会开启whoami，log插件 注册caddy服务需要提供context，其类型为func(inst *Instance) Context： 默认的NewContext为\nfunc newContext(i *caddy.Instance) caddy.Context { return \u0026amp;dnsContext{keysToConfigs: make(map[string]*Config)} } type dnsContext struct { keysToConfigs map[string]*Config configs []*Config } func (h *dnsContext) saveConfig(key string, cfg *Config) { h.configs = append(h.configs, cfg) h.keysToConfigs[key] = cfg } var _ caddy.Context = \u0026amp;dnsContext{} func (h *dnsContext) InspectServerBlocks(sourceFile string, serverBlocks []caddyfile.ServerBlock) ([]caddyfile.ServerBlock, error) { ... return serverBlocks, nil } func (h *dnsContext) MakeServers() ([]caddy.Server, error) { ... return servers, nil } 编译其检查了var _ caddy.Context = \u0026amp;dnsContext{}，InspectServerBlocks，MakeServers目前并没有执行，只是加载到了内存中\n2. coremain/run.go#init 流程分析：\n设置默认加载的配置文件caddy.DefaultConfigFile = \u0026quot;Corefile\u0026quot;\n设置不输出caddy的init logcaddy.Quiet = true\n解析从命令行加载的参数：\na. conf: Corefile路径\nb. plugins: 显示安装的插件列表\nc. pidfile: 写入pid的路径\nd. version: 打印版本\ne. quiet: 不打印初始化日志\n注册caddyfileLoaders caddy.RegisterCaddyfileLoader(\u0026quot;flag\u0026quot;, caddy.LoaderFunc(confLoader))\n注册defaultCaddyfileLoadercaddy.SetDefaultCaddyfileLoader(\u0026quot;default\u0026quot;, caddy.LoaderFunc(defaultLoader))\nCoredns Run 至此load结束，正式进入Run方法\n1.coremain/run.go#Run func Run() { caddy.TrapSignals() flag.Parse() if len(flag.Args()) \u0026gt; 0 { mustLogFatal(fmt.Errorf(\u0026quot;extra command line arguments: %s\u0026quot;, flag.Args())) } log.SetOutput(os.Stdout) log.SetFlags(0) // Set to 0 because we're doing our own time, with timezone if version { showVersion() os.Exit(0) } if plugins { fmt.Println(caddy.DescribePlugins()) os.Exit(0) } // Get Corefile input corefile, err := caddy.LoadCaddyfile(serverType) if err != nil { mustLogFatal(err) } // Start your engines instance, err := caddy.Start(corefile) if err != nil { mustLogFatal(err) } if !dnsserver.Quiet { showVersion() } // Twiddle your thumbs instance.Wait() } 设置TrapSignals 加载serverType对应的corefile：corefile, err := caddy.LoadCaddyfile(serverType) 开启caddy服务：instance, err := caddy.Start(corefile) Caddy Start CoreDNS使用了 Caddy 提供的一些功能，因此需要开启caddy服务\nfunc Start(cdyfile Input) (*Instance, error) { inst := \u0026amp;Instance{serverType: cdyfile.ServerType(), wg: new(sync.WaitGroup), Storage: make(map[interface{}]interface{})} err := startWithListenerFds(cdyfile, inst, nil) if err != nil { return inst, err } signalSuccessToParent() if pidErr := writePidFile(); pidErr != nil { log.Printf(\u0026quot;[ERROR] Could not write pidfile: %v\u0026quot;, pidErr) } // Execute instantiation events EmitEvent(InstanceStartupEvent, inst) return inst, nil } 1. 初始化caddy.Instance实例 func startWithListenerFds(cdyfile Input, inst *Instance, restartFds map[string]restartTriple) error { ... instances = append(instances, inst) ... err = ValidateAndExecuteDirectives(cdyfile, inst, false) ... slist, err := inst.context.MakeServers() ... err = startServers(slist, inst, restartFds) .. return nil } 2. 将这个newInstance启动 func ValidateAndExecuteDirectives(cdyfile Input, inst *Instance, justValidate bool) error { .... sblocks, err := loadServerBlocks(stypeName, cdyfile.Path(), bytes.NewReader(cdyfile.Body())) ... sblocks, err = inst.context.InspectServerBlocks(cdyfile.Path(), sblocks) ... return executeDirectives(inst, cdyfile.Path(), stype.Directives(), sblocks, justValidate) } 3. 解析CoreFile，并加载server blocks func loadServerBlocks(serverType, filename string, input io.Reader) ([]caddyfile.ServerBlock, error) { validDirectives := ValidDirectives(serverType) serverBlocks, err := caddyfile.Parse(filename, input, validDirectives) ... return serverBlocks, nil } 找到所有可用插件 解析corefile，加载为serverBlocks type ServerBlock struct { Keys []string Tokens map[string][]Token } 4. 通过load阶段定义的NewContext的InspectServerBlocks方法重写/检查serverBlocks func (h *dnsContext) InspectServerBlocks(sourceFile string, serverBlocks []caddyfile.ServerBlock) ([]caddyfile.ServerBlock, error) { for ib, s := range serverBlocks { for ik, k := range s.Keys { za, err := normalizeZone(k) ..... s.Keys[ik] = za.String() ..... h.saveConfig(keyConfig, cfg) } } return serverBlocks, nil } CoreDNS在此步骤中做了两个事情\n检查并重写serverBlock.Key 将serverBlock 中的Token转换，并保存至context中 5. 执行插件初始化 func executeDirectives(inst *Instance, filename string, directives []string, sblocks []caddyfile.ServerBlock, justValidate bool) error { storages := make(map[int]map[string]interface{}) for _, dir := range directives { for i, sb := range sblocks { var once sync.Once if _, ok := storages[i]; !ok { storages[i] = make(map[string]interface{}) } for j, key := range sb.Keys { // Execute directive if it is in the server block if tokens, ok := sb.Tokens[dir]; ok { ......... setup, err := DirectiveAction(inst.serverType, dir) ......... err = setup(controller) ......... storages[i][dir] = controller.ServerBlockStorage } } } ......... } return nil } 此方法的主要目的是加载每个插件的setup方法\n6. 构造Server List 执行NewContext.MakeServers(), 看起来代码很多，实际上做的事只有一件：根据不同的addr初始化正确的Server实例（会注册插件）\nfunc (h *dnsContext) MakeServers() ([]caddy.Server, error) { ... groups, err := groupConfigsByListenAddr(h.configs) if err != nil { return nil, err } var servers []caddy.Server for addr, group := range groups { switch tr, _ := parse.Transport(addr); tr { case transport.DNS: s, err := NewServer(addr, group) if err != nil { return nil, err } servers = append(servers, s) case transport.TLS: s, err := NewServerTLS(addr, group) if err != nil { return nil, err } servers = append(servers, s) case transport.GRPC: s, err := NewServergRPC(addr, group) if err != nil { return nil, err } servers = append(servers, s) case transport.HTTPS: s, err := NewServerHTTPS(addr, group) if err != nil { return nil, err } servers = append(servers, s) } } return servers, nil } 7.监听服务 至此加载完毕，等待请求\n","id":11,"section":"posts","summary":"1. load 入口文件位于 coremain/run.go#Run，在真正执行Run之前有一些init操作, 暂可称为Load阶段 1. core/dnsserver/register.go#init func init() { flag.StringVar(\u0026amp;Port, serverType+\u0026quot;.port\u0026quot;, DefaultPort, \u0026quot;Default port\u0026quot;) caddy.RegisterServerType(serverType, caddy.ServerType{","tags":["k8s","dns","coredns","go"],"title":"coredns-启动流程","uri":"https://blog.yoogo.top/2021/03/coredns-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","year":"2021"},{"content":"gRPC 负载均衡 范围 本文档解释了gRPC中负载均衡的设计。\n背景 gRPC中的负载均衡是基于每次调用进行的，并不是基于每次连接的。换句话说，即使所有请求都来自同一客户端，我们依然希望他们在所有的服务端中进行负载均衡。\n架构 Overview gRPC客户端支持允许实现负载均衡策略并将其插入gRPC的API。这个负载均衡策略主要负责：\n从resolver接收更新配置和服务器地址列表 为服务器地址创建子通道，并管理他们的连接行为 设置通道的整体连接状态(通常通过聚合子通道的连接状态来计算) 为通道上发送的每个RPC，确定在哪个子通道上发送RPC gRPC提供了一些负载均衡策略。最值得注意的是pick_first(默认)、round_robin和grpclb。也一些额外的策略支持xDS,但是他们当前并没有直接配置。\nWorkflow 负载平衡策略适合 gRPC 客户端工作流，介于名称解析和与服务器的连接之间。以下是它的工作原理：\n再开始的时候，gRPC客户端对服务名发起名称解析请求。这个服务名会解析成一个ip地址列表，一个服务配置，指示使用哪个客户端负载均衡策略(例如round_robin或grpclb)，并提供该策略的配置和一组属性(C-core中的通道args)。 客户端实例化负载平衡策略并将其配置从服务配置、IP 地址列表和属性传递给它。 负载平衡策略为服务器的 IP 地址创建一组子通道（可能与解析器返回的 IP 地址不同；见下文）。它还监视子通道的连接状态并决定每个子通道何时应尝试连接。 对于每个发送的 RPC，负载平衡策略决定 RPC 应该发送到哪个子通道（即哪个服务器）。 有关 grpclb 的更多信息，请参见下文。\n负载均衡策略 pick_first 如果服务配置没有指定其他策略的情况下，pick_first是默认的LB策略。它不需要任何配置。\n这个策略会从解析器取到地址列表。它尝试按顺序一次连接一个地址，直到找到一个可访问的地址。如果没有可访问的，它在尝试重新连接时将通道的状态设置为 TRANSIENT_FAILURE。对重复的连接尝试应当需要适当的backoff。\n如果能连接到其中一个地址，它会设置通道的状态为READY，并且通道上的所有RPC都会从这个地址发送。如果这个地址的连接在之后断开了，pick_first策略会把通道置于IDLE状态，并且它不会尝试重新连接，直到应用程序请求它这样做（通过通道的连接状态 API 或通过发送 RPC）。\nround_robin 此 LB 策略是通过服务配置选择的。它不需要任何配置。\n这个策略会从解析器取到地址列表。它为每个地址创建一个子通道，并持续监控子通道的连接状态。每当子通道断开连接时，round_robin 策略将要求它重新连接，并带有适当的连接backoff。\n这个策略通过子通道状态的聚合来设置通道的连接状态：\n如果任何一个子通道是READY状态，那么这个主通道的状态就是READY。 如果任何一个子通道是CONNECTING状态，那么这个主通道的状态就是CONNECTING。 如果任何一个子通道是IDLE状态，那么这个主通道的状态就是IDLE。 如果所有子通道是TRANSIENT_FAILURE状态，那么这个主通道的状态才是TRANSIENT_FAILURE。 注意：当给定子通道报告 TRANSIENT_FAILURE 时，它被认为仍处于 TRANSIENT_FAILURE 中，直到它成功重新连接并报告 READY。特别是，我们忽略了从 TRANSIENT_FAILURE 到 CONNECTING 的转换。\n当在通道上发送RPC时，round_robin策略将遍历当前处于READY状态的所有子通道，将每个连续的RPC发送到列表中的下一个连续的子通道，在需要时环绕到列表的开始。\ngrpclb (这个策略已被废弃，推荐使用xDS代替)\n此 LB 策略最初旨在作为 gRPC 用于负载平衡的主要可扩展性机制。其目的是，与其直接在客户端中添加新的 LB 策略，客户端可以只实现简单的算法，如 round_robin，而任何更复杂的算法都将由后备负载均衡器提供。\n客户端依赖负载均衡器来提供配置和客户端应该向其发送请求的服务器地址列表。平衡器根据需要更新服务器列表以平衡负载以及处理服务器不可用或健康问题。负载均衡器将做出任何必要的复杂决策并通知客户端。负载均衡器可以与后端服务器通信以收集负载和健康信息。\ngrpclb 策略使用解析器返回的地址（如果有）作为备用地址，当它与平衡器失去联系时使用这些地址。\ngrpclb 策略通过解析器返回的属性获取要与之通信的平衡器的地址列表。\n译自：load-balancing\n","id":12,"section":"posts","summary":"gRPC 负载均衡 范围 本文档解释了gRPC中负载均衡的设计。 背景 gRPC中的负载均衡是基于每次调用进行的，并不是基于每次连接的。换句话说，即使所有请","tags":["go"],"title":"gRPC 负载均衡","uri":"https://blog.yoogo.top/2021/01/grpc-lb/","year":"2021"},{"content":"what is TCP 简介 传输控制协议（英语：Transmission Control Protocol，缩写：TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。在简化的计算机网络OSI模型中，它完成第四层传输层所指定的功能。用户数据报协议（UDP）是同一层内另一个重要的传输协议。\n概念 数据在TCP层称为流（Stream），数据分组称为分段（Segment）。作为比较，数据在IP层称为Datagram，数据分组称为分片（Fragment）。 UDP 中分组称为Message。\n**CWR(Congestion Window Reduce)：拥塞窗口减少标志被发送主机设置，用来表明它接收到了设置ECE标志的TCP包，发送端通过降低发送窗口的大小来降低发送速率\nECE(ECN Echo)：ECN响应标志被用来在TCP3次握手时表明一个TCP端是具备ECN功能的，并且表明接收到的TCP包的IP头部的ECN被设置为11。更多信息请参考RFC793。\nURG(Urgent)：该标志位置位表示紧急(The urgent pointer) 标志有效。该标志位目前已经很少使用参考后面流量控制和窗口管理部分的介绍。\nACK(Acknowledgment)：取值1代表Acknowledgment Number字段有效，这是一个确认的TCP包，取值0则不是确认包。后续文章介绍中当ACK标志位有效的时候我们称呼这个包为ACK包，使用大写的ACK称呼。\nPSH(Push)：该标志置位时，一般是表示发送端缓存中已经没有待发送的数据，接收端不将该数据进行队列处理，而是尽可能快将数据转由应用处理。在处理 telnet 或 rlogin 等交互模式的连接时，该标志总是置位的。\nRST(Reset)：用于复位相应的TCP连接。通常在发生异常或者错误的时候会触发复位TCP连接。\nSYN(Synchronize)：同步序列编号(Synchronize Sequence Numbers)有效。该标志仅在三次握手建立TCP连接时有效。它提示TCP连接的服务端检查序列编号，该序列编号为TCP连接初始端(一般是客户端)的初始序列编号。在这里，可以把TCP序列编号看作是一个范围从0到4，294，967，295的32位计数器。通过TCP连接交换的数据中每一个字节都经过序列编号。在TCP报头中的序列编号栏包括了TCP分段中第一个字节的序列编号。类似的后续文章介绍中当这个SYN标志位有效的时候我们称呼这个包为SYN包。\nFIN(Finish)：带有该标志置位的数据包用来结束一个TCP会话，但对应端口仍处于开放状态，准备接收后续数据。当FIN标志有效的时候我们称呼这个包为FIN包。\n运行 TCP协议的运行可划分为三个阶段：连接创建(connection establishment)、数据传送（data transfer）和连接终止（connection termination）。操作系统将TCP连接抽象为套接字表示的本地端点（local end-point），作为编程接口给程序使用。在TCP连接的生命期内，本地端点要经历一系列的状态改变。\n连接创建（3次握手） 第一次握手(client -\u0026gt; server) IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [S], seq 332753718, win 65535, options [mss 16344,nop,wscale 6,nop,nop,TS val 1572533652 ecr 0,sackOK,eol], length 0 0x0000: 4500 0040 0000 4000 4006 0000 7f00 0001 0x0010: 7f00 0001 f81b 22b8 13d5 6b36 0000 0000 0x0020: b002 ffff fe34 0000 0204 3fd8 0103 0306 0x0030: 0101 080a 5dba f594 0000 0000 0402 0000 过程：客户端（通过执行connect函数）向服务器端发送一个SYN包，请求一个主动打开。该包携带客户端为这个连接请求而设定的随机数（seq=332753718,hex(seq)=13d56b36）作为消息序列号。暂且称此次数据包的seq为s1\n意义：确认客户端发送能力正常\n第二次握手(server -\u0026gt; client) IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [S.], seq 1886023911, ack 332753719, win 65535, options [mss 16344,nop,wscale 6,nop,nop,TS val 1572533652 ecr 1572533652,sackOK,eol], length 0 0x0000: 4500 0040 0000 4000 4006 0000 7f00 0001 0x0010: 7f00 0001 22b8 f81b 706a 70e7 13d5 6b37 0x0020: b012 ffff fe34 0000 0204 3fd8 0103 0306 0x0030: 0101 080a 5dba f594 5dba f594 0402 0000 过程：服务器端收到一个合法的SYN包后，把该包放入SYN队列中；回送一个SYN/ACK。ACK的确认码应为第一次握手的s1+1，SYN/ACK包本身携带一个随机产生的seq。暂且称此次数据包的ACK为a1，暂且称此次数据包的seq为s2\n意义：服务端的接收、发送能力正常\n第三次 握手(client -\u0026gt; server, server -\u0026gt; client)\nIP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [.], ack 1, win 6379, options [nop,nop,TS val 1572533652 ecr 1572533652], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 0x0010: 7f00 0001 f81b 22b8 13d5 6b37 706a 70e8 0x0020: 8010 18eb fe28 0000 0101 080a 5dba f594 0x0030: 5dba f594 IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [.], ack 1, win 6379, options [nop,nop,TS val 1572533652 ecr 1572533652], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 0x0010: 7f00 0001 22b8 f81b 706a 70e8 13d5 6b37 0x0020: 8010 18eb fe28 0000 0101 080a 5dba f594 0x0030: 5dba f594 过程：客户端收到SYN/ACK包后，发送一个ACK包，该包的seq被设定为a1+1，而ACK的确认码则为s2+1。然后客户端的connect函数成功返回。当服务器端收到这个ACK包的时候，把请求帧从SYN队列中移出，放至ACCEPT队列中；这时accept函数如果处于阻塞状态，可以被唤醒，从ACCEPT队列中取出ACK包，重新创建一个新的用于双向通信的sockfd，并返回。\n意义： 客户端接收能力正常\n我自己感觉虽然是3次握手，但是由于tcp协议的对称性，即有发送就会有响应，所以真实是4次？\n数据传输 IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [P.], seq 1:1060, ack 1, win 6379, options [nop,nop,TS val 1572533652 ecr 1572533652], length 1059 0x0000: 4500 0457 0000 4000 4006 0000 7f00 0001 E..W..@.@....... 0x0010: 7f00 0001 f81b 22b8 13d5 6b37 706a 70e8 ......\u0026quot;...k7pjp. 0x0020: 8018 18eb 024c 0000 0101 080a 5dba f594 .....L......]... 0x0030: 5dba f594 504f 5354 202f 6261 7365 2f6c ]...POST./base/l 0x0040: 6f67 696e 2048 5454 502f 312e 310d 0a48 ogin.HTTP/1.1..H 0x0050: 6f73 743a 2031 3237 2e30 2e30 2e31 3a38 ost:.127.0.0.1:8 0x0060: 3838 380d 0a43 6f6e 6e65 6374 696f 6e3a 888..Connection: 0x0070: 206b 6565 702d 616c 6976 650d 0a43 6f6e .keep-alive..Con 0x0080: 7465 6e74 2d4c 656e 6774 683a 2039 380d tent-Length:.98. 0x0090: 0a61 6363 6570 743a 2061 7070 6c69 6361 .accept:.applica 0x00a0: 7469 6f6e 2f6a 736f 6e0d 0a55 7365 722d tion/json..User- 0x00b0: 4167 656e 743a 204d 6f7a 696c 6c61 2f35 Agent:.Mozilla/5 0x00c0: 2e30 2028 4d61 6369 6e74 6f73 683b 2049 .0.(Macintosh;.I 0x00d0: 6e74 656c 204d 6163 204f 5320 5820 3131 ntel.Mac.OS.X.11 0x00e0: 5f31 5f30 2920 4170 706c 6557 6562 4b69 _1_0).AppleWebKi 0x00f0: 742f 3533 372e 3336 2028 4b48 544d 4c2c t/537.36.(KHTML, 0x0100: 206c 696b 6520 4765 636b 6f29 2043 6872 .like.Gecko).Chr 0x0110: 6f6d 652f 3837 2e30 2e34 3238 302e 3134 ome/87.0.4280.14 0x0120: 3120 5361 6661 7269 2f35 3337 2e33 360d 1.Safari/537.36. 0x0130: 0a43 6f6e 7465 6e74 2d54 7970 653a 2061 .Content-Type:.a 0x0140: 7070 6c69 6361 7469 6f6e 2f6a 736f 6e0d pplication/json. 0x0150: 0a4f 7269 6769 6e3a 2068 7474 703a 2f2f .Origin:.http:// 0x0160: 3132 372e 302e 302e 313a 3838 3838 0d0a 127.0.0.1:8888.. 0x0170: 5365 632d 4665 7463 682d 5369 7465 3a20 Sec-Fetch-Site:. 0x0180: 7361 6d65 2d6f 7269 6769 6e0d 0a53 6563 same-origin..Sec 0x0190: 2d46 6574 6368 2d4d 6f64 653a 2063 6f72 -Fetch-Mode:.cor 0x01a0: 730d 0a53 6563 2d46 6574 6368 2d44 6573 s..Sec-Fetch-Des 0x01b0: 743a 2065 6d70 7479 0d0a 5265 6665 7265 t:.empty..Refere 0x01c0: 723a 2068 7474 703a 2f2f 3132 372e 302e r:.http://127.0. 0x01d0: 302e 313a 3838 3838 2f73 7761 6767 6572 0.1:8888/swagger 0x01e0: 2f69 6e64 6578 2e68 746d 6c0d 0a41 6363 /index.html..Acc 0x01f0: 6570 742d 456e 636f 6469 6e67 3a20 677a ept-Encoding:.gz 0x0200: 6970 2c20 6465 666c 6174 652c 2062 720d ip,.deflate,.br. 0x0210: 0a41 6363 6570 742d 4c61 6e67 7561 6765 .Accept-Language 0x0220: 3a20 7a68 2d43 4e2c 7a68 3b71 3d30 2e39 :.zh-CN,zh;q=0.9 0x0230: 0d0a 436f 6f6b 6965 3a20 5f67 613d 4741 ..Cookie:._ga=GA 0x0240: 312e 312e 3131 3434 3731 3235 3531 2e31 1.1.1144712551.1 0x0250: 3630 3939 3430 3830 323b 205f 686f 6d65 609940802;._home 0x0260: 6c61 6e64 5f73 6573 7369 6f6e 3d73 4962 land_session=sIb 0x0270: 7a4b 2532 4243 776b 4525 3242 5954 3978 zK%2BCwkE%2BYT9x 0x0280: 425a 3675 5041 7877 4641 4b46 4869 7732 BZ6uPAxwFAKFHiw2 0x0290: 6861 7150 3951 796b 5968 7371 7647 4d70 haqP9QykYhsqvGMp 0x02a0: 3033 5872 4470 5330 6a38 4b4f 6535 6c36 03XrDpS0j8KOe5l6 0x02b0: 6e48 2532 4654 4770 5151 324e 6157 7154 nH%2FTGpQQ2NaWqT 0x02c0: 654a 4f71 4944 6a6b 3272 537a 347a 4e54 eJOqIDjk2rSz4zNT 0x02d0: 6c63 586a 5843 4a64 3537 4b57 6642 4142 lcXjXCJd57KWfBAB 0x02e0: 7861 6463 5657 3070 586f 4177 3933 7444 xadcVW0pXoAw93tD 0x02f0: 5331 7a51 6663 6458 6d4d 3869 7263 3337 S1zQfcdXmM8irc37 0x0300: 7834 6c77 3179 514d 635a 6c33 4771 616e x4lw1yQMcZl3Gqan 0x0310: 4462 5639 5a58 6464 7375 6725 3246 7452 DbV9ZXddsug%2FtR 0x0320: 5655 4766 5953 3550 3874 6d44 5161 3362 VUGfYS5P8tmDQa3b 0x0330: 4968 4e7a 6a25 3242 6555 506a 6866 3725 IhNzj%2BeUPjhf7% 0x0340: 3246 7671 5446 4353 4c31 6a72 3268 4146 2FvqTFCSL1jr2hAF 0x0350: 2532 4249 4225 3242 5255 4834 5551 6453 %2BIB%2BRUH4UQdS 0x0360: 6b76 6750 6f7a 6d74 6e48 6172 6241 4f6b kvgPozmtnHarbAOk 0x0370: 4e6f 5250 2532 4676 596a 5376 3261 4f37 NoRP%2FvYjSv2aO7 0x0380: 506a 7358 7178 6248 5932 4a44 3866 5a51 PjsXqxbHY2JD8fZQ 0x0390: 6453 6c45 6d70 577a 4c41 5844 6745 7665 dSlEmpWzLAXDgEve 0x03a0: 3536 5755 4f69 6869 3559 6876 3564 4b76 56WUOihi5Yhv5dKv 0x03b0: 6171 7047 6d37 5736 534f 716d 436f 2533 aqpGm7W6SOqmCo%3 0x03c0: 442d 2d4d 5271 6745 4f6b 3343 4e45 4543 D--MRqgEOk3CNEEC 0x03d0: 5a6e 462d 2d72 3166 7154 3148 7830 5764 ZnF--r1fqT1Hx0Wd 0x03e0: 796e 7444 3357 3744 5951 6725 3344 2533 yntD3W7DYQg%3D%3 0x03f0: 440d 0a0d 0a7b 0a20 2022 6361 7074 6368 D....{...\u0026quot;captch 0x0400: 6122 3a20 2273 7472 696e 6722 2c0a 2020 a\u0026quot;:.\u0026quot;string\u0026quot;,... 0x0410: 2263 6170 7463 6861 4964 223a 2022 7374 \u0026quot;captchaId\u0026quot;:.\u0026quot;st 0x0420: 7269 6e67 222c 0a20 2022 7061 7373 776f ring\u0026quot;,...\u0026quot;passwo 0x0430: 7264 223a 2022 7374 7269 6e67 222c 0a20 rd\u0026quot;:.\u0026quot;string\u0026quot;,.. 0x0440: 2022 7573 6572 6e61 6d65 223a 2022 7374 .\u0026quot;username\u0026quot;:.\u0026quot;st 0x0450: 7269 6e67 220a 7d ring\u0026quot;.} IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [.], ack 1060, win 6363, options [nop,nop,TS val 1572533652 ecr 1572533652], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 22b8 f81b 706a 70e8 13d5 6f5a ....\u0026quot;...pjp...oZ 0x0020: 8010 18db fe28 0000 0101 080a 5dba f594 .....(......]... 0x0030: 5dba f594 IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [P.], seq 1:550, ack 1060, win 6363, options [nop,nop,TS val 1572533671 ecr 1572533652], length 549 0x0000: 4500 0259 0000 4000 4006 0000 7f00 0001 E..Y..@.@....... 0x0010: 7f00 0001 22b8 f81b 706a 70e8 13d5 6f5a ....\u0026quot;...pjp...oZ 0x0020: 8018 18db 004e 0000 0101 080a 5dba f5a7 .....N......]... 0x0030: 5dba f594 4854 5450 2f31 2e31 2032 3030 ]...HTTP/1.1.200 0x0040: 204f 4b0d 0a41 6363 6573 732d 436f 6e74 .OK..Access-Cont 0x0050: 726f 6c2d 416c 6c6f 772d 4372 6564 656e rol-Allow-Creden 0x0060: 7469 616c 733a 2074 7275 650d 0a41 6363 tials:.true..Acc 0x0070: 6573 732d 436f 6e74 726f 6c2d 416c 6c6f ess-Control-Allo 0x0080: 772d 4865 6164 6572 733a 2043 6f6e 7465 w-Headers:.Conte 0x0090: 6e74 2d54 7970 652c 4163 6365 7373 546f nt-Type,AccessTo 0x00a0: 6b65 6e2c 582d 4353 5246 2d54 6f6b 656e ken,X-CSRF-Token 0x00b0: 2c20 4175 7468 6f72 697a 6174 696f 6e2c ,.Authorization, 0x00c0: 2054 6f6b 656e 2c58 2d54 6f6b 656e 2c58 .Token,X-Token,X 0x00d0: 2d55 7365 722d 4964 0d0a 4163 6365 7373 -User-Id..Access 0x00e0: 2d43 6f6e 7472 6f6c 2d41 6c6c 6f77 2d4d -Control-Allow-M 0x00f0: 6574 686f 6473 3a20 504f 5354 2c20 4745 ethods:.POST,.GE 0x0100: 542c 204f 5054 494f 4e53 2c44 454c 4554 T,.OPTIONS,DELET 0x0110: 452c 5055 540d 0a41 6363 6573 732d 436f E,PUT..Access-Co 0x0120: 6e74 726f 6c2d 416c 6c6f 772d 4f72 6967 ntrol-Allow-Orig 0x0130: 696e 3a20 6874 7470 3a2f 2f31 3237 2e30 in:.http://127.0 0x0140: 2e30 2e31 3a38 3838 380d 0a41 6363 6573 .0.1:8888..Acces 0x0150: 732d 436f 6e74 726f 6c2d 4578 706f 7365 s-Control-Expose 0x0160: 2d48 6561 6465 7273 3a20 436f 6e74 656e -Headers:.Conten 0x0170: 742d 4c65 6e67 7468 2c20 4163 6365 7373 t-Length,.Access 0x0180: 2d43 6f6e 7472 6f6c 2d41 6c6c 6f77 2d4f -Control-Allow-O 0x0190: 7269 6769 6e2c 2041 6363 6573 732d 436f rigin,.Access-Co 0x01a0: 6e74 726f 6c2d 416c 6c6f 772d 4865 6164 ntrol-Allow-Head 0x01b0: 6572 732c 2043 6f6e 7465 6e74 2d54 7970 ers,.Content-Typ 0x01c0: 650d 0a43 6f6e 7465 6e74 2d54 7970 653a e..Content-Type: 0x01d0: 2061 7070 6c69 6361 7469 6f6e 2f6a 736f .application/jso 0x01e0: 6e3b 2063 6861 7273 6574 3d75 7466 2d38 n;.charset=utf-8 0x01f0: 0d0a 4461 7465 3a20 5361 742c 2031 3620 ..Date:.Sat,.16. 0x0200: 4a61 6e20 3230 3231 2030 393a 3336 3a34 Jan.2021.09:36:4 0x0210: 3420 474d 540d 0a43 6f6e 7465 6e74 2d4c 4.GMT..Content-L 0x0220: 656e 6774 683a 2034 340d 0a0d 0a7b 2263 ength:.44....{\u0026quot;c 0x0230: 6f64 6522 3a37 2c22 6461 7461 223a 7b7d ode\u0026quot;:7,\u0026quot;data\u0026quot;:{} 0x0240: 2c22 6d73 6722 3a22 e9aa 8ce8 af81 e7a0 ,\u0026quot;msg\u0026quot;:\u0026quot;........ 0x0250: 81e9 9499 e8af af22 7d .......\u0026quot;} IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [.], ack 550, win 6371, options [nop,nop,TS val 1572533671 ecr 1572533671], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 f81b 22b8 13d5 6f5a 706a 730d ......\u0026quot;...oZpjs. 0x0020: 8010 18e3 fe28 0000 0101 080a 5dba f5a7 .....(......]... 0x0030: 5dba f5a7 可以看出http协议中的get,post\u0026hellip;本质上是没有区别的，请求数据放在query，header，body本质是没有区别的，只是要用合适方式去做合适的事\n关闭连接 IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [F.], seq 550, ack 1060, win 6363, options [nop,nop,TS val 1572533681 ecr 1572533671], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 22b8 f81b 706a 730d 13d5 6f5a ....\u0026quot;...pjs...oZ 0x0020: 8011 18db fe28 0000 0101 080a 5dba f5b1 .....(......]... 0x0030: 5dba f5a7 ]... IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [.], ack 551, win 6371, options [nop,nop,TS val 1572533681 ecr 1572533681], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 f81b 22b8 13d5 6f5a 706a 730e ......\u0026quot;...oZpjs. 0x0020: 8010 18e3 fe28 0000 0101 080a 5dba f5b1 .....(......]... 0x0030: 5dba f5b1 ]... IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [.], ack 551, win 6371, length 0 0x0000: 4500 0028 6b78 0000 4006 0000 7f00 0001 E..(kx..@....... 0x0010: 7f00 0001 f81b 22b8 13d5 6f59 706a 730e ......\u0026quot;...oYpjs. 0x0020: 5010 18e3 fe1c 0000 P....... IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [.], ack 1060, win 6363, options [nop,nop,TS val 1572578966 ecr 1572533681], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 22b8 f81b 706a 730e 13d5 6f5a ....\u0026quot;...pjs...oZ 0x0020: 8010 18db fe28 0000 0101 080a 5dbb a696 .....(......]... 0x0030: 5dba f5b1 ]... IP 127.0.0.1.63515 \u0026gt; 127.0.0.1.8888: Flags [F.], seq 1060, ack 551, win 6371, options [nop,nop,TS val 1572607233 ecr 1572578966], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 f81b 22b8 13d5 6f5a 706a 730e ......\u0026quot;...oZpjs. 0x0020: 8011 18e3 fe28 0000 0101 080a 5dbc 1501 .....(......]... 0x0030: 5dbb a696 ]... IP 127.0.0.1.8888 \u0026gt; 127.0.0.1.63515: Flags [.], ack 1061, win 6363, options [nop,nop,TS val 1572607233 ecr 1572607233], length 0 0x0000: 4500 0034 0000 4000 4006 0000 7f00 0001 E..4..@.@....... 0x0010: 7f00 0001 22b8 f81b 706a 730e 13d5 6f5b ....\u0026quot;...pjs...o[ 0x0020: 8010 18db fe28 0000 0101 080a 5dbc 1501 .....(......]... 0x0030: 5dbc 1501 ]... 连接终止使用了四路握手过程（或称四次握手，four-way handshake），在这个过程中连接的每一侧都独立地被终止。当一个端点要停止它这一侧的连接，就向对侧发送FIN，对侧回复ACK表示确认。因此，拆掉一侧的连接过程需要一对FIN和ACK，分别由两侧端点发出。\n首先发出FIN的一侧，如果给对侧的FIN响应了ACK，那么就会超时等待2*MSL时间，然后关闭连接。在这段超时等待时间内，本地的端口不能被新连接使用；避免延时的包的到达与随后的新连接相混淆。RFC793定义了MSL为2分钟，Linux设置成了30s。参数tcp_max_tw_buckets控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的TIME_WAIT状态的连接给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow）\n连接可以工作在TCP半开状态。即一侧关闭了连接，不再发送数据；但另一侧没有关闭连接，仍可以发送数据。已关闭的一侧仍然应接收数据，直至对侧也关闭了连接。\n也可以通过测三次握手关闭连接。主机A发出FIN，主机B回复FIN \u0026amp; ACK，然后主机A回复ACK.\nhttps://zh.wikipedia.org/wiki/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE\ntcp抓包工具：tcpdump\n","id":13,"section":"posts","summary":"what is TCP 简介 传输控制协议（英语：Transmission Control Protocol，缩写：TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议","tags":["计算机网络"],"title":"tcp学习","uri":"https://blog.yoogo.top/2021/01/tcp%E5%AD%A6%E4%B9%A0/","year":"2021"},{"content":"为什么用 ’为什么用‘基本等价于：他比别人好在哪里；解决了哪些刚需及主要矛盾；以及他的设计思想对我们自身现有及未来的技术栈有什么影响；会引入哪些问题，是否在可接受范围内；容错率的高低；运维成本如何······\n（这大概也是团队每引入一个新技术所思考的）\n核心思想 The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem.\n翻译：我们开发的应用应该是对网络无感知的，当发生问题时可以简单并且准确的定位究竟是网络的问题还是应用本身的问题\n注释：事实上，这个思想与12factor中的’IV. 后端服务‘是不谋而合的，应用程序应该尽可能的无状态化\n（说很简单，做很难）\n运行时 sidecar模式：每启用一个应用程序就会启动一个对应的envoy进程，与应用程序通信使用localhost，优点：应用无感知，缺点：部署复杂度提高。 可以拦截l3/l4层，因此可以拦截tcp，udp请求，envoy正是通过拦截请求的方式来实现的 可以拦截http请求 （待完善）····\n概念 host，具有稳定访问路径的主机 Downstream，发送请求的envoy的host Upstream，接收请求的evnoy的host Listener，是可以连接Downstream的网络地址（port，unix socket···） Cluster，一组同一服务注册的Upstream Mesh，一组提供同一网络拓扑的host Runtime configuration，与Envoy一起部署的带外实时配置系统 基本配置 静态配置（简单分析下给出的示例） static_resources: listeners: # 监听器设置 - name: listener_0 address: socket_address: address: 0.0.0.0 # 监听地址 port_value: 10000 # 监听端口， 也就是说，外部可以通过 ip:10000的形式访问 filter_chains: # 过滤链 - filters: # 一组过滤器链，按序执行 - name: envoy.filters.network.http_connection_manager # 名称，envoy支持的 typed_config: #对应过滤器的配置,pb对象 \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http # 你所需要的状态前缀，可在http://你自己的admin-ui/stats下看到 access_log: #http访问日志 - name: envoy.access_loggers.file typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout http_filters: # http过滤链，按序执行 - name: envoy.filters.http.router route_config: #静态路由配置 name: local_route # 随便给你的路由起个名字 virtual_hosts: # 组成路由表的虚拟主机服务数组 - name: local_service # 随便给你的虚拟主机起个名字，仅在统计信息时使用 domains: [\u0026quot;*\u0026quot;] # 匹配域列表，支持通配符，具体看官网文档 routes: # 路由表 - match: # 匹配条件，具体支持哪些条件看官网文档 prefix: \u0026quot;/\u0026quot; route: # 路由信息 host_rewrite_literal: www.envoyproxy.io # 请求过程中重写http host header cluster: service_envoyproxy_io # 需要路由到的上游集群（envoy概念中的集群） clusters: - name: service_envoyproxy_io # 要和route中的cluster相符 connect_timeout: 30s # 连接超时 type: LOGICAL_DNS # 服务发现类型，此处的LOGICAL_DNS具体理解要看官网 # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY # dns解析策略，AUTO/V4_ONLY/V6_ONLY load_assignment: # 加载策略 cluster_name: service_envoyproxy_io endpoints: # 字面意思，翻译成中文感觉会损失一部分意思，还是直接读英文比较好 - lb_endpoints: - endpoint: address: #目标地址 socket_address: # 套接字地址 address: www.envoyproxy.io #真正地址 port_value: 443 # 端口 transport_socket: # 交换套接字？ 主要用于TLS name: envoy.transport_sockets.tls # 固定字符串 typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io #创建TLS后端连接时使用的SNI字符串 上述文档实现的东西很简单：\n访问0.0.0.0:10000会看到www.envoyproxy.io的页面\n看起来配置着实过于复杂\n动态配置 （待续···）\n","id":14,"section":"posts","summary":"为什么用 ’为什么用‘基本等价于：他比别人好在哪里；解决了哪些刚需及主要矛盾；以及他的设计思想对我们自身现有及未来的技术栈有什么影响；会引入哪","tags":["cloudnative"],"title":"first envoy(待完善)","uri":"https://blog.yoogo.top/2020/12/first-envoy/","year":"2020"},{"content":"核心组件 Prometheus Server， 服务本身，主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。 client libraries，客户端资源库，用于对接 Prometheus Server, 可以查询和上报数据。 push gateway ，推送网关，用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等。 各种汇报数据的 exporters ，例如汇报机器数据的 node_exporter, 汇报 MongoDB 信息的 MongoDB exporter 等等。 用于告警通知管理的 alertmanager 。 官网架构图：\nhelm安装 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add stable https://charts.helm.sh/stable helm repo update helm upgrade --install prometheus prometheus-community/prometheus \\ -n prometheus --create-namespace \\ --set alertmanager.ingress.enabled=true \\ --set alertmanager.ingress.hosts[0]=alertmanager.yoogo.cn \\ --set pushgateway.ingress.enabled=true \\ --set pushgateway.ingress.hosts[0]=pushgateway.yoogo.cn \\ --set server.ingress.enabled=true,server.ingress.hosts[0]=prometheus.yoogo.cn helm install grafana bitnami/grafana -n prometheus \\ --set global.storageClass=local-path \\ --set ingress.enabled=true,ingress.hosts[0].name=grafana.yoogo.cn ","id":15,"section":"posts","summary":"核心组件 Prometheus Server， 服务本身，主要用于抓取数据和存储时序数据，另外还提供查询和 Alert Rule 配置管理。 client libraries，客户端资源库，用于对接","tags":["k8s"],"title":"first prometheus(待完善)","uri":"https://blog.yoogo.top/2020/11/first-prometheus/","year":"2020"},{"content":"基本概念 VirtualService 介于service负载均衡到deploy之间，使目标service可控，进而可以达到蓝绿部署/金丝雀部署的效果\n模板例子\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 解释 hosts：虚拟服务的主机。虚拟服务主机名可以是 IP 地址、DNS 名称，或者依赖于平台的一个简称（例如 Kubernetes 服务的短名称），隐式或显式地指向一个完全限定域名（FQDN）。您也可以使用通配符（“*”）前缀，让您创建一组匹配所有服务的路由规则。虚拟服务的 hosts 字段实际上不必是 Istio 服务注册的一部分，它只是虚拟的目标地址。这让您可以为没有路由到网格内部的虚拟主机建模。\nhttp: http协议下的路由规则\n（待续···）\n","id":16,"section":"posts","summary":"基本概念 VirtualService 介于service负载均衡到deploy之间，使目标service可控，进而可以达到蓝绿部署/金丝雀部署的效果 模板例子 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name:","tags":["k8s"],"title":"first istio(待完善)","uri":"https://blog.yoogo.top/2020/11/first-istio/","year":"2020"},{"content":"默认HikariDataSource\n创建测试项目，引入相关包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;42.2.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; spring boot会在启动时DataSourceConfiguration.class,由于spring-boot-starter-jdbc中包含hikari包，则会加载HikariDataSource：\n// source in: DataSourceConfiguration.java#static class Hikari /** * Hikari DataSource configuration. */ @Configuration(proxyBeanMethods = false) @ConditionalOnClass(HikariDataSource.class) @ConditionalOnMissingBean(DataSource.class) @ConditionalOnProperty(name = \u0026quot;spring.datasource.type\u0026quot;, havingValue = \u0026quot;com.zaxxer.hikari.HikariDataSource\u0026quot;, matchIfMissing = true) static class Hikari { @Bean @ConfigurationProperties(prefix = \u0026quot;spring.datasource.hikari\u0026quot;) HikariDataSource dataSource(DataSourceProperties properties) { HikariDataSource dataSource = createDataSource(properties, HikariDataSource.class); if (StringUtils.hasText(properties.getName())) { dataSource.setPoolName(properties.getName()); } return dataSource; } } protected static \u0026lt;T\u0026gt; T createDataSource(DataSourceProperties properties, Class\u0026lt;? extends DataSource\u0026gt; type) { return (T) properties.initializeDataSourceBuilder().type(type).build(); } 查看DataSourceConfiguration上下文可得结论：无论哪种数据源/连接池都需要调用其静态方法createDataSource来初始化\n进入DataSourceProperties#initializeDataSourceBuilder方法，这里会真正的创建DataSource\npublic DataSourceBuilder\u0026lt;?\u0026gt; initializeDataSourceBuilder() { return DataSourceBuilder.create(getClassLoader()).type(getType()).driverClassName(determineDriverClassName()) .url(determineUrl()).username(determineUsername()).password(determinePassword()); } a. 通过getClassLoader()).type(getType())加载出HikariDataSource\nb. 设置Driver及相关参数\n由上代码可以知道determineDriverClassName()方法中会有确定驱动的算法\npublic String determineDriverClassName() { if (StringUtils.hasText(this.driverClassName)) { Assert.state(driverClassIsLoadable(), () -\u0026gt; \u0026quot;Cannot load driver class: \u0026quot; + this.driverClassName); return this.driverClassName; } String driverClassName = null; if (StringUtils.hasText(this.url)) { driverClassName = DatabaseDriver.fromJdbcUrl(this.url).getDriverClassName(); } if (!StringUtils.hasText(driverClassName)) { driverClassName = this.embeddedDatabaseConnection.getDriverClassName(); } if (!StringUtils.hasText(driverClassName)) { throw new DataSourceBeanCreationException(\u0026quot;Failed to determine a suitable driver class\u0026quot;, this, this.embeddedDatabaseConnection); } return driverClassName; } 显而易见的算法：\n优先级最高的是直接指定spring.datasource.driver-class-name 其次是spring.datasource.url 如果都没指定，则会从embeddedDatabaseConnection中取得 embeddedDatabaseConnection的加载逻辑是NONE，H2，DERBY，HSQL中按序循环，如果能找到相关类，就返回相对的embeddedDatabaseConnection determine**()方法都是类似的就不展开说明了 properties.initializeDataSourceBuilder().type(type).build() =》\nprivate void bind(DataSource result) { ConfigurationPropertySource source = new MapConfigurationPropertySource(this.properties); ConfigurationPropertyNameAliases aliases = new ConfigurationPropertyNameAliases(); aliases.addAliases(\u0026quot;driver-class-name\u0026quot;, \u0026quot;driver-class\u0026quot;); aliases.addAliases(\u0026quot;url\u0026quot;, \u0026quot;jdbc-url\u0026quot;); aliases.addAliases(\u0026quot;username\u0026quot;, \u0026quot;user\u0026quot;); Binder binder = new Binder(source.withAliases(aliases)); binder.bind(ConfigurationPropertyName.EMPTY, Bindable.ofInstance(result)); } bind方法会绑定相应bean，其中会调用HikariConfig#setDriverClassName方法\npublic void setDriverClassName(String driverClassName) { checkIfSealed(); Class\u0026lt;?\u0026gt; driverClass = attemptFromContextLoader(driverClassName); try { if (driverClass == null) { driverClass = this.getClass().getClassLoader().loadClass(driverClassName); LOGGER.debug(\u0026quot;Driver class {} found in the HikariConfig class classloader {}\u0026quot;, driverClassName, this.getClass().getClassLoader()); } } catch (ClassNotFoundException e) { LOGGER.error(\u0026quot;Failed to load driver class {} from HikariConfig class classloader {}\u0026quot;, driverClassName, this.getClass().getClassLoader()); } if (driverClass == null) { throw new RuntimeException(\u0026quot;Failed to load driver class \u0026quot; + driverClassName + \u0026quot; in either of HikariConfig class loader or Thread context classloader\u0026quot;); } try { driverClass.getConstructor().newInstance(); this.driverClassName = driverClassName; } catch (Exception e) { throw new RuntimeException(\u0026quot;Failed to instantiate class \u0026quot; + driverClassName, e); } } 这里会尝试driverClass.getConstructor().newInstance();，即实例化一个h2/pg Driver对象，打开org.h2.Driver/org.postgresql.Driver 可以看到有static代码块初始化registerDriver操作\n","id":17,"section":"posts","summary":"默认HikariDataSource 创建测试项目，引入相关包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;42.2.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; spring boot会在启动时DataSourceConfig","tags":["java"],"title":"java学习笔记-spring datasource加载driver分析","uri":"https://blog.yoogo.top/2020/10/spring-datasource%E5%8A%A0%E8%BD%BDdriver%E5%88%86%E6%9E%90/","year":"2020"},{"content":"为什么需要flyway/liquibase 准确的说应该是为什么需要database migrations？\n方便于多环境下的数据结构一致，例如多开发/测试环境下的数据库一致性，开发、测试、生产的结构一致性 迁移文件跟随每次git迭代而迭代，保证了对db结构的版本控制 清楚的通过代码了解数据库所处的状态 db环境轻松克隆 database migrations工具不算缺点的缺点 额外的增加了运维成本，此类工具需要与发布流程集成，即再每次发布应用前先执行migration操作，判断迁移成功后继续发布应用，对于运维和开发的协作来说，有一定的沟通成本和学习成本 增加发布部署负担，因为无法自动化知道哪一次发布需要迁移，所以需要在每次执行，会增加一定的发布时间 对于锁表的一些迁移只能通过手工执行而不是通过工具来执行，否则很可能发生部署事故，所以需要code reviewer 严格审核 基于flyway java api的开发环境 模块结构：\n. ├── Dockerfile ├── Makefile ├── README.md ├── build.gradle └── src └── main ├── java │ └── Migrant.java └── resources ├── db │ └── migration │ ├── V20200911114448__change_some_table.sql │ ├── V20200915102250__create_log_tables.sql │ ├── V20200919095412__add_colume_for_order.sql │ ├── V20200930171100__rename_colume_for_order.sql │ └── V20201009165800__add_colume_for_order.sql ├── migrant.properties └── migrant.properties.example flyway 默认db/migration/为迁移目录 migrant.properties.example 配置example，migrant.properties为个性化配置，加入gitignore，方便区分多环境 基于flyway docker image 的部署方案 Dockerfile\nFROM flyway/flyway:7-alpine COPY src/main/resources/db/migration/ /flyway/sql 看起来非常简单，只是将本模块下的迁移文件复制到镜像中\nMakefile\nNAME\t:= \u0026quot;mall-migration\u0026quot; DOCKER_BASE_NAME\t:= $(NAME):$(tag) BUILD_TAG\t:= $(tag) ifeq (\u0026quot;$(BUILD_TAG)\u0026quot;,\u0026quot;\u0026quot;) BUILD_TAG\t:= $(shell date +%Y%m%d%H%M) endif .PHONY: docker .DEFAULT_GOAL := docker docker: $(info ======== build $(NAME) release image:) docker build -t $(NAME):$(BUILD_TAG) --rm -f Dockerfile . 加入Makefile是为了方便运维，运维只需要make tag=xxx就可以了，屏蔽了docker build细节\nrun 这个镜像\nK8s job方式:\napiVersion: batch/v1 kind: Job metadata: name: migration namespace: migration spec: template: spec: imagePullSecrets: - name: registry-xx restartPolicy: Never containers: - name: migration image: xxx:xx restartPolicy: Never imagePullPolicy: IfNotPresent args: - info - migrate - info env: - name: FLYWAY_URL value: jdbc:postgresql://pgsql:5432/xx?characterEncoding=utf8\u0026amp;useSSL=false - name: FLYWAY_USER value: postgres - name: FLYWAY_PASSWORD value: postgres # 可以放到secrets里，这里为了直观就直接放了 - name: FLYWAY_SCHEMAS value: \u0026quot;false\u0026quot; - name: FLYWAY_VALIDATE_ON_MIGRATE value: \u0026quot;false\u0026quot; - name: FLYWAY_BASELINE_ON_MIGRATE value: \u0026quot;true\u0026quot; - name: FLYWAY_OUT_OF_ORDER value: \u0026quot;true\u0026quot; backoffLimit: 0 ​ docker 运行：docker run --name xx-migration xx/migration migration 加入到你的部署流程里，放在部署应用之前\n除了配置数据源外其他有用的配置\na. createSchemas: 是否创建一个新的schema\nb. schemas: 如果迁移文件未指定schema，则以这里指定的schema为准\nc. validateOnMigrate: 是否验证迁移文件，在执行完migration后，flyway会计算出每个文件的checksum，如果开启，那迁移完毕后再次调整这个文件时候抛出异常的\nd. baselineOnMigrate: 已存在的数据库需要指定baselineOnMigrate为true,才会生成flyway_schema_history表\ne. outOfOrder: 无序执行迁移，如果outOfOrder为false，则已经应用了1.0和3.0版本，现在又找到了2.0版，那么它也会执行而不是忽略。\n所有配置： https://flywaydb.org/documentation/configuration/parameters/\n","id":18,"section":"posts","summary":"为什么需要flyway/liquibase 准确的说应该是为什么需要database migrations？ 方便于多环境下的数据结构一致，例如多","tags":["java"],"title":"flyway初次实践","uri":"https://blog.yoogo.top/2020/10/flyway%E5%88%9D%E6%AC%A1%E5%AE%9E%E8%B7%B5/","year":"2020"},{"content":"setup kubectl create namespace kafka-connect-tutorial kubectl config set-context --current --namespace kafka-connect-tutorial # optional kafka helm install kafka --namespace kafka-connect-tutorial apphub/kafka --set external.enabled=true,global.storageClass=fast,persistence.size=1Gi Kafka Connect client cat \u0026gt; kafka-client-deploy.yaml \u0026lt;\u0026lt;EOF # kafka-client-deploy.yaml apiVersion: v1 kind: Pod metadata: name: kafka-client spec: containers: - name: kafka-client image: confluentinc/cp-kafka:5.0.1 command: - sh - -c - \u0026quot;exec tail -f /dev/null\u0026quot; EOF kubectl create -f kafka-client-deploy.yaml -n kafka-connect-tutorial kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-offsets --create --partitions 1 --replication-factor 1 kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-configs --create --partitions 1 --replication-factor 1 kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-status --create --partitions 1 --replication-factor 1 Kafka Connect # kafka-connect-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kafkaconnect-deploy labels: app: kafkaconnect spec: replicas: 1 selector: matchLabels: app: kafkaconnect template: metadata: labels: app: kafkaconnect spec: containers: - name: kafkaconnect-container image: debezium/connect:latest readinessProbe: httpGet: path: / port: 8083 livenessProbe: httpGet: path: / port: 8083 env: - name: BOOTSTRAP_SERVERS value: kafka-headless.kafka:9092 - name: GROUP_ID value: \u0026quot;1\u0026quot; - name: OFFSET_STORAGE_TOPIC value: connect-offsets - name: CONFIG_STORAGE_TOPIC value: connect-configs - name: STATUS_STORAGE_TOPIC value: connect-status ports: - containerPort: 8083 --- apiVersion: v1 kind: Service metadata: name: kafkaconnect-service labels: app: kafkaconnect-service spec: type: NodePort ports: - name: kafkaconnect protocol: TCP port: 8083 nodePort: 30500 selector: app: kafkaconnect kubectl apply -f kafka-connect-deploy.yaml -n kafka-connect-tutorial pg # extended.conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 kubectl create configmap --namespace kafka-connect-tutorial --from-file=extended.conf postgresql-config helm install postgres --namespace kafka-connect-tutorial --set extendedConfConfigMap=postgresql-config --set service.type=NodePort --set service.nodePort=30600 --set postgresqlPassword=passw0rd,global.storageClass=fast,persistence.size=1Gi apphub/postgresql kubectl exec --namespace kafka-connect-tutorial -it postgres-postgresql-0 -- /bin/sh psql --user postgres # =\u0026gt; 导入测试数据 # 绑定 curl -X POST \\ http://192.168.1.61:30500/connectors \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;name\u0026quot;: \u0026quot;containers-connector\u0026quot;, \u0026quot;config\u0026quot;: { \u0026quot;connector.class\u0026quot;: \u0026quot;io.debezium.connector.postgresql.PostgresConnector\u0026quot;, \u0026quot;plugin.name\u0026quot;: \u0026quot;pgoutput\u0026quot;, \u0026quot;database.hostname\u0026quot;: \u0026quot;postgres-postgresql-headless.postgres\u0026quot;, \u0026quot;database.port\u0026quot;: \u0026quot;5432\u0026quot;, \u0026quot;database.user\u0026quot;: \u0026quot;postgres\u0026quot;, \u0026quot;database.password\u0026quot;: \u0026quot;postgres\u0026quot;, \u0026quot;database.dbname\u0026quot;: \u0026quot;mmtip-production\u0026quot;, \u0026quot;database.server.name\u0026quot;: \u0026quot;postgres\u0026quot; } }' kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --list kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-console-consumer --topic postgres.public.containers --from-beginning --bootstrap-server kafka:9092 ","id":19,"section":"posts","summary":"setup kubectl create namespace kafka-connect-tutorial kubectl config set-context --current --namespace kafka-connect-tutorial # optional kafka helm install kafka --namespace kafka-connect-tutorial apphub/kafka --set external.enabled=true,global.storageClass=fast,persistence.size=1Gi Kafka Connect client cat \u0026gt; kafka-client-deploy.yaml \u0026lt;\u0026lt;EOF # kafka-client-deploy.yaml apiVersion: v1 kind: Pod metadata: name: kafka-client spec: containers: - name: kafka-client image: confluentinc/cp-kafka:5.0.1 command: - sh - -c - \u0026quot;exec tail -f /dev/null\u0026quot; EOF kubectl create -f kafka-client-deploy.yaml -n kafka-connect-tutorial kubectl -n kafka-connect-tutorial exec kafka-client -- kafka-topics --zookeeper","tags":["k8s"],"title":"Streaming PostgreSQL Updates to Kafka with Debezium","uri":"https://blog.yoogo.top/2020/10/streaming-postgresql-updates-to-kafka-with-debezium-in-k8s/","year":"2020"},{"content":" 创建 secret kubectl create secret generic ceph-secret --type=\u0026quot;kubernetes.io/rbd\u0026quot; \\ --from-literal=key='AQDbfnVfkbqeLxAAiPhSJvt1hVZrM9ntL7JGNQ==' \\ --namespace=kube-system 创建存储类 cat \u0026gt; storage_class.yml \u0026lt;\u0026lt;EOF apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rbd annotations: \u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;true\u0026quot; provisioner: kubernetes.io/rbd parameters: monitors: 192.168.1.31:6789,192.168.1.32:6789,192.168.1.33:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: rbd-k8s1 userId: admin userSecretName: ceph-secret userSecretNamespace: kube-system fsType: ext4 imageFormat: \u0026quot;2\u0026quot; imageFeatures: \u0026quot;layering\u0026quot; EOF kubectl apply -f storage_class.yml 创建pvc 应用绑定pvc ","id":20,"section":"posts","summary":"创建 secret kubectl create secret generic ceph-secret --type=\u0026quot;kubernetes.io/rbd\u0026quot; \\ --from-literal=key='AQDbfnVfkbqeLxAAiPhSJvt1hVZrM9ntL7JGNQ==' \\ --namespace=kube-system 创建存储类 cat \u0026gt; storage_class.yml \u0026lt;\u0026lt;EOF apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rbd annotations: \u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;true\u0026quot; provisioner: kubernetes.io/rbd parameters: monitors: 192.168.1.31:6789,192.168.1.32:6789,192.168.1.33:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: rbd-k8s1 userId: admin userSecretName: ceph-secret userSecretNamespace: kube-system fsType: ext4 imageFormat: \u0026quot;2\u0026quot; imageFeatures: \u0026quot;layering\u0026quot; EOF kubectl apply -f storage_class.yml 创建pvc 应用绑","tags":["k8s"],"title":"k8s \u0026 rbd","uri":"https://blog.yoogo.top/2020/10/k8scephrbd/","year":"2020"},{"content":"REQUIREMENTS docker NTP lvm Any modern Linux * 3 INSTALL CEPHADM curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release octopus ./cephadm install BOOTSTRAP A NEW CLUSTER mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.1.31 =\u0026gt;\nURL: https://ceph:8443/ User: admin Password: 62zgr41d25 ENABLE CEPH CLI # 1. 使用adm代理 cephadm shell # 2. 安装ceph-common cephadm add-repo --release octopus cephadm install ceph-common ADD HOSTS TO THE CLUSTER # ssh-copy-id -f -i /etc/ceph/ceph.pub root@*\u0026lt;new-host\u0026gt;* ssh-copy-id -f -i /etc/ceph/ceph.pub root@host2 # ceph orch host add *newhost* ceph orch host add host2 192.168.1.32 DEPLOY ADDITIONAL MONITORS (OPTIONAL)¶ add host以后会自动部署mon\nDEPLOY OSDS(IMPORTANT) # ceph orch daemon add osd *\u0026lt;host\u0026gt;*:*\u0026lt;device-path\u0026gt;* ceph orch daemon add osd ceph:/dev/vdc DEPLOY MDSS(CephFS) # ceph orch apply mds *\u0026lt;fs-name\u0026gt;* --placement=\u0026quot;*\u0026lt;num-daemons\u0026gt;* [*\u0026lt;host1\u0026gt;* ...]\u0026quot; DEPLOY RGWS # ceph orch apply rgw *\u0026lt;realm-name\u0026gt;* *\u0026lt;zone-name\u0026gt;* --placement=\u0026quot;*\u0026lt;num-daemons\u0026gt;* [*\u0026lt;host1\u0026gt;* ...]\u0026quot; ceph orch apply rgw yoogo cn-guangzhou-1 --placement=\u0026quot;1 ceph\u0026quot; radosgw-admin user create --uid=\u0026quot;test\u0026quot; --display-name=\u0026quot;test user\u0026quot; =\u0026gt; \u0026quot;access_key\u0026quot;: \u0026quot;LT5J2627727WOD89ULZV\u0026quot; =\u0026gt; \u0026quot;secret_key\u0026quot;: \u0026quot;2PyRDiJezEgjdrw6JUMaAmD4m4XE7TdUKnFBFuL9\u0026quot; radosgw-admin user create --uid='admin' --display-name='admin' --system =\u0026gt; { \u0026quot;user\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;access_key\u0026quot;: \u0026quot;Z9MK5JDAF5Z3HCZNML8A\u0026quot;, \u0026quot;secret_key\u0026quot;: \u0026quot;rx4e9K7SZ7wDzwiqwVM1czOBNa6NIPkxzkumntZY\u0026quot; } Gotcha 如果单节点部署必须要关掉自动扩容，否则Cluster Status可能一直warn，而rgw service依赖HEALTH_OK，\n多节点部署也可能会出现这个问题，做法：多加几个osd\nENABLING THE OBJECT GATEWAY MANAGEMENT FRONTEND radosgw-admin user create --uid=\u0026lt;user_id\u0026gt; --display-name=\u0026lt;display_name\u0026gt; --system radosgw-admin user info --uid=\u0026lt;user_id\u0026gt; ceph dashboard set-rgw-api-access-key \u0026lt;access_key\u0026gt; ceph dashboard set-rgw-api-secret-key \u0026lt;secret_key\u0026gt; ceph dashboard set-rgw-api-host \u0026lt;host\u0026gt; ceph dashboard set-rgw-api-port \u0026lt;port\u0026gt; ceph dashboard set-rgw-api-scheme \u0026lt;scheme\u0026gt; # http or https ceph dashboard set-rgw-api-admin-resource \u0026lt;admin_resource\u0026gt; # 默认可以用 ceph dashboard set-rgw-api-user-id \u0026lt;user_id\u0026gt; s3cmd s3cmd --configure =\u0026gt; Access key and Secret key 填 radosgw-admin user info --uid=\u0026lt;user_id\u0026gt; 拿到的相应值 =\u0026gt; Default Region 这个默认US即可 =\u0026gt; S3 Endpoint 输入自己的rgw地址 =\u0026gt; DNS-style bucket+hostname 如果rgw已经有域名则可以设置为%(bucket)s.s3.amazonaws.com或者s3.amazonaws.com/%(bucket)，如果没有的话，只能设置为192.168.1.31/%(bucket)（特指格式，不是照抄就行） =\u0026gt; 其余没有特殊要求均可默认，http/https自选 RBD 最好用的块存储\n命令行创建或者dashborad创建一个pool\nrbd初始化rbd pool init \u0026lt;pool-name\u0026gt;\n创建一个供rbd访问的user\n# ceph auth get-or-create client.{ID} mon 'profile rbd' osd 'profile {profile name} [pool={pool-name}][, profile ...]' mgr 'profile rbd [pool={pool-name}]' ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes' [client.kubernetes] key = AQBYwUBgKY0aDhAAj2Smeemh4p/lU4U4RbaoJw== 创建rbd image # rbd create --size {megabytes} {pool-name}/{image-name} rbd create --size 1024 rbd-k8s1/yoogo ","id":21,"section":"posts","summary":"REQUIREMENTS docker NTP lvm Any modern Linux * 3 INSTALL CEPHADM curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release octopus ./cephadm install BOOTSTRAP A NEW CLUSTER mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.1.31 =\u0026gt; URL: https://ceph:8443/ User: admin Password: 62zgr41d25 ENABLE CEPH CLI # 1. 使用adm代理 cephadm shell # 2. 安装ceph-comm","tags":["linux"],"title":"ceph install","uri":"https://blog.yoogo.top/2020/10/ceph-install/","year":"2020"},{"content":"换了台机器，顺便把常用的app记录下来吧\nxcode 直接app store安装即可\nnavicat，idea，vs code。。。\nIterm2(官网下载)，oh my zsh，调教iterm\nBrew(官网下载)， 国内源\n工具类软件\na. md: typora\nb. ms office\nc. 压缩 Unarchiver\n基础设施类软件\na. docker, minikube\nb. postgres.app\nc. rbenv, nvm, gvm, sdkman\nd. feem\ne. postman\ncheatsheet\nsnipaste\nmagnet\nalfred\niina\nupaste\n","id":22,"section":"posts","summary":"换了台机器，顺便把常用的app记录下来吧 xcode 直接app store安装即可 navicat，idea，vs code。。。 Iterm2(官网下载)，","tags":["mac"],"title":"mac装机必备工具","uri":"https://blog.yoogo.top/2020/09/mac%E5%BF%85%E5%A4%87%E5%B7%A5%E5%85%B7/","year":"2020"},{"content":"named-checkconf named-checkzone yoogo.local zones/yoogo.local systemctl restart bind9.service # 一定要重启下网络 netplan apply ","id":23,"section":"posts","summary":"named-checkconf named-checkzone yoogo.local zones/yoogo.local systemctl restart bind9.service # 一定要重启下网络 netplan apply","tags":["devops"],"title":"bind9(todo detail)","uri":"https://blog.yoogo.top/2020/08/bind9/","year":"2020"},{"content":"requirment 生产环境的部署Kubernetes集群方案 kubeadm Kubeadm是一个K8s部署工具，提供kubeadm init和kubeadm join，用于快速部署Kubernetes集群。\n官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\nkubeasz 国内大佬开源的使用Ansible脚本安装K8S集群，方便直接，不受国内网络环境影响\ngithub地址：https://github.com/easzlab/kubeasz\n二进制包编译安装 从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。\n上述两种方式虽然降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。（主要是有利于学习）\n安装要求 连通外网 具有内网静态ip 4台linux(我用ubuntu20)服务器 ，all in one 规划 角色 IP 组件 master1 192.168.1.81 kube-apiserver，kube-controller-manager，kube-scheduler，etcd，kubelet，kube-proxy，docker 主机初始化 # 关闭swap sed -ri 's/.*swap.*/#\u0026amp;/' /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname \u0026lt;hostname\u0026gt; # 将桥接的IPv4流量传递到iptables的链 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF apt install bash-completion conntrack ipvsadm jq libseccomp2 golang-cfssl # 方便后续操作 cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ~/.bashrc export PATH=/opt/etcd/bin:/opt/kubernetes/bin:/opt/cni/bin:$PATH EOF 部署etcd 颁发证书 # 创建工作目录： mkdir -p ~/TLS/{etcd,k8s} \u0026amp;\u0026amp; cd TLS/etcd # 自签CA： cat \u0026gt; ca-config.json \u0026lt;\u0026lt; EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;www\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF cat \u0026gt; ca-csr.json \u0026lt;\u0026lt; EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd CA\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot; } ] } EOF # 使用自签CA签发Etcd HTTPS证书 cat \u0026gt; server-csr.json \u0026lt;\u0026lt; EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.81\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BeiJing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;BeiJing\u0026quot; } ] } EOF 下载源码及配置 wget https://github.com/etcd-io/etcd/releases/download/v3.4.11/etcd-v3.4.11-linux-amd64.tar.gz mkdir /opt/etcd/{bin,cfg,ssl} -p tar zxvf etcd-v3.4.11-linux-amd64.tar.gz mv etcd-v3.4.11-linux-amd64/{etcd,etcdctl} /opt/etcd/bin/ cat \u0026gt; /opt/etcd/cfg/etcd.conf \u0026lt;\u0026lt; EOF #[Member] ETCD_NAME=\u0026quot;etcd-1\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://192.168.1.81:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://192.168.1.81:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://192.168.1.81:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://192.168.1.81:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://192.168.1.81:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; EOF systemd管理etcd cat \u0026gt; /usr/lib/systemd/system/etcd.service \u0026lt;\u0026lt; EOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify ExecStart=/opt/etcd/bin/etcd \\ --name=etcd-1 \\ --data-dir=/var/lib/etcd/default.etcd \\ --listen-peer-urls=https://192.168.1.81:2380 \\ --listen-client-urls=https://192.168.1.81:2379 \\ --initial-advertise-peer-urls=https://192.168.1.81:2380 \\ --initial-cluster='etcd-1=https://192.168.1.81:2380' \\ --initial-cluster-state='new' \\ --initial-cluster-token='etcd-cluster' \\ --advertise-client-urls='https://192.168.1.81:2379' \\ --cert-file=/opt/etcd/ssl/server.pem \\ --key-file=/opt/etcd/ssl/server-key.pem \\ --peer-cert-file=/opt/etcd/ssl/server.pem \\ --peer-key-file=/opt/etcd/ssl/server-key.pem \\ --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --logger=zap Restart=on-failure LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF export PATH=/opt/etcd/bin:$PATH #验证集群状态 etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints=\u0026quot;https://192.168.1.81:2379\u0026quot; endpoint health docker apt install docker.io cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://30l6yhjq.mirror.aliyuncs.com\u0026quot; ], \u0026quot;exec-opts\u0026quot;: [ \u0026quot;native.cgroupdriver=systemd\u0026quot; ] } EOF systemctl daemon-reload systemctl restart docker systemctl enable docker kube-apiserver 自签证书颁发机构 cd ~/TLS/k8s cat \u0026gt; ca-config.json \u0026lt;\u0026lt; EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;kubernetes\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF cat \u0026gt; ca-csr.json \u0026lt;\u0026lt; EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;k8s\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 自签apiserver https证书 cat \u0026gt; server-csr.json \u0026lt;\u0026lt; EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;10.0.0.1\u0026quot;, \u0026quot;127.0.0.1\u0026quot;, \u0026quot;192.168.1.81\u0026quot;, \u0026quot;kubernetes\u0026quot;, \u0026quot;kubernetes.default\u0026quot;, \u0026quot;kubernetes.default.svc\u0026quot;, \u0026quot;kubernetes.default.svc.cluster\u0026quot;, \u0026quot;kubernetes.default.svc.cluster.local\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BeiJing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;BeiJing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;k8s\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF 启用 TLS Bootstrapping 机制,创建token文件 cat \u0026gt; /opt/kubernetes/cfg/token.csv \u0026lt;\u0026lt; EOF c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,\u0026quot;system:node-bootstrapper\u0026quot; EOF apiserver wget https://dl.k8s.io/v1.18.8/kubernetes-server-linux-amd64.tar.gz mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs} tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin # 拷贝二进制文件 cp kube-apiserver kube-scheduler kube-controller-manager kubectl /opt/kubernetes/bin # 拷贝证书 cp ~/TLS/k8s/ca*pem ~/TLS/k8s/server*pem /opt/kubernetes/ssl/ systemd管理apiserver cat \u0026gt; /usr/lib/systemd/system/kube-apiserver.service \u0026lt;\u0026lt; EOF [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/opt/kubernetes/bin/kube-apiserver \\ --logtostderr=false \\ --v=2 \\ --log-dir=/opt/kubernetes/logs \\ --etcd-servers=https://192.168.1.81:2379 \\ --bind-address=192.168.1.81 \\ --secure-port=6443 \\ --advertise-address=192.168.1.81 \\ --allow-privileged=true \\ --service-cluster-ip-range=10.0.0.0/24 \\ --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\ --authorization-mode=RBAC,Node \\ --enable-bootstrap-token-auth=true \\ --token-auth-file=/opt/kubernetes/cfg/token.csv \\ --service-node-port-range=20000-40000 \\ --kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \\ --kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \\ --tls-cert-file=/opt/kubernetes/ssl/server.pem \\ --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\ --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/opt/etcd/ssl/ca.pem \\ --etcd-certfile=/opt/etcd/ssl/server.pem \\ --etcd-keyfile=/opt/etcd/ssl/server-key.pem \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/opt/kubernetes/logs/k8s-audit.log Restart=on-failure [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl start kube-apiserver systemctl enable kube-apiserver kube-controller-manager systemd管理controller-manager cat \u0026gt; /usr/lib/systemd/system/kube-controller-manager.service \u0026lt;\u0026lt; EOF [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/opt/kubernetes/bin/kube-controller-manager \\ --logtostderr=false \\ --v=2 \\ --log-dir=/opt/kubernetes/logs \\ --leader-elect=true \\ --master=127.0.0.1:8080 \\ --bind-address=127.0.0.1 \\ --allocate-node-cidrs=true \\ --cluster-cidr=10.244.0.0/16 \\ --service-cluster-ip-range=10.0.0.0/24 \\ --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/opt/kubernetes/ssl/ca.pem \\ --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --experimental-cluster-signing-duration=87600h0m0s Restart=on-failure [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl start kube-controller-manager systemctl enable kube-controller-manager kube-scheduler cat \u0026gt; /usr/lib/systemd/system/kube-scheduler.service \u0026lt;\u0026lt; EOF [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/opt/kubernetes/bin/kube-scheduler \\ --logtostderr=false \\ --v=2 \\ --log-dir=/opt/kubernetes/logs \\ --leader-elect \\ --master=127.0.0.1:8080 \\ --bind-address=127.0.0.1 Restart=on-failure [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl start kube-scheduler systemctl enable kube-scheduler 至此，master所需组件已经安装完毕\nkubelet 拷贝二进制文件 mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs} cd kubernetes/server/bin cp kubelet kube-proxy /opt/kubernetes/bin 配置参数文件 cat \u0026gt; /opt/kubernetes/cfg/kubelet-config.yml \u0026lt;\u0026lt; EOF kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 address: 0.0.0.0 port: 10250 readOnlyPort: 10255 cgroupDriver: cgroupfs clusterDNS: - 10.0.0.2 clusterDomain: cluster.local failSwapOn: false authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% maxOpenFiles: 1000000 maxPods: 110 EOF 生成bootstrap.kubeconfig文件 KUBE_APISERVER=\u0026quot;https://192.168.1.81:6443\u0026quot; # apiserver IP:PORT TOKEN=\u0026quot;c47ffb939f5ca36231d9e3121a252940\u0026quot; # 与token.csv里保持一致 # 生成 kubelet bootstrap kubeconfig 配置文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-credentials \u0026quot;kubelet-bootstrap\u0026quot; \\ --token=${TOKEN} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=\u0026quot;kubelet-bootstrap\u0026quot; \\ --kubeconfig=bootstrap.kubeconfig kubectl config use-context default --kubeconfig=bootstrap.kubeconfig cp bootstrap.kubeconfig /opt/kubernetes/cfg systemd管理kubelet cat \u0026gt; /usr/lib/systemd/system/kubelet.service \u0026lt;\u0026lt; EOF [Unit] Description=Kubernetes Kubelet After=docker.service [Service] ExecStart=/opt/kubernetes/bin/kubelet \\ --logtostderr=false \\ --v=2 \\ --log-dir=/opt/kubernetes/logs \\ --hostname-override=km1 \\ --network-plugin=cni \\ --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\ --bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\ --config=/opt/kubernetes/cfg/kubelet-config.yml \\ --cert-dir=/opt/kubernetes/ssl \\ --pod-infra-container-image=easzlab/pause-amd64:3.2 Restart=on-failure LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl start kubelet systemctl enable kubelet 批准kubelet证书申请并加入集群 kubectl get csr kubectl certificate approve node-csr-1hBi056TVssBlAKSC2wQgcqeJNdY9cTJKS8Ytziub24 kubectl get node 由于cni没有部署，所以node is NotReady\nkube-proxy 配置proxy参数 cat \u0026gt; /opt/kubernetes/cfg/kube-proxy-config.yml \u0026lt;\u0026lt; EOF kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 metricsBindAddress: 0.0.0.0:10249 clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig hostnameOverride: km1 clusterCIDR: 10.0.0.0/24 EOF 生成kube-proxy.kubeconfig # 切换工作目录 cd ~/TLS/k8s # 创建证书请求文件 cat \u0026gt; kube-proxy-csr.json \u0026lt;\u0026lt; EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-proxy\u0026quot;, \u0026quot;hosts\u0026quot;: [], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BeiJing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;BeiJing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;k8s\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF # 生成证书 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 生成kube-proxy.kubeconfig文件 KUBE_APISERVER=\u0026quot;https://192.168.1.81:6443\u0026quot; kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \\ --client-certificate=./kube-proxy.pem \\ --client-key=./kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig cp kube-proxy.kubeconfig /opt/kubernetes/cfg/ systemd管理kube-proxy cat \u0026gt; /usr/lib/systemd/system/kube-proxy.service \u0026lt;\u0026lt; EOF [Unit] Description=Kubernetes Proxy After=network.target [Service] ExecStart=/opt/kubernetes/bin/kube-proxy \\ --logtostderr=false \\ --v=2 \\ --log-dir=/opt/kubernetes/logs \\ --config=/opt/kubernetes/cfg/kube-proxy-config.yml Restart=on-failure LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl start kube-proxy systemctl enable kube-proxy 部署CNI网络 wget https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz mkdir -p /opt/cni/bin tar zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i -r \u0026quot;s#quay.io/coreos/flannel:.*-amd64#lizhenliang/flannel:v0.12.0-amd64#g\u0026quot; kube-flannel.yml kubectl apply -f kube-flannel.yml 正常情况下 node 已经Ready\n授权apiserver访问kubelet cat \u0026gt; apiserver-to-kubelet-rbac.yaml \u0026lt;\u0026lt; EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics - pods/log verbs: - \u0026quot;*\u0026quot; --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \u0026quot;\u0026quot; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF kubectl apply -f apiserver-to-kubelet-rbac.yaml 部署core dns cat \u0026gt; coredns.yaml \u0026lt;\u0026lt; EOF apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \u0026quot;CriticalAddonsOnly\u0026quot; operator: \u0026quot;Exists\u0026quot; nodeSelector: beta.kubernetes.io/os: linux affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: k8s-app operator: In values: [\u0026quot;kube-dns\u0026quot;] topologyKey: kubernetes.io/hostname containers: - name: coredns image: coredns/coredns:1.7.0 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \u0026quot;-conf\u0026quot;, \u0026quot;/etc/coredns/Corefile\u0026quot; ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \u0026quot;9153\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: \u0026quot;true\u0026quot; kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: selector: k8s-app: kube-dns clusterIP: 10.0.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP EOF kubectl apply -f coredns.yaml DNS解析测试 kubectl run -it --rm dns-test --image=busybox:1.28.4 sh nslookup kubernetes # 正常情况下会显示 Server: 10.0.0.2 Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local 总结 etcd和api server都是只用https通讯, 所以要注意证书过期时间\netcd只有api server访问\ncontroller-manager，scheduler ，kubelet会访问api server，采用http 长连接方式watch apiserver状态变化，使用短连接报告apiserver自身状态\n","id":24,"section":"posts","summary":"requirment 生产环境的部署Kubernetes集群方案 kubeadm Kubeadm是一个K8s部署工具，提供kubeadm init和kubeadm join，用于快","tags":["k8s"],"title":"二进制安装k8s","uri":"https://blog.yoogo.top/2020/08/%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85k8s/","year":"2020"},{"content":"install(ubuntu 20) install command # 查看cpu是否支持虚拟化，大于0即为支持 grep -Eoc '(vmx|svm)' /proc/cpuinfo sudo apt update \u0026amp;\u0026amp; sudo apt install cpu-checker -y kvm-ok sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager sudo systemctl is-active libvirtd # 按需调整addresses，nameservers # 你的网卡名可能不是enp1s0 cat \u0026gt; /etc/netplan/00-installer-config.yaml \u0026lt;\u0026lt;EOF network: bridges: kvmbr0: interfaces: [enp7s0] dhcp4: no dhcp6: no addresses: [192.168.1.50/24] gateway4: 192.168.1.1 nameservers: addresses: [223.5.5.5, 223.6.6.6] ethernets: enp7s0: dhcp4: no dhcp6: no version: 2 EOF netplan apply explain qemu-kvm: 为KVM管理程序提供硬件仿真的软件 libvirt-daemon-system: 配置文件以将libvirt守护程序作为系统服务运行。 libvirt-clients: 用于管理虚拟化平台的软件。 bridge-utils: 一组用于配置以太网桥的命令行工具。 virtinst: 一组用于创建虚拟机的命令行工具。 virt-manager: 一个易于使用的GUI界面和支持命令行工具，用于通过libvirt管理虚拟机。 Gotcha 大部分无线网卡是不支持桥接的 no cloud-init 方式制作虚拟机ubuntu20.04模板机 主机启动template虚拟机 virt-install -n u2004-demo --memory memory=8192,currentMemory=2048 --vcpus 2,maxvcpus=10,cpuset=auto -s 30 -c /data/iso/ubuntu20042.iso --hvm --os-type=generic -f /data/img/u2004-demo.img --graphics vnc,listen=0.0.0.0 --force --autostart --network bridge=kvmbr0 virt-install -n u16-demo --memory memory=8192,currentMemory=2048 --vcpus 6,maxvcpus=12,cpuset=2 -s 30 -c /data/iso/u16.iso --hvm --os-type=generic -f /data/img/u16-demo.img --graphics vnc,listen=0.0.0.0 --force --autostart --network bridge=kvmbr0 通过vnc连接进入虚拟机，装机，配置静态ip\n虚拟机基本配置\n# 设置root密码 sudo -i passwd # ssh 支持 root 连接, 将PermitRootLogin置为yes vim /etc/ssh/sshd_config service ssh restart # 使虚拟机可以通过virsh console 进入 systemctl start serial-getty@ttyS0 \u0026amp;\u0026amp; systemctl enable serial-getty@ttyS0 安装ansible apt-get update \u0026amp;\u0026amp; apt-get upgrade -y \u0026amp;\u0026amp; apt-get dist-upgrade -y \u0026amp;\u0026amp; apt-get install python2.7 ln -s /usr/bin/python2.7 /usr/bin/python curl https://bootstrap.pypa.io/get-pip.py --output get-pip.py \u0026amp;\u0026amp; python get-pip.py pip install ansible==2.6.18 netaddr==0.7.19 -i https://mirrors.aliyun.com/pypi/simple/ 推荐的配置\na. Vim -\u0026gt; set paste\n复制虚拟机及需要更改的配置 复制虚拟机,虚拟机配置\nvirt-clone -o u2004-demo -n ceph-1 -f /data/vmdisk/ceph-1.img \u0026amp;\u0026amp; \\ virt-clone -o u2004-demo -n ceph-2 -f /data/vmdisk/ceph-2.img \u0026amp;\u0026amp; \\ virt-clone -o u2004-demo -n ceph-3 -f /data/vmdisk/ceph-3.img \u0026amp;\u0026amp; \\ virt-clone -o u2004-demo -n k8s-n2 -f /data/vmdisk/k8s-n2.img virsh start k8s-m1 \u0026amp;\u0026amp; \\ virsh start k8s-m2 \u0026amp;\u0026amp; \\ virsh start k8s-n1 \u0026amp;\u0026amp; \\ virsh start k8s-n2 hostnamectl set-hostname harbor \u0026amp;\u0026amp; \\ cat \u0026gt; /etc/netplan/00-installer-config.yaml \u0026lt;\u0026lt;EOF network: ethernets: ens3: addresses: - 192.168.1.152/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.150 - 223.5.5.5 version: 2 EOF netplan apply virsh snapshot-revert k8s-master 1596347792 \u0026amp;\u0026amp; \\ virsh snapshot-revert k8s-worker1 1596347848 \u0026amp;\u0026amp; \\ virsh snapshot-revert k8s-worker2 1596347890 \u0026amp;\u0026amp; \\ virsh snapshot-revert k8s-worker3 1596347927 为虚拟机增加额外硬盘 qemu-img create -f qcow2 /data/vmdisk/ceph-attach1.img 20G \u0026lt;disk type='file' device='disk'\u0026gt; \u0026lt;driver name='qemu' type='qcow2' cache='none'/\u0026gt; \u0026lt;source file='/data/vmdisk/ceph2-attach1.img'/\u0026gt; \u0026lt;target dev='vda' bus='virtio'/\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;disk type='file' device='disk'\u0026gt; \u0026lt;driver name='qemu' type='qcow2' cache='none'/\u0026gt; \u0026lt;source file='/data/vmdisk/ceph2-attach2.img'/\u0026gt; \u0026lt;target dev='vdb' bus='virtio'/\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;disk type='file' device='disk'\u0026gt; \u0026lt;driver name='qemu' type='qcow2' cache='none'/\u0026gt; \u0026lt;source file='/data/vmdisk/ceph2-attach3.img'/\u0026gt; \u0026lt;target dev='vdc' bus='virtio'/\u0026gt; \u0026lt;/disk\u0026gt; 踩过的坑 硬盘容量超过一定百分比会出现虚拟机paused状态，且resume无效，仍是paused ","id":25,"section":"posts","summary":"install(ubuntu 20) install command # 查看cpu是否支持虚拟化，大于0即为支持 grep -Eoc '(vmx|svm)' /proc/cpuinfo sudo apt update \u0026amp;\u0026amp; sudo apt install cpu-checker -y kvm-ok sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager sudo systemctl is-active libvirtd # 按需调整addresses，nam","tags":["linux"],"title":"kvm install","uri":"https://blog.yoogo.top/2020/07/kvm/","year":"2020"},{"content":" vim /etc/netplan/00-installer-config.yaml network: renderer: NetworkManager wifis: wlp2s0: dhcp4: no dhcp6: no addresses: - 192.168.1.60/24 gateway4: 192.168.1.1 nameservers: addresses: - 223.5.5.5 - 223.6.6.6 access-points: \u0026quot;yoogo5g\u0026quot;: password: \u0026quot;123\u0026quot; 安装插件 sudo apt-get install wpasupplicant network-manager 应用 netplan generate netplan apply ","id":26,"section":"posts","summary":"vim /etc/netplan/00-installer-config.yaml network: renderer: NetworkManager wifis: wlp2s0: dhcp4: no dhcp6: no addresses: - 192.168.1.60/24 gateway4: 192.168.1.1 nameservers: addresses: - 223.5.5.5 - 223.6.6.6 access-points: \u0026quot;yoogo5g\u0026quot;: password: \u0026quot;123\u0026quot; 安装插件 sudo apt-get install wpasupplicant network-manager 应用 netplan generate netplan apply","tags":["linux"],"title":"在有网络的情况下配置linux的wifi","uri":"https://blog.yoogo.top/2020/07/%E5%9C%A8%E6%9C%89%E7%BD%91%E7%BB%9C%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E9%85%8D%E7%BD%AElinux%E7%9A%84wifi/","year":"2020"},{"content":"helm 本身 install # 不推荐，慢如蜗牛 curl https://helm.baltorepo.com/organization/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \u0026quot;deb https://baltocdn.com/helm/stable/debian/ all main\u0026quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # 快如闪电 sudo snap install helm --classic helm repo add apphub https://apphub.aliyuncs.com 获取charts渲染后的yml helm create test cd test helm install --debug --generate-name --dry-run test . helm template test . helm \u0026amp; ns helm与kubectl一样，默认namespace为default，需要手动指定all\nhelm ls --all-namespaces --all helm ls -A -a helm cli \u0026amp; harbor 安装harbor时 ./install.sh --with-chartmuseum 使其开启chartmuseum功能 安装helm push 插件 helm plugin install https://github.com/chartmuseum/helm-push 详细文档 坑点1，helm repo add 自定义库时可能会发生‘503 Service Unavailable’，重启终端即可 坑点2，helm push 一定要注意version的一致性，否则harbor会显示HELM_CHART.NO_DETAIL 安装某些组件 kong helm repo add kong https://charts.konghq.com helm repo update # 如果下载不成功就clone github.com/kong/charts,并且把require lock相关删掉 helm install kong . \\ --set ingressController.installCRDs=false \\ --set admin.enabled=true,admin.http.enabled=true \\ --set manager.ingress.enabled=true,manager.ingress.hostname=kong.cc \\ -n kong --create-namespace helm install kong kong/kong --set admin.enabled=true --set admin.http.enabled=true --set postgresql.enabled=true konga helm install konga-pg . -n kong --set ingress.enabled=true,ingress.hosts[0].host=konga.cc,ingress.hosts[0].paths[0]=/ # konga 不支持pg10+ config.node_env=production,config.db_adapter=postgres,config.db_host=pg-serivce.pg,config.db_port=5432,config.db_user=postgres,config.db_password=pgsql@123,config.db_uri=postgresql://pg-serivce.pg:5432/konga_database,config.ssl_key_path=nil,config.ssl_crt_path=nil,config.token_secret=yoogosecret Rancher helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=rancher.yoogo.vip --set ingress.tls.source=tls-rancher-ingress\t--set addLocal=\u0026quot;false\u0026quot; helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set hostname=rancher.yoogo.vip,replicas=1,tls=external metallb kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml cat \u0026gt; metallb-ns.yml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: name: metallb-system labels: app: metallb EOF kubectl apply -f metallb-ns.yml cat \u0026gt; metallb-config.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.220-192.168.1.229 EOF kubectl apply -f metallb-config.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml # On first install only kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026quot;$(openssl rand -base64 128)\u0026quot; traefik helm repo add traefik https://helm.traefik.io/traefik helm install traefik traefik/traefik -n kube-system --set service.loadBalancerIP=192.168.1.220 cat \u0026gt; dashboard.yaml \u0026lt;\u0026lt;EOF # dashboard.yaml apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard spec: entryPoints: - web routes: - match: Host(\\`traefik.yoogo.vip\\`) \u0026amp;\u0026amp; (PathPrefix(\\`/dashboard\\`) || PathPrefix(\\`/api\\`)) kind: Rule services: - name: api@internal kind: TraefikService EOF kubectl apply -f dashboard.yaml nginx-ingress-controller helm install nginx-ingress-controller bitnami/nginx-ingress-controller -n nginx-system --create-namespace --set service.loadBalancerIP=192.168.1.220 redis helm install redis bitnami/redis \\ --set auth.enabled=false \\ --set global.storageClass=local-path \\ --set master.persistence.size=1Gi \\ --set master.service.nodePort=32001,master.service.type=NodePort \\ --set architecture=standalone -n redis --create-namespace pg \u0026amp; debezium cat \u0026gt; extended.conf \u0026lt;\u0026lt;EOF # extended.conf wal_level = logical max_wal_senders = 1 max_replication_slots = 1 EOF kubectl create configmap --namespace postgres --from-file=extended.conf postgresql-config helm install postgres bitnami/postgresql --set image.repository=debezium/postgres,image.tag=12,global.postgresql.postgresqlPassword=postgres,global.storageClass=local-path,service.type=NodePort,service.nodePort=35432,persistence.size=1Gi,extendedConfConfigMap=postgresql-config -n postgres --create-namespace chartmuseum helm install chart apphub/chartmuseum --set persistence.enabled=true,persistence.storageClass=fast,persistence.size=2Gi,ingress.enabled=true,ingress.hosts[0].name='charts.yoogo.cc' -n env --create-namespace kafka helm upgrade -i kafka --namespace kafka bitnami/kafka --create-namespace \\ --set external.enabled=true \\ --set global.storageClass=local-path,persistence.size=50Gi \\ --set zookeeper.persistence.size=10Gi \\ --set livenessProbe.initialDelaySeconds=100 \\ --set livenessProbe.timeoutSeconds=100,readinessProbe.initialDelaySeconds=100 \\ --set readinessProbe.timeoutSeconds=100 \\ --set externalAccess.enabled=true \\ --set externalAccess.service.type=NodePort \\ --set externalAccess.service.nodePorts[0]=29094 \\ --set externalAccess.service.useHostIPs=true jenkins helm install jenkins jenkins/jenkins -n jenkins --create-namespace --set persistence.storageClass=fast,persistence.size=1Gi,master.ingress.enabled=true,master.ingress.hostName=jenkins.yoogo.cc,master.ingress.path=/ mysql helm install mysql bitnami/mysql -n mysql --create-namespace --set global.storageClass=local-path,auth.rootPassword=mysql nacos helm install nacos . -n nacos --create-namespace \\ --set mysql.persistence.enabled=true \\ --set global.mode=cluster,persistence.enabled=true \\ --set ingress.enabled=true \\ --set ingress.hosts[0].host='nacos.yoogo.cc',ingress.hosts[0].paths[0]='/' \\ --set service.rpcPort=37848 \\ --set resources.requests.cpu=100m,resources.requests.memory=128Mi longhorn helm install longhorn longhorn/longhorn --namespace longhorn-system --set ingress.enabled=true,ingress.host=longhorn.yoogo.cc,csi.kubeletRootDir=/var/lib/kubelet kafdrop helm upgrade -i kafdrop chart --set kafka.brokerConnect=kafka-headless:9092,server.servlet.contextPath=\u0026quot;/\u0026quot;,cmdArgs=\u0026quot;--message.format=AVRO --schemaregistry.connect=http://localhost:8080\u0026quot;,jvm.opts=\u0026quot;-Xms32M -Xmx64M\u0026quot;,ingress.enabled=true,ingress.hosts[0]='kafdrop.yoogo.cc' -n kafka es helm install elasticsearch apphub/elasticsearch -n elasticsearch --create-namespace --set global.storageClass=longhorn,master.service.nodePort=30003,master.persistence.size=2Gi,master.livenessProbe.enabled=false,master.readinessProbe.enabled=false,coordinating.replicas=1,coordinating.readinessProbe.enabled=false,coordinating.livenessProbe.enabled=false,data.replicas=1,data.persistence.size=2Gi,data.livenessProbe.enabled=false,data.readinessProbe.enabled=false,global.kibanaEnabled=true,kibana.global.storageClass=longhorn,kibana.persistence.size=2Gi,kibana.livenessProbe.enabled=fasle,kibana.readinessProbe.enabled=false,kibana.ingress.enabled=true,kibana.ingress.hosts[0].name=kibana.yoogo.cc gitlab 不建议gitlab放在k8s上\nhelm install gitlab gitlab/gitlab -n gitlab --create-namespace \\ --set global.hosts.domain=yoogo.cn \\ --set global.ingress.configureCertmanager=false,certmanager.install=false \\ --set prometheus.install=false,global.grafana.enabled=false \\ --set nginx-ingress.enabled=false \\ --set global.ingress.tls.enabled=false,global.hosts.https=false \\ --set global.ingress.class=nginx loki stack helm upgrade --install loki grafana/loki-stack \\ --set fluent-bit.enabled=true,promtail.enabled=false \\ --set grafana.enabled=true,prometheus.enabled=true \\ --set prometheus.alertmanager.persistentVolume.enabled=false \\ --set prometheus.server.persistentVolume.enabled=false \\ --set loki.persistence.enabled=true \\ --set grafana.persistence.enabled=true \\ --set grafana.ingress.enabled=true \\ --set grafana.ingress.hosts[0]=grafana.yoogo.cc \\ -n loki --create-namespace Kiali helm upgrade -i kiali-server -n istio-system \\ --set server.web_fqdn=kiali.yoogo.cc,auth.strategy=anonymous \\ --repo https://kiali.org/helm-charts kiali-server kubeapps helm upgrade -i kubeapps --namespace kubeapps bitnami/kubeapps \\ --set ingress.enabled=true,ingress.hostname=kubeapps.yoogo.cc \\ --set postgresql.persistence.enabled=true \\ --set postgresql.resources.requests={} Fluentd helm upgrade -i fluentd --namespace monitor bitnami/fluentd -f fluentd-helm-values.yaml prometheus helm install kube-prometheus bitnami/kube-prometheus \\ --set prometheus.ingress.enabled=true \\ --set prometheus.ingress.hostname=prometheus.yoogo.cc \\ -n monitor --create-namespace cat\u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: monitor --- apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus namespace: monitor spec: serviceMonitorSelector: matchLabels: team: frontend serviceAccountName: prometheus resources: requests: {} EOF local-path-provisioner gh repo clone rancher/local-path-provisioner \u0026amp;\u0026amp; cd local-path-provisioner/deploy/chart/local-path-provisioner/ helm install local-path-provisioner . --namespace local-path-storage --create-namespace --set storageClass.defaultClass=true ","id":27,"section":"posts","summary":"helm 本身 install # 不推荐，慢如蜗牛 curl https://helm.baltorepo.com/organization/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \u0026quot;deb https://baltocdn.com/helm/stable/debian/ all main\u0026quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # 快如闪电 sudo snap install helm --classic helm repo add apphub https://apphub.aliyuncs.com 获取charts渲染后的ym","tags":["k8s"],"title":"helm","uri":"https://blog.yoogo.top/2020/07/helm/","year":"2020"},{"content":"requirment k8s dns组件 helm 持久化存储 Ingress Controller(nginx-ingress/traefik) Gitlab 代码管理仓库 ","id":28,"section":"posts","summary":"requirment k8s dns组件 helm 持久化存储 Ingress Controller(nginx-ingress/traefik) Gitlab 代码管理仓库","tags":["k8s"],"title":"jenkins4k8s","uri":"https://blog.yoogo.top/2020/07/jenkins4k8s/","year":"2020"},{"content":"删除节点 # 主机执行 kubectl delete node k8s-worker-x # node机执行 sudo kubeadm reset 常用查看命令 kubectl get nodes kubectl describe node xxx 好用的配置 # 永久保存该上下文中所有后续 kubectl 命令使用的命名空间 kubectl config set-context --current --namespace=\u0026lt;insert-namespace-name-here\u0026gt; 踩到的坑 莫名ingress映射的host访问不通 node单节点ping不通：sudo /etc/init.d/networking restart thinking ###万物皆容器\nMaster 关键进程： Kubernetes API Server (kube-apiserver)：提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程 Kubernetes Controller Manager (kube-controller-manager): Kubernetes里所有的资源对象的自动化控制中心 Kubernetes Scheduler (kube-scheduler)：负责资源调度(Pod调度)的进程 etcd: Kubernetes里的所有资源对象的数据全部是保存在etcd中 node 关键进程 **kubelet：**负责Pod对应容器的创建、停止等任务，同时与Master节点密切协作，实现集群管理的基本功能 **kube-proxy：**实现Kubernetes Service的通信与负载均衡机制的重要组件。 **Docker Engine(Docker)：**Docker引擎，负责本机的容器创建和管理工作。 知识点 每创建一个 Service 时，会创建一个相应的 DNS 条目， 访问形式：\u0026lt;service-name\u0026gt;.\u0026lt;namespace-name\u0026gt;.svc.cluster.local\n如果pvc在某些条件下删除以后一直处于terminating，则很有可能是因为存在finalizers，尝试清空kubectl patch pvc db-pv-claim -p '{\u0026quot;metadata\u0026quot;:{\u0026quot;finalizers\u0026quot;:null}}'，应该会被删除掉\n","id":29,"section":"posts","summary":"删除节点 # 主机执行 kubectl delete node k8s-worker-x # node机执行 sudo kubeadm reset 常用查看命令 kubectl get nodes kubectl describe node xxx 好用的配置 # 永久保存该上下文中所有后续 kubectl 命令使用的命名空间 kubectl config set-context --current","tags":["k8s"],"title":"k8s基本操作","uri":"https://blog.yoogo.top/2020/05/k8s-learn/","year":"2020"},{"content":"base: 在主机、和节点需要执行（保证虚拟机/物理机内网地址稳定） sudo vim /etc/hostname # k8s-xx，目的：看起来好看 sudo vim /etc/hosts # x.x.x.x k8s-xx，目的：爽 sudo vim /etc/fstab # 将swap注释掉，目的：禁用交换分区 sudo reboot -h docker sudo apt install docker.io sudo vi /etc/docker/daemon.json 加速docker 执行sudo vi /etc/docker/daemon.json 输入：\n自己去阿里云容器镜像服务申请自己的加速地址\n{ \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;, \u0026quot;http://hub-mirror.c.163.com\u0026quot; ], \u0026quot;max-concurrent-downloads\u0026quot;: 10, \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;log-level\u0026quot;: \u0026quot;warn\u0026quot;, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;3\u0026quot; }, \u0026quot;data-root\u0026quot;: \u0026quot;/var/lib/docker\u0026quot; } sudo systemctl daemon-reload sudo systemctl restart docker sudo systemctl enable docker 安装 kubeadm、kubelet 和 kubectl sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl # 国内不能用curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list # deb https://apt.kubernetes.io/ kubernetes-xenial main # EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl init master 坑点：\nInit 需要拉镜像，网上的例子有好多竟然是把镜像拉下来然后retag解决，着实不优雅，换个镜像地址就搞定了 阿里云可能同步的没有那么快，可以通过手动指定版本解决，事实上生产环境也不会去拉latest # apiserver-advertise-address 更换成自己局域网地址 sudo kubeadm init \\ --apiserver-advertise-address=192.168.1.220 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=NumCPU \\ --kubernetes-version=v1.18.3 # 安装成功以后在worker节点执行： sudo kubeadm join 192.168.1.220:6443 --token pck7jp.l1l0oq33ue8c40eq \\ --discovery-token-ca-cert-hash sha256:e4f6763e5a9a2d7aa290cf6380a5ee3000ed2a367ca81a373d66308fef44cd36 config kubectl mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config config network kubectl apply -f https://docs.projectcalico.org/v3.15/manifests/calico.yaml lb ansible-playbook roles/ex-lb/ex-lb.yml rancher # rancher 2.5以后会自启一个k3s单节点，因此必须要开特权模式 sudo docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /var/lib/rancher/:/var/lib/rancher/ \\ -v /root/var/log/auditlog:/var/log/auditlog \\ -e CATTLE_SYSTEM_CATALOG=bundled \\ -e AUDIT_LEVEL=3 \\ --name rancher \\ --privileged=true \\ rancher/rancher rm -rf /var/lib/rancher/ \u0026amp;\u0026amp; rm -rf /root/var/log/auditlog ansible安装 不能通过二进制安装docker，issues vim /etc/ssh/sshd_config service ssh restart ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id $IPs #$IPs为所有节点地址包括自身，按照提示输入yes 和root密码 curl https://bootstrap.pypa.io/get-pip.py --output get-pip.py python get-pip.py export release=2.2.1 curl -C- -fLO --retry 3 https://github.com/easzlab/kubeasz/releases/download/${release}/easzup chmod +x ./easzup ./easzup -D helm\ncurl https://helm.baltorepo.com/organization/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \u0026quot;deb https://baltocdn.com/helm/stable/debian/ all main\u0026quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm # 或者 wget https://get.helm.sh/helm-v3.2.1-linux-amd64.tar.gz # 自己去找版本号 mv ./linux-amd64/helm /usr/bin debug 网络、dns kubectl run -it --rm --restart=Never --image=busybox busybox kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools =\u0026gt; nslookup kubernetes.default.svc.cluster.local k3s 占用资源少，但是坑还是很多的，亮点是使用sqlite代替etcd,以及自带local storage class，一定要看安装指南，不然都不知道坑（feature）是哪里来的，比如Service Load Balancer（slb）\nhttps://rancher.com/docs/k3s/latest/en/networking/\nhttps://rancher.com/docs/k3s/latest/en/installation/install-options/server-config/\nQuick start curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026quot;--disable=servicelb\u0026quot; sh - 如果网络不好可以先将二进制k3s下载至本地\n# 自己修改指定的版本号 wget https://github.com/rancher/k3s/releases/download/v1.20.4%2Bk3s1/k3s chmod 755 k3s \u0026amp; mv k3s /usr/local/bin/ curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_DOWNLOAD=true sh - cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl cat \u0026gt;\u0026gt; /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl \u0026lt;\u0026lt; EOF [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://30l6yhjq.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] EOF systemctl restart k3s cp /etc/rancher/k3s/k3s.yaml ~/.kube/config cat /var/lib/rancher/k3s/server/node-token # cri 加速 cat \u0026gt;\u0026gt; /etc/rancher/k3s/registries.yaml \u0026lt;\u0026lt;EOF mirrors: \u0026quot;docker.io\u0026quot;: endpoint: - \u0026quot;https://30l6yhjq.mirror.aliyuncs.com\u0026quot; - \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot; EOF systemctl restart k3s Add node curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.1.91:6443 K3S_TOKEN=K10f5c3ae2cd90c834a10eae8cee2225e5b9b250d64187ffd235552b77b14da365f::server:903e9a1b0b622a686dfa85710f09c848 sh - cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl cat \u0026gt;\u0026gt; /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl \u0026lt;\u0026lt; EOF [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://30l6yhjq.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] EOF systemctl restart k3s-agent.service ","id":30,"section":"posts","summary":"base: 在主机、和节点需要执行（保证虚拟机/物理机内网地址稳定） sudo vim /etc/hostname # k8s-xx，目的：看起来好看 sudo vim /etc/hosts # x.x.x.x k8s-xx，目的：爽 sudo vim /etc/fstab # 将s","tags":["k8s"],"title":"ubuntu下k8s安装","uri":"https://blog.yoogo.top/2020/05/k8s-start/","year":"2020"},{"content":"StopWatch 计时器\n","id":31,"section":"posts","summary":"StopWatch 计时器","tags":["java"],"title":"spring知识点记录","uri":"https://blog.yoogo.top/2020/05/spring%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AE%B0%E5%BD%95/","year":"2020"},{"content":"1. SpringApplication 默认情况下，会显示信息日志消息，包括一些相关的启动细节，比如启动应用程序的用户。如果您需要一个日志级别，而不是INFO，您可以设置它，如日志级别中所述。应用程序版本是使用主应用程序类包中的实现版本确定的。通过设置spring.main.log-startup-info可以关闭启动信息日志。\n1. Lazy init spring.main.lazy-initialization=true\nSpringApplication允许应用程序被延迟初始化。当启用延迟初始化时，bean是根据需要创建的，而不是在应用程序启动期间创建的。因此，启用延迟初始化可以减少应用程序启动所需的时间。在web应用程序中，启用延迟初始化将导致在接收到HTTP请求之前不会初始化许多与web相关的bean。\n延迟初始化的一个缺点是，它会延迟应用程序问题的发现。如果错误配置的bean是惰性初始化的，那么在启动期间将不再出现故障，只有在初始化bean时问题才会变得明显。还必须注意确保JVM有足够的内存来容纳应用程序的所有bean，而不仅仅是那些在启动期间初始化的bean。由于这些原因，默认情况下不会启用延迟初始化，建议在启用延迟初始化之前对JVM堆大小进行微调。\n可以使用SpringApplicationBuilder上的lazyinitialize方法或SpringApplication上的setlazyinitialize方法以编程方式启用延迟初始化。或者，也可以使用spring.main来启用它。延迟初始化属性如下例所示:\n2. 自定义Banner 支持的变量：\nVariable Description ${application.version} The version number of your application, as declared in MANIFEST.MF. For example, Implementation-Version: 1.0 is printed as 1.0. ${application.formatted-version} The version number of your application, as declared in MANIFEST.MF and formatted for display (surrounded with brackets and prefixed with v). For example (v1.0). ${spring-boot.version} The Spring Boot version that you are using. For example 2.2.6.RELEASE. ${spring-boot.formatted-version} The Spring Boot version that you are using, formatted for display (surrounded with brackets and prefixed with v). For example (v2.2.6.RELEASE). ${Ansi.NAME} (or ${AnsiColor.NAME}, ${AnsiBackground.NAME}, ${AnsiStyle.NAME}) Where NAME is the name of an ANSI escape code. See AnsiPropertySource for details. ${application.title} The title of your application, as declared in MANIFEST.MF. For example Implementation-Title: MyApp is printed as MyApp. The SpringApplication.setBanner(…) method can be used if you want to generate a banner programmatically. Use the org.springframework.boot.Banner interface and implement your own printBanner() method.\n","id":32,"section":"posts","summary":"1. SpringApplication 默认情况下，会显示信息日志消息，包括一些相关的启动细节，比如启动应用程序的用户。如果您需要一个日志级别，而不是INFO，您可以设置它，如","tags":["java"],"title":"spring官网文档-spring boot features","uri":"https://blog.yoogo.top/2020/05/spring%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3-spring-boot-features/","year":"2020"},{"content":"docker 加速 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-'EOF' { \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://1nj0zren.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;, \u0026quot;http://f1361db2.m.daocloud.io\u0026quot;, \u0026quot;https://registry.docker-cn.com\u0026quot; ] } EOF sudo systemctl daemon-reload sudo systemctl restart docker gitlab docker run --detach \\ --hostname git.yoogo.cc \\ --publish 4430:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume ~/gitlab/config:/etc/gitlab \\ --volume ~/gitlab/logs:/var/log/gitlab \\ --volume ~/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce jenkins docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkinsci/blueocean redis sudo docker run -d --name redis -p 6379:6379 redis --requirepass \u0026quot;336991\u0026quot; chrome https证书错误导致不给打开网页 在该页面顶层空白处输入thisisunsafe\n","id":33,"section":"posts","summary":"docker 加速 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-'EOF' { \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://1nj0zren.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;, \u0026quot;http://f1361db2.m.daocloud.io\u0026quot;, \u0026quot;https://registry.docker-cn.com\u0026quot; ] } EOF sudo systemctl daemon-reload sudo systemctl restart docker gitlab docker run --detach \\ --hostname git.yoogo.cc \\ --publish 4430:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume ~/gitlab/config:/etc/gitlab \\ --volume ~/gitlab/logs:/var/log/gitlab \\ --volume ~/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce jenkins docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v","tags":["devops"],"title":"常用环境-gitlab","uri":"https://blog.yoogo.top/2020/04/%E5%B8%B8%E7%94%A8%E7%8E%AF%E5%A2%83/","year":"2020"},{"content":"elasticsearch sudo docker pull elasticsearch:7.7.1 sudo docker run -p 9200:9200 -p 9300:9300 --name elasticsearch -e \u0026quot;discovery.type=single-node\u0026quot; -d elasticsearch:7.7.1 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.7.1/elasticsearch-analysis-ik-7.7.1.zip logstash sudo docker pull logstash:7.7.1 kibana sudo docker pull kibana:7.7.1 sudo docker run --link elasticsearch:elasticsearch -d -it --rm -p 5601:5601 kibana:7.7.1 rancher sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server \\ --db-host tieba.yoogo9.me --db-port 3307 --db-user root --db-pass 199633 --db-name rancher ","id":34,"section":"posts","summary":"elasticsearch sudo docker pull elasticsearch:7.7.1 sudo docker run -p 9200:9200 -p 9300:9300 --name elasticsearch -e \u0026quot;discovery.type=single-node\u0026quot; -d elasticsearch:7.7.1 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.7.1/elasticsearch-analysis-ik-7.7.1.zip logstash sudo docker pull logstash:7.7.1 kibana sudo docker pull kibana:7.7.1 sudo docker run --link elasticsearch:elasticsearch -d -it --rm -p 5601:5601 kibana:7.7.1 rancher sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server \\ --db-host tieba.yoogo9.me --db-port 3307 --db-user root --db-pass 199633 --db-name rancher","tags":["devops"],"title":"elk docker单例搭建","uri":"https://blog.yoogo.top/2020/02/elk/","year":"2020"},{"content":"1. 正常的Spring boot项目 基本的spring boot项目只需要写个配置类，实现WebMvcConfigurer接口的addCorsMappings方法(或者直接注册CorsFilter bean)\n@Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\u0026quot;/**\u0026quot;) .allowCredentials(true) .allowedHeaders(\u0026quot;*\u0026quot;) .allowedOrigins(\u0026quot;*\u0026quot;) .allowedMethods(\u0026quot;*\u0026quot;); } 这样就会给所有路由的请求加上跨域所需的headers\n2. Spring Boot + Shiro Spring Boot整合Shiro之后，默认所有请求会先经过shiro的监听器，所以上面的全局方法已经不管用了。（怎么整合shiro就不说了）\n这时候就得祭出第二招，cors监听器。实现一个监听器，放在shiro的监听器之前，这样就可以保证在所有请求到来的时候，都会给response加上跨域headers。\n@Bean public FilterRegistrationBean corsFilter() { final UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); final CorsConfiguration config = new CorsConfiguration(); // 允许cookies跨域 config.setAllowCredentials(true); // #允许向该服务器提交请求的URI，*表示全部允许，在SpringMVC中，如果设成*，会自动转成当前请求头中的Origin config.addAllowedOrigin(\u0026quot;*\u0026quot;); // #允许访问的头信息,*表示全部 config.addAllowedHeader(\u0026quot;*\u0026quot;); // 预检请求的缓存时间（秒），即在这个时间段里，对于相同的跨域请求不会再预检了 config.setMaxAge(18000L); // 允许提交请求的方法，*表示全部允许 config.addAllowedMethod(\u0026quot;OPTIONS\u0026quot;); config.addAllowedMethod(\u0026quot;HEAD\u0026quot;); config.addAllowedMethod(\u0026quot;GET\u0026quot;); config.addAllowedMethod(\u0026quot;PUT\u0026quot;); config.addAllowedMethod(\u0026quot;POST\u0026quot;); config.addAllowedMethod(\u0026quot;DELETE\u0026quot;); config.addAllowedMethod(\u0026quot;PATCH\u0026quot;); source.registerCorsConfiguration(\u0026quot;/**\u0026quot;, config); FilterRegistrationBean bean = new FilterRegistrationBean(new CorsFilter(source)); // 设置监听器的优先级 bean.setOrder(0); return bean; } ","id":35,"section":"posts","summary":"1. 正常的Spring boot项目 基本的spring boot项目只需要写个配置类，实现WebMvcConfigurer接口的addCorsMa","tags":["java"],"title":"spring 跨域坑点","uri":"https://blog.yoogo.top/2020/02/spring%E8%B7%A8%E5%9F%9F%E5%9D%91%E7%82%B9/","year":"2020"},{"content":"后台 深入了解spring ioc 掌握mb, jpa 能用es做搜索和日志 node 能写vue/react 的状态管理 能写ts 能写css ","id":36,"section":"posts","summary":"后台 深入了解spring ioc 掌握mb, jpa 能用es做搜索和日志 node 能写vue/react 的状态管理 能写ts 能写css","tags":["java"],"title":"2020 第一季度 flag","uri":"https://blog.yoogo.top/2020/02/2020-first-flag/","year":"2020"},{"content":"使用场景 公司有虚拟机可以用,但总不能在家一直挂着vpn吧,一直挂着也可以,但是vpn跟梯子是冲突的,只能开一个,这可咋整,之前用过钉钉的内网穿透服务,感觉很nice,但是现在不给随便用了,没办法,自己搭一个吧\n前期准备 一台有公网ip的服务器, 以下称为 A 一台内网服务器, 以下称为 B 优秀且简单的轮子: frp , releases ,wiki Fuck it! 两台服务器都下载各自对应版本的frp, A需要使用frps, frps.ini,B需要使用 frpc, frpc.ini 相应配置(自己可以去readme看啥意思) # frps.ini [common] bind_port = 7000 token = xxxx # frpc.ini [common] server_addr = x.x.x.x server_port = 7000 token = xxx [pg] type = tcp local_ip = 127.0.0.1 local_port = 5432 remote_port = 5432 [rabbitmq-server] type = tcp local_ip = 127.0.0.1 local_port = 5672 remote_port = 5672 [rabbitmq-dashboard] type = tcp local_ip = 127.0.0.1 local_port = 15672 remote_port = 15672 [kibana] type = tcp local_ip = 127.0.0.1 local_port = 5601 remote_port = 5601 [mongo] type = tcp local_ip = 127.0.0.1 local_port = 27017 remote_port = 27017 [elasticsearch1] type = tcp local_ip = 127.0.0.1 local_port = 9200 remote_port = 9200 [elasticsearch1] type = tcp local_ip = 127.0.0.1 local_port = 9300 remote_port = 9300 后台启动 sudo vim /lib/systemd/system/frps.service [Unit] Description=frps service After=network.target syslog.target Wants=network.target [Service] Type=simple #启动服务的命令（此处写你的frps的实际安装目录） ExecStart=/your/path/frps -c /your/path/frps.ini [Install] WantedBy=multi-user.target sudo systemctl start frps sudo systemctl restart frps sudo systemctl stop frps sudo systemctl status frps 按照官方service路径轻修改 wget https://github.com/fatedier/frp/releases/download/v0.34.3/frp_0.34.3_linux_amd64.tar.gz tar -zxvf frp_0.34.3_linux_amd64.tar.gz frp \u0026amp;\u0026amp; cd frp cp systemd/frpc.service /lib/systemd/system/ cp frpc /usr/bin/ vim frpc.ini cp frpc.ini /etc/frp/ service frpc start ","id":37,"section":"posts","summary":"使用场景 公司有虚拟机可以用,但总不能在家一直挂着vpn吧,一直挂着也可以,但是vpn跟梯子是冲突的,只能开一个,这可咋整,之前用过钉钉的内网","tags":["devops"],"title":"frp优雅实现内网穿透","uri":"https://blog.yoogo.top/2020/01/frp%E4%BC%98%E9%9B%85%E5%AE%9E%E7%8E%B0%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/","year":"2020"},{"content":" 支持的密码encoder PasswordEncoderFactories\nurl中 /不能有重复,否则会报org.springframework.security.web.firewall.RequestRejectedException: The request was rejected because the URL was not normalized.,从而触发RestAuthenticationEntryPoint\nbranner :\n_ _ ___ ___ __ _ ___ | | | | / _ \\ / _ \\ / _` | / _ \\ | |_| | | (_) | | (_) | | (_| | | (_) | \\__, | \\___/ \\___/ \\__, | \\___/ __/ | __/ | |___/ |___/ 重构多文件众多相似代码\n// 重构前 @Api(tags = \u0026quot;OmsOrderSettingController\u0026quot;, description = \u0026quot;订单设置管理\u0026quot;) // 使用正则搜索 // \\@Api\\(tags = \\\u0026quot;(\\w*)\\\u0026quot;\\, description = // \\@Api\\(value = \\\u0026quot;.*\u0026quot;, description = // 替换为 // \\@Api\\(tags = // 重构后 @Api(tags = \u0026quot;订单设置管理\u0026quot;) \\if\\(\\w*)\\ 在Springboot应用开发中使用JPA时，通常在主应用程序所在包或者其子包的某个位置定义我们的Entity和Repository，这样基于Springboot的自动配置，无需额外配置，我们定义的Entity和Repository即可被发现和使用。但有时候我们需要定义Entity和Repository不在应用程序所在包及其子包，那么这时候就需要使用@EntityScan和@EnableJpaRepositories了\nparent为spring-boot-starter-parent ,就不要再dependencyManagement引入xxx了\ndependencyManagement中引入新包,如果相关dependencies中没有使用,则maven不会去下载这个包,从而导致 Not found\n","id":38,"section":"posts","summary":"支持的密码encoder PasswordEncoderFactories url中 /不能有重复,否则会报org.springframework.security.web.firewall.","tags":["java"],"title":"spring 相关","uri":"https://blog.yoogo.top/2020/01/spring/","year":"2020"},{"content":"swagger2 swagger2作为业界优秀的轮子,其通用性和可拓展性都是首屈一指的(java界的只用过这个)\n没有对比就没有伤害 公司用ruby,现在用的文档工具是apipie, 这玩意无力吐槽,只能生成简单的文档页面,其实这个还好,大不了就是麻烦一点,但是重点:\nruby作为一门资深的动态语言,方法的入参木有列表啊,随便来啊,还要手撸一边文档才能生成文档,ruby写起来很爽,但是维护起来要命,文档年久失修或者某一次加/减了参数但没有更新文档,那接下来的人根本没法快速定位,出来混,总是要还的. 极大降低性能,众所周知,ruby是一个很神奇的语言,ruby确实能写出性能不低的代码,但是大部分人只能写出普通的代码(include me),因此稍微大一点的公司用ruby随着时间的推移系统会越来越慢,可能会是指数级的慢 可拓展性差,ruby可以做到拓展性强,但是一大堆money patch玩不来玩不来. 我现在认为的干货 ApiInfoBuilder().contact() String类型参数已经过期,推荐使用Contact类型 ApiInfoBuilder().securitySchemes() 设置请求头信息 ApiInfoBuilder().securityContexts(): private List\u0026lt;SecurityContext\u0026gt; securityContexts() { List\u0026lt;SecurityContext\u0026gt; contexts = new ArrayList\u0026lt;\u0026gt;(1); SecurityContext securityContext = SecurityContext.builder() .securityReferences(defaultAuth()) .forPaths(PathSelectors.regex(\u0026quot;^((?!login).)*$\u0026quot;)) .build(); contexts.add(securityContext); return contexts; } private List\u0026lt;SecurityReference\u0026gt; defaultAuth() { List\u0026lt;SecurityReference\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); AuthorizationScope authorizationScope = new AuthorizationScope(\u0026quot;global\u0026quot;, \u0026quot;accessEverything\u0026quot;); AuthorizationScope[] authorizationScopes = new AuthorizationScope[1]; authorizationScopes[0] = authorizationScope; result.add(new SecurityReference(tokenHeader, authorizationScopes)); return result; } ","id":39,"section":"posts","summary":"swagger2 swagger2作为业界优秀的轮子,其通用性和可拓展性都是首屈一指的(java界的只用过这个) 没有对比就没有伤害 公司用ruby,现在用的文","tags":["java"],"title":"swagger2 for spring boot","uri":"https://blog.yoogo.top/2020/01/java-swagger2/","year":"2020"},{"content":" 字符串不包含login =\u0026gt; ^((?!login).)*$ ","id":40,"section":"posts","summary":"字符串不包含login =\u0026gt; ^((?!login).)*$","tags":["正则"],"title":"有趣的正则","uri":"https://blog.yoogo.top/2020/01/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%AD%A3%E5%88%99/","year":"2020"},{"content":"变量定义 未使用的变量编译会报错 := 只能用于函数内部 数据类型 字符串是不可变的 var arr [n]type 定义数组，注意数组长度也是类型的一部分 var arr [10]int // 声明了一个int类型的数组 arr[0] = 42 // 数组下标是从0开始的 a := [3]int{1, 2, 3} // 声明了一个长度为3的int数组 b := [10]int{1, 2, 3} // 声明了一个长度为10的int数组，其中前三个元素初始化为1、2、3，其它默认为0 c := [...]int{4, 5, 6} // 可以省略长度而采用`...`的方式，Go会自动根据元素个数来计算长度 // 声明了一个二维数组，该数组以两个数组作为元素，其中每个数组中又有4个int类型的元素 doubleArray := [2][4]int{[4]int{1, 2, 3, 4}, [4]int{5, 6, 7, 8}} // 上面的声明可以简化，直接忽略内部的类型 easyArray := [2][4]int{{1, 2, 3, 4}, {5, 6, 7, 8}} slice、切片、动态数组：并不是真正意义上的动态数组，而是一个引用类型。slice总是指向一个底层array，slice的声明也可以像array一样，只是不需要长度。 map // 初始化一个字典 rating := map[string]float32{\u0026quot;C\u0026quot;:5, \u0026quot;Go\u0026quot;:4.5, \u0026quot;Python\u0026quot;:4.5, \u0026quot;C++\u0026quot;:2 } // map有两个返回值，第二个返回值，如果不存在key，那么ok为false，如果存在ok为true csharpRating, ok := rating[\u0026quot;C#\u0026quot;] if ok { fmt.Println(\u0026quot;C# is in the map and its rating is \u0026quot;, csharpRating) } else { fmt.Println(\u0026quot;We have no rating associated with C# in the map\u0026quot;) } delete(rating, \u0026quot;C\u0026quot;) // 删除key为C的元素 Tips 在Go语言中，同时声明多个常量、变量，或者导入多个包时，可采用分组的方式进行声明。 import( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; ) const( i = 100 pi = 3.1415 prefix = \u0026quot;Go_\u0026quot; ) var( i int pi float32 prefix string ) 分析ssa GOSSAFUNC=main go tool compile main.go\n","id":41,"section":"posts","summary":"变量定义 未使用的变量编译会报错 := 只能用于函数内部 数据类型 字符串是不可变的 var arr [n]type 定义数组，注意数组长度也是类型的一部分 var arr [10]int // 声明了一个int","tags":["go"],"title":"go笔记","uri":"https://blog.yoogo.top/2019/05/go%E7%AC%94%E8%AE%B0/","year":"2019"},{"content":"mac + vs code 版本go环境搭建 mac，vscode, go 准备好(官网有现成的安装包，重在配置)\n安装code的go拓展\n安装依赖包：\ngo get -u -v github.com/nsf/gocode go get -u -v github.com/rogpeppe/godef go get -u -v github.com/zmb3/gogetdoc go get -u -v github.com/golang/lint/golint go get -u -v github.com/lukehoban/go-outline go get -u -v sourcegraph.com/sqs/goreturns go get -u -v golang.org/x/tools/cmd/gorename go get -u -v github.com/tpng/gopkgs go get -u -v github.com/newhook/go-symbols go get -u -v golang.org/x/tools/cmd/guru go get -u -v github.com/cweill/gotests/ # 调试工具 go get -v -u github.com/peterh/liner github.com/derekparker/delve/cmd/dlv ","id":42,"section":"posts","summary":"mac + vs code 版本go环境搭建 mac，vscode, go 准备好(官网有现成的安装包，重在配置) 安装code的go拓展 安装依赖包： go get -u -v github.com/nsf/gocode go get -u -v github.com/rogpeppe/godef","tags":["go"],"title":"go环境配置","uri":"https://blog.yoogo.top/2019/05/go-env/","year":"2019"},{"content":" 只是一种实现的思路，照抄照搬行不通\nInstallation gem 'rubyzip', '\u0026gt;= 1.2.1' gem 'axlsx', git: 'https://github.com/randym/axlsx.git', ref: 'c8ac844' gem 'axlsx_rails' Usage controller render :xlsx, template: \u0026quot;members/index.xlsx.axlsx\u0026quot;, layout: false if params[:need_export] 这里有个小坑，一定要layout: false ，只是草草实现了功能还没看他的实现\nview 在index.erb link_to 到下载：\n\u0026lt;%= link_to '导出', members_path(request.parameters.merge({need_export: true})), class: 'btn btn-info' %\u0026gt; 增加模板 index.xlsx.axlsx\nwb = xlsx_package.workbook wb.styles do |style| heading = style.add_style(b: true) wb.add_worksheet(name: \u0026quot;Leads\u0026quot;) do |sheet| sheet.add_row %w(姓名 性别 所属城市 手机号 手机号2 邮箱 监护人名字 监护人关系 年龄 地址 街道 市场渠道分组 备注 证件号 意向球场 销售顾问 创建时间 学员状态), style: heading @members.each do |member| sheet.add_row [ member.name, tt(member.gender, 'enum.gender'), member.region_name, member.mobile, member.mobile2, member.email, member.parent_name, tt(member.parent_type, 'enum.parent_type'), member.age, member.address, member.district, member.ascription_desc, member.comment, member.credentials_no, member.court_name, member.follow_name, lt(member.created_at), member.tag.try(:name) ] end end end 然后就可以啦！Enjoy this！\n自我感觉导出模板应该也想html，json一样，具有高度定制化，不建议用Nokogiri的方式去爬，反正都是要请求一次，何必再去算一次dom呢\n参考链接： https://github.com/jasondoggart/inventory https://github.com/straydogstudio/axlsx_rails https://github.com/randym/axlsx\n","id":43,"section":"posts","summary":"只是一种实现的思路，照抄照搬行不通 Installation gem 'rubyzip', '\u0026gt;= 1.2.1' gem 'axlsx', git: 'https://github.com/randym/axlsx.git', ref: 'c8ac844' gem 'axlsx_rails' Usage controller render :xlsx, template: \u0026quot;members/index.xlsx.axlsx\u0026quot;, layout: false if params[:need_export] 这里有个小坑，一定要layout: false ，只是草草实现了功能还没","tags":["rails"],"title":"rails定制化导出","uri":"https://blog.yoogo.top/2019/05/rails%E5%AE%9A%E5%88%B6%E5%8C%96%E5%AF%BC%E5%87%BAxlsx/","year":"2019"},{"content":"##\u0008Hash#dig ruby 2.3 中引入了\u0008dig方法: 通过在每个步骤调用dig从给定key中提取嵌套参数。如果中间步骤为nil，则返回nil ❌：\n... if params[:user] \u0026amp;\u0026amp; params[:user][:address] \u0026amp;\u0026amp; params[:user][:address][:somewhere_deep] ✔️：\n... if params.dig(:user, :address, :somewhere_deep) Object#presence_in 如果接收方包含在参数中，则返回接收方，否则返回nil。参数必须是响应#include?的任何对象。 ❌：\nsort_options = [:by_date, :by_title, :by_author] ... sort = sort_options.include?(params[:sort]) ? params[:sort] : :by_date # Another option sort = (sort_options.include?(params[:sort]) \u0026amp;\u0026amp; params[:sort]) || :by_date ✔️：\nparams[:sort].presence_in(sort_options) || :by_date crush class Object def crush self end end class Array def crush r = map(\u0026amp;:crush).compact r.empty? ? nil : r end end class Hash def crush r = each_with_object({}) do |(k, v), h| if (_v = v.crush) h[k] = _v end end r.empty? ? nil : r end end preload scope 错误的案例：\nclass Comment \u0026lt; ActiveRecord::Base belongs_to :post scope :active, -\u0026gt; { where(soft_deleted: false) } end class Post \u0026lt; ActiveRecord::Base def active_comments comments.active end end 这样势必会造成post.active_comments的时候会where，导致N+1 正确的案例：\nclass Post has_many :comments has_many :active_comments, -\u0026gt; { active }, class_name: \u0026quot;Comment\u0026quot; end class Comment belongs_to :post scope :active, -\u0026gt; { where(soft_deleted: false) } end class PostsController def index @posts = Post.includes(:active_comments) end end ","id":44,"section":"posts","summary":"##\u0008Hash#dig ruby 2.3 中引入了\u0008dig方法: 通过在每个步骤调用dig从给定key中提取嵌套参数。如果中间步骤为nil，则返回nil ❌： ... if params[:user] \u0026amp;\u0026amp; params[:user][:address] \u0026amp;\u0026amp; params[:user][:address][:somewhere_deep] ✔️： ...","tags":["rails"],"title":"rails-tips","uri":"https://blog.yoogo.top/2018/12/rails-tips/","year":"2018"},{"content":"概述 计算机运行前提： 任务描述 计算机本身的运行能力 组成部分 Memory Processing Unit Input Output Control Unit 内存 结构：门电路和锁存器组成 门电路： 与或非门 锁存器：\nR-S锁存器 一开始处于静态： R = 1，S = 1 假设：输出 a= 1,则A=1, 由与非门特性得到b=0，即B=0，继而得到a=1 相反，输出 a= 0,则A=0, 由与非门特性得到b=1，即B=1，继而得到a=0 结论： 只要输入的值R S都保持为1时，输出a的值就不变 =\u0026gt; “记忆”能力或存储能力 写入：保持R=1不变，set S=0,则输出a= 1, 同理，相反亦如此。\n门控D锁存器 WE =\u0026gt; Write Enable 当且仅当 WE=1时，才使得输出值等于D\n寻址空间： 能最多用到多少内存 寻址能力：每个内存位置中包含的bit数目 32位操作系统最大可接受内存： 2^32/1024/1024/1024 = 4G 64位操作系统最大可接受内存： 2^64/1024/1024/1024 = 17179869184G 例： 4G内存条的寻址空间：2^32\n内存的读操作 向内存提供被访问内存单元的地址 将被访问内存单元的地址放入CPU的内存地址寄存器（Memory Address Register，MAR） 发送读信号通知内存 内存将该单元中存放的数据传送至内存数据寄存器（Memory Data Register，MDR） 内存的写操作 向内存提供被访问内存单元的地址 将内存单元的地址放入MAR 将写入的数据放入MDR 发送读信号通知内存 MDR的内容写入MAR指向的内存单元 处理单元： 信息真正被处理的地方 现代计算机有很多单元组成，每个功能单元负责一个功能。 最简单的功能单元\u0026ndash;ALU(Arithmetic Logical Unit)。（冯·诺依曼体系中唯一的功能单元） 功能： 基本运算add,subtract\u0026hellip;, 基本逻辑运算and,or,not 临时存储器：通常会在ALU附近配置少量寄存器，一边存放中间计算结果，以免引起不必要的内存访问。常见的设计是一组寄存器，其中每个寄存器宽度与ALU能处理数据的宽度一致。\nIO单元 输入：键盘，鼠标，硬盘。。。。。 输出：显示器，硬盘。。。。。\n控制单元： 控制其他单元协同工作。 在计算机程序逐步执行过程中，既控制程序执行的每一步，又负责控制其中每条指令执行过程的每一步。\n最重要的一个结构：有限状态机 有限个状态以及在这些状态之间的转移和动作等行为的数学模型。 组成：\n状态（有限数目） 外部输入（有限数目） 对外输出（有限数目） 一个简单的状态图： 其中特殊的寄存器：\n1. 指令寄存器（instruction register）：保存正在执行的指令 2. PC寄存器（program counter）:保存下一条被执行的指令 program counter or instruction pointer ：程序计数器 还是 指令指针？\n指令处理 冯·诺依曼的核心思想是：程序和数据都是以bit流形式存放在计算机内存中，程序在控制单元的控制下，依次完成指令的读取和执行。\n指令：计算机最小执行单位，由操作码和操作数组成 指令周期 1. 取指令FETCH (1) PC-(load)-\u0026raquo; MAR (2) MAR -\u0026gt; MDR (3) 控制单元将MDR内容装入IR\n2. 译码DECODE IR指令寄存器中的指令会发送到指令译码部件。\n3. 地址计算EVALUATE ADDRESS 如果存在地址计算，则执行此步骤\n4. 取操作数FETCH OPERAND 读取指令处理所需要的源操作数\n5. 执行EXECUTE 执行需要处理单元控制的操作\n6. 存放结果STORE RESULT 将之前的执行结果写入目的寄存器\n示例（8个寄存器R0~R7，且寻址模式为基址+偏移量的计算机） 给定一条加操作命令：\n0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 add R6 R2 第二个源操作数寻址方式 占位 R6 给定一条LDR操作命令：\n0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 LDR R1 R2 X1D ","id":45,"section":"posts","summary":"概述 计算机运行前提： 任务描述 计算机本身的运行能力 组成部分 Memory Processing Unit Input Output Control Unit 内存 结构：门电路和锁存器组成 门电路： 与或非门 锁存器： R-S锁存器 一开始","tags":["体系结构"],"title":"冯·诺依曼模型","uri":"https://blog.yoogo.top/2018/12/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC%E6%A8%A1%E5%9E%8B/","year":"2018"},{"content":"计算思维 It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use. 它代表了一种普遍适用的态度和技能，每个人，不仅仅是计算机科学家，将渴望学习和使用。\nComputational thinking builds on the po limits of computing omputational thinking wer and processes, whether they are executed by a human or by a machine. Computational methods and models give us the courage to solve problems and design systems that no one of us would be capable of tackling alone. Computational thinking confronts the riddle of machine intelligence: What can humans do better than computers? and What can computers do better than humans? Most fundamentally it addresses the question: What is computable? Today, we know only parts of the answers to such questions. 计算思维建立在计算过程的能力和限制之上，无论这些计算过程是由人还是由机器执行的。计算方法和模型给了我们解决问题和设计系统的勇气，这些问题和系统是我们任何人都无法单独解决的。计算思维面临着机器智能的谜题:人类有什么能比计算机做得更好?还有什么电脑能比人类做得更好呢?最根本的问题是:什么是可计算的?今天，我们只知道这些问题的部分答案。 Computational thinking is a fundamental skill for everyone, not just for computer scientists. To reading, writing, and arithmetic, we should add computational thinking to every child’s analytical ability. Just as the printing press facilitated the spread of the three Rs[1], what is appropriately incestuous about this vision is that computing and computers facilitate the spread of computational thinking. 计算思维是每个人的基本技能，而不仅仅是计算机科学家。除了阅读、写作和算术，我们还应该在每个孩子的分析能力中加入计算思维。正如印刷术促进了三个r的传播一样，计算和计算机促进了计算思维的传播。 Computational thinking involves solving problems, designing systems, and understanding human behavior, by drawing on the concepts fundamental to computer science. Computational thinking includes a range of mental tools that reflect the breadth of the field of computer science. 计算思维包括利用计算机科学的基本概念解决问题、设计系统和理解人类行为。计算思维包括一系列反映计算机科学领域广度的思维工具。 Having to solve a particular problem, we might ask: How difficult is it to solve? and What’s the best way to solve it? Computer science rests on solid theoretical underpinnings to answer such questions precisely. Stating the difficulty of a problem accounts for the underlying power of the machine—the computing device that will run the solution. We must consider the machine’s instruction set, its resource constraints, and its operating environment. 要解决一个特定的问题，我们可能会问:它有多难解决?最好的解决办法是什么?计算机科学为准确回答这些问题提供了坚实的理论基础。描绘问题的难度解释了机器的潜在能力 \u0026ndash; 将运行解决方案的计算设备。我们必须考虑机器的指令集、资源约束和操作环境。 In solving a problem efficiently, we might further ask whether an approximate solution is good enough, whether we can use randomization to our advantage, and whether false positives or false negatives are allowed. Computational thinking is reformulating a seemingly difficult problem into one we know how to solve, perhaps by reduction, embedding, transformation, or simulation. 在真正地解决一个问题时，我们可能会进一步问近似解是否足够好，是否可以利用随机化的优势，是否允许假阳性或假阴性。计算思维是将一个看似困难的问题重新表述成一个我们知道如何解决的问题，也许是通过约简、嵌入、转换或模拟。 Computational thinking is thinking recursively. It is parallel processing. It is interpreting code as data and data as code. It is type checking as the generalization of dimensional analysis. It is recognizing both the virtues and the dangers of aliasing, or giving someone or something more than one name. It is recognizing both the cost and power of indirect addressing and procedure call. It is judging a program not just for correctness and efficiency but for aesthetics, and a system’s design for simplicity and elegance. 计算思维是递归思维。它是并行处理。它将代码解释为数据，将数据解释为代码。它是量纲分析的推广，是类型检验。它是认识到混叠的优点和危险，或者给某人或某事一个以上的名字。它认识到间接寻址和过程调用的成本和能力。它不仅从正确性和效率来判断一个程序，而且从美观来判断一个程序，从简洁和优雅来判断一个系统的设计。 Computational thinking is using abstraction and decomposition when attacking a large complex task or designing a large complex system. It is separation of concerns. It is choosing an appropriate representation for a problem or modeling the relevant aspects of a problem to make it tractable. It is using invariants to describe a system’s behavior succinctly and declaratively. It is having the confidence we can safely use, modify, and influence a large complex system without understanding its every detail. It is modularizing something in anticipation of multiple users or prefetching and caching in anticipation of future use. 计算思维是在处理大型复杂任务或设计大型复杂系统时使用抽象和分解。这是关注点的分离。它是为问题选择适当的表示形式，或为问题的相关方面建模以使其易于处理。它使用不变量简洁地描述系统的行为。它是我们有信心安全地使用、修改和影响一个大型复杂系统，而不需要了解它的每个细节。它是在多个用户预期的情况下对某些东西进行模块化，或者在可预见的未来使用的情况下对某些东西进行预取和缓存。 Computational thinking is thinking in terms of prevention, protection, and recovery from worst-case scenarios through redundancy, damage containment, and error correction. It is calling gridlock deadlock and contracts interfaces. It is learning to avoid race conditions when synchronizing meetings with one another. 计算思维是从最坏情况中恢复的角度来思考预防、保护并通过通过冗余、容错和纠错来实现的。它采用堵塞、死锁和约定接口的方式。它正在学习在同步相遇时避免竞态条件。 Computational thinking is using heuristic reasoning to discover a solution. It is planning, learning, and scheduling in the presence of uncertainty. It is search, search, and more search, resulting in a list of Web pages, a strategy for winning a game, or a counterexample. Computational thinking is using massive amounts of data to speed up computation. It is making trade-offs between time and space and between processing power and storage capacity 计算思维是利用启发式推理来发现问题的解决方案。它是在不确定的情况下进行计划、学习和调度。它是搜索，搜索，更多的搜索，结果是一个网页列表，一个赢得游戏的策略，或者一个反例。计算思维是利用大量的数据来加速计算。它在时间和空间、处理能力和存储能力之间进行权衡 [1] the three Rs: reading, writing and arithmetic\n","id":46,"section":"posts","summary":"计算思维 It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use. 它代表了一种普遍适用的态度和技能，每个人，不仅仅是计算机科学家，将渴望学习和使用。 Computational thinking builds on","tags":["计算思维"],"title":"Computational-Thinking 中文（part 1）","uri":"https://blog.yoogo.top/2018/12/computational-thinking-i18n/","year":"2018"},{"content":"终于找到了原版的 计算思维，先放个原版的，等有时间了翻译下~（flag）\nPrevious Next \u0026nbsp; \u0026nbsp; Page: / ","id":47,"section":"posts","summary":"终于找到了原版的 计算思维，先放个原版的，等有时间了翻译下~（flag） Previous Next \u0026nbsp; \u0026nbsp; Page: /","tags":["计算思维"],"title":"Computational Thinking","uri":"https://blog.yoogo.top/2018/11/computational-thinking/","year":"2018"},{"content":"\u0026lt;%= f.select :order_status, options_for_select(collection, value), {}, class: 'form-control', multiple: true %\u0026gt; 这里无论怎么选择，都会提交成这种[\u0026quot;\u0026quot;, ........] ，烦了我好久。 就困于我，遂google之：stackoverflow，有一条Answers，告诉我需要看：https://api.rubyonrails.org/classes/ActionView/Helpers/FormOptionsHelper.html#method-i-select。 终于，在gotcha中找到了答案：\nWhen no selection is made for a collection of checkboxes most web browsers will not send any value. If no category_ids are selected then we can safely assume this field will not be updated. This is possible thanks to a hidden field generated by the helper method for every collection of checkboxes. This hidden field is given the same field name as the checkboxes with a blank value. In the rare case you don\u0026rsquo;t want this hidden field, you can pass the include_hidden: false option to the helper method.\n","id":48,"section":"posts","summary":"\u0026lt;%= f.select :order_status, options_for_select(collection, value), {}, class: 'form-control', multiple: true %\u0026gt; 这里无论怎么选择，都会提交成这种[\u0026quot;\u0026quot;, ........] ，烦了我好久。 就困于我，遂google之：stackov","tags":["rails"],"title":"rails 多选的一个小tip","uri":"https://blog.yoogo.top/2018/11/rails-multiple-select/","year":"2018"},{"content":"弄了大半天，可算把写的笔记都搬到hexo来了，对比了一下jekyll和hexo，果然还是hexo好用啊。\n部署到github pages 每次部署都会丢失Custom domain 解决: 在博客的 source 目录下新建一个 CNAME 文件，然后在这个文件中填入你的域名，这样就不会每次发布之后，gitpage 里的 custom domain 都被重置掉啦。\n","id":49,"section":"posts","summary":"弄了大半天，可算把写的笔记都搬到hexo来了，对比了一下jekyll和hexo，果然还是hexo好用啊。 部署到github pages 每次部署都会丢失","tags":["hexo"],"title":"happy hexo","uri":"https://blog.yoogo.top/2018/11/happy-blog/","year":"2018"},{"content":" 顺序（Sequence） 工作流中的各个活动在同一个进程中按顺序依次执行。 例子：在“发送货物”之后“发送单据”。\n平行拆分（Parallel Split） 工作流中从一个线程中的一个点拆分为在多个线程中平行执行的多个活动。 例子：活动“付款”激活了“发送货物”以及“通知顾客”的执行。\n同步（Synchronization） 工作流中的多个活动在一个点上汇合成一个线程。 例子：活动“归档”在“发票”和“收款”全部完成后被激活。\n排他选择（Exclusive Choice） 工作流中的一个点，基于决定或者工作流中的数据，流向若干个分支中的一个。\n单合并（Single Merge） 工作流程中的一个点在两个或者多个分支发生异步的汇合时执行。它假设这些分支中不存在平行执行的情况。 例子：在收到支付或者确认信用之后，汽车被交付给顾客。\n多选（Multi-choice） 工作流中的一个点，基于决定或者工作流中的数据，流向若干个分支中的几个。\n平行合并（Synchronize Merge） 工作流程中的多个路径在一个点被汇合成一个单一的进程。如果多于一个的路径到达了，活动线程就需要进行同步。如果只有一个路径到达，那么其它的路径应该异步的重新会聚。该模式假设在一个分支被激活后，不会在等待其它分支完成的过程中再次被激活。 这种模式的难点在于决定在哪些分支到达后启动活动。\n多合并（Multi-merge） 工作流程中的多个分支在一个点进行异步的汇聚。如果多于一个的分支被激活——可能是并发的——每个进入分支的每个活动都在合并之后启动新的活动。\n鉴别器（Discriminator） 鉴别器是工作流中的一个点，它等待进入分支中的一个完成，然后才激活其后的活动序列。从那个时刻开始，它等待所有剩余的分支完成，并且“忽略”它们。一旦所有的分支都已被触发，它就会重置自己，以便能被再次触发。 例子：在三个分支中的任意两个到达之后启动后面的活动。\nM中的N模式（N-out-of-M Join） 合并多条路径，进行部分同步，只执行一次后续活动\n强制循环（Arbitrary Cycles） 工作流中的一个点可以让一个或多个活动反复的执行。\n隐式终止（Implicit Termination） 一个子过程应该在没有什么事情可做的时候被停下来。换句话说，在工作流中没有别的活动在执行并且没有活动可被激活的时候终止。\n异步的多实例（Multiple Instances Without Synchronization） 在一个工作流的内部中，可以创建一个活动的多个实例，当然，这需要对新产生的线程的控制能力。\n在设计期间预先确定的多实例（Multiple Instances With a Priori Design Time Knowledge） 在特定过程中的特定活动的数量是在设计时就被确定的。一旦所有的实例都完成了，其它的活动应该被启动。 例子：有关危险品的请求需要三个不同的审核。\n在运行期预先确定的多实例（Multiple Instances With a Priori Runtime Knowledge） 在一个活动能够被多次激活的这种情况下，在指定情况下的指定活动的实例数量可能取决于情况的特性或者资源的可用性。但是，在活动被创建之前，在运行中的某个阶段，这个数量是可以预知的。一旦所有的实例都完成了，其它的活动应该被启动。 例子：在预定旅行的过程中，如果旅途包含多个飞行的话，活动“预定机票”会被多次执行。如果所有预定都完成了，发票会被发送给客户。\n无法在运行期预先确定的多实例（Multiple Instances With no Priori Runtime Knowledge） 在一个活动能够被多次激活的这种情况下，在指定情况下的指定活动的实例数量无论是在设计时或者运行时都不能在活动的实例被创建之前预先确定。但是，在活动被创建之前，在运行中的某个阶段，这个数量是可以预知的。一旦所有的实例都完成了，其它的活动应该被启动。这个模式和模式14的区别在于，在某些实例运行结束之后，新的实例仍能被创建。\n延迟选择（Deferred Choice） 工作流中的一个点，有一个或多个分支已经被选择。与XOR拆分相比，并没有明确的选择，但是，选择是取决于环境的。与AND拆分相比，两者中只有一个被执行。这意味着一旦环境启动了其中的一个，另一个就被取消。要注意，选择是被延迟到两个分支中的一个真正开始执行时，也就是说，选择是可以尽可能的推后的。 例子：在收到货物之后，有两种方法可以将其送到。选择取决于相关资源的可用性。因此，选择会被推迟到直到其中一个资源可用为止。\n交替平行路由（Interleaved Parallel Routing） 一组活动以任意的顺序执行，每个活动都被执行，他们的顺序是在运行时决定的，并且在任意一个时刻都不会有两个活动在执行。\n里程碑（Milestone） 一个活动能否执行取决于一个指定的状态。也就是说，只有在到达一个特定的未过期的里程碑时，活动才被执行。 例子：一个顾客只有在进行六个月以上的飞行之后才能声明自己的飞行里程。\n取消活动（Cancel Activity） 一个可执行的活动被强制失效了，也就是说，一个正在等待执行的活动所在线程被移除了。 例子：如果顾客取消了一个请求，那么与之相关的活动也被取消了。\n取消实例（Cancel Case） 一个活动的实例被完全消除了。\n","id":50,"section":"posts","summary":"顺序（Sequence） 工作流中的各个活动在同一个进程中按顺序依次执行。 例子：在“发送货物”之后“发送单据”。 平行拆分（Parallel Sp","tags":["工作流"],"title":"21种工作流模式","uri":"https://blog.yoogo.top/2018/11/21%E7%A7%8D%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%A8%A1%E5%BC%8F/","year":"2018"},{"content":"插件 atom-ctags： 代码提示及补全 atom-runner： 可以在atom中运行脚本文件，支持多种脚本语言 dash：将dash整合至atom editorconfig: 维持代码风格 emmit：提高HTML/CSS编写速度 file-icons: 文件图标不再单调 git-plus: 可以在atom中完成git的所有操作 goto-definition 跳转到定义 highlight-selected 双击单词，代码高亮 minimap-highlight-selected 对highlight-selected封装，向sublime一样具有源码导航 minimap-plus 源码预览 rails-snippets 在项目中加入自定义rails片段 better-git-blame 在atom中更好地显示 git-blame intentions 显示意图 formatter 格式化 markdown-preview-plus makedown插件 修改registry到淘宝npm镜像 apm config set registry https://registry.npm.taobao.org 如果需要删除该镜像设置，使用：\napm config delete registry ","id":51,"section":"posts","summary":"插件 atom-ctags： 代码提示及补全 atom-runner： 可以在atom中运行脚本文件，支持多种脚本语言 dash：将dash整合至at","tags":["editor"],"title":"Atom插件收集","uri":"https://blog.yoogo.top/2018/11/atom%E6%8F%92%E4%BB%B6%E6%94%B6%E9%9B%86/","year":"2018"},{"content":" BIOS（Basic Input/Output System的缩写、中文：基本输入输出系统）, 在IBM PC兼容系统上，是一种业界标准的固件接口。[1]。BIOS这个字眼是在1975年第一次由CP/M操作系统中出现。BIOS是个人计算机启动时加载的第一个软件。\nBIOS用于计算机引导时运行系统各部分的自我检测（Power On Self Test），并加载引导程序（IPL）或存储在主存的操作系统。此外，BIOS还向操作系统提供一些系统参数。系统硬件的变化是由BIOS隐藏，程序使用BIOS功能而不是直接控制硬件。现代操作系统会忽略BIOS提供的抽象层并直接控制硬件组件。\n启动计算机原理 当计算机的电源打开，BIOS就会由主板上的闪存（flash memory）运行，并将芯片组和存储器子系统初始化。BIOS会把自己从闪存中，解压缩到系统的主存；并且从那边开始运行。PC的BIOS代码也包含诊断功能，以保证某些重要硬件组件，像是键盘、磁盘、输出输入端口等等，可以正常运作且正确地初始化。几乎所有的BIOS都可以选择性地运行CMOS存储器的设置程序；也就是保存BIOS会访问的用户自定义设置数据（时间、日期、硬盘细节，等等）。IBM技术参考手册中曾经包含早期PC和AT BIOS的80x86源代码。\n现代的BIOS可以让用户选择由哪个设备启动计算机，如光盘驱动器、硬盘、软盘、USB U盘等等。这项功能对于安装操作系统、以LiveCD启动计算机、以及改变计算机找寻引导媒体的顺序特别有用。\n有些BIOS系统允许用户可以选择要加载哪个操作系统（例如从第二颗硬盘加载其他操作系统），虽然这项功能通常是由第二阶段的引导程序（boot loader）来处理。\n","id":52,"section":"posts","summary":"BIOS（Basic Input/Output System的缩写、中文：基本输入输出系统）, 在IBM PC兼容系统上，是一种业界标准的固件接口。[1]。BIOS这个字","tags":["os"],"title":"blos","uri":"https://blog.yoogo.top/2018/11/bios/","year":"2018"},{"content":"结构 物理结构 硬盘的物理结构一般由磁头与碟片、电动机、主控芯片与排线等部件组成；当主电动机带动碟片旋转时，副电动机带动一组（磁头）到相对应的碟片上并确定读取正面还是反面的碟面，磁头悬浮在碟面上画出一个与碟片同心的圆形轨道（磁轨或称柱面），这时由磁头的磁感线圈感应碟面上的磁性与使用硬盘厂商指定的读取时间或数据间隔定位扇区，从而得到该扇区的数据内容；\n磁道 当磁盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道（Track）。\n柱面 在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面（Cylinder）。\n扇区 磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区（Sector）。硬盘的第一个扇区，叫做引导扇区。\n逻辑结构 操作系统对硬盘进行读写时需要用到文件系统把硬盘的扇区组合成簇，并创建文件和树形目录制度，使操作系统对其访问和查找变得容易，这是因为操作系统直接对数目众多的扇区进行寻址会十分麻烦。\nMBR和GPT 主引导记录（Master Boot Record，缩写：MBR），又叫做主引导扇区，是计算机引导后访问硬盘时所必须要读取的首个扇区，主引导扇区记录着硬盘本身的相关消息以及硬盘各个分区的大小及位置消息，是数据消息的重要入口。如果它受到破坏，硬盘上的基本数据结构消息将会丢失，需要用繁琐的方式试探性的重建数据结构消息后才可能重新访问原先的数据，对于那些扇区为512位组的磁盘，MBR分区表不支持容量大于2.2TB（2.2×1012字节）的分区，[5]。\n全局唯一标识分区表（GUID Partition Table，缩写：GPT）是一个实体硬盘的分区表的结构布局的标准。它是可扩展固件接口（EFI）标准（被Intel用于替代个人计算机的BIOS）的一部分。GPT分配64bits给逻辑块地址，因而使得最大分区大小在264-1个扇区成为了可能。对于每个扇区大小为512字节的磁盘，那意味着可以有9.4ZB（9.4 x 1021字节）[5][6]或8 ZiB-512字节（9,444,732,965,739,290,426,880字节或 18,446,744,073,709,551,615（264-1）个扇区x 512（29）字节每扇区）。\n","id":53,"section":"posts","summary":"结构 物理结构 硬盘的物理结构一般由磁头与碟片、电动机、主控芯片与排线等部件组成；当主电动机带动碟片旋转时，副电动机带动一组（磁头）到相对应的碟","tags":["os"],"title":"disk","uri":"https://blog.yoogo.top/2018/11/disk/","year":"2018"},{"content":"emoji 指南 emoji emoji代码 commit说明 🎨 (调色板) 🎨 改进代码结构/代码格式 ⚡️ (闪电) ⚡ 提升性能 🐎 (赛马) 🐎 提升性能 🔥 (火焰) 🔥 移除代码或文件 🐛 (bug) 🐛 修复 bug 🚑 (急救车) 🚑 重要补丁 ✨ (火花) ✨ 引入新功能 📝 (铅笔) ✏ 撰写文档 🚀 (火箭) 🚀 部署功能 💄 (口红) 💄 更新 UI 和样式文件 🎉 (庆祝) 🎉 初次提交 ✅ (白色复选框) ✅ 增加测试 🔒 (锁) 🔒 修复安全问题 🍎 (苹果) 🍎 修复 macOS 下的问题 🐧 (企鹅) 🐧 修复 Linux 下的问题 🏁 (旗帜) :checked_flag: 修复 Windows 下的问题 🔖 (书签) 🔖 发行/版本标签 🚨 (警车灯) 🚨 移除 linter 警告 🚧 (施工) 🚧 工作进行中 💚 (绿心) 💚 修复 CI 构建问题 ⬇️ (下降箭头) ⬇️ 降级依赖 ⬆️ (上升箭头) ⬆️ 升级依赖 👷 (工人) 👷‍♂️ 添加 CI 构建系统 📈 (上升趋势图) 📈 添加分析或跟踪代码 🔨 (锤子) 🔨 重大重构 ➖ (减号) ➖ 减少一个依赖 🐳 (鲸鱼) 🐳 Docker 相关工作 ➕ (加号) ➕ 增加一个依赖 🔧 (扳手) 🔧 修改配置文件 🌐 (地球) 🌐 国际化与本地化 ✏️ (铅笔) ✏️ 修复 typo ","id":54,"section":"posts","summary":"emoji 指南 emoji emoji代码 commit说明 🎨 (调色板) 🎨 改进代码结构/代码格式 ⚡️ (闪电) ⚡ 提升性能 🐎 (赛马) 🐎 提升性能 🔥 (火焰) 🔥 移除代码或文","tags":["git"],"title":"emoji 指南","uri":"https://blog.yoogo.top/2018/11/git-commit-emoji-%E6%8C%87%E5%8D%97/","year":"2018"},{"content":"查看分支：git branch\n创建分支：git branch \u0026lt;name\u0026gt;\n切换分支：git checkout \u0026lt;name\u0026gt;\n创建+切换分支：git checkout -b \u0026lt;name\u0026gt;\n合并某分支到当前分支：git merge \u0026lt;name\u0026gt;\n删除分支：git branch -d \u0026lt;name\u0026gt;\n将已经tracked的文件追加到上一次commit：git commit -a --amend 等于\ngit rebase -i HEAD~2 将自己的分支和其他分支进行对比: git diff branch1 branch 对比暂存区和当前的 HEAD: git diff --cached\n","id":55,"section":"posts","summary":"查看分支：git branch 创建分支：git branch \u0026lt;name\u0026gt; 切换分支：git checkout \u0026lt;name\u0026gt; 创建+切换分支：git checkout -b \u0026lt;name\u0026gt; 合并某分支到当前分支：git merge \u0026lt;name\u0026gt; 删除分支：git branch -d \u0026lt;name\u0026gt;","tags":["git"],"title":"git常用命令","uri":"https://blog.yoogo.top/2018/11/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","year":"2018"},{"content":"git reflog 当你提交或修改分支时，reflog 就会更新。 任何时间运行 git reflog 命令可以查看当前的状态 当你的commit是追加到上一次的commit，例如 git commit -a --amend ，这时，意识到错误了，想要撤销这一次的commit，且不伤害原有的commit, 可以有如下操作：\ngit reflog #查看自己的commit，pull,rebase,reset,checkout 等操作 git reset HEAD@{?} #从reflog拿出你想还原到什么位置 git cat-file -t xxx xxx是随便的字符串，cat-file -t 可以判断xxx的类型 cat-file -p 查看xxx内容 git branch -av git branch --set-upstream-to=aaaaa/master vvvvv 给 aaaaa remote设置默认分支为 vvvvv\ngit repo同步 git clone 虽然会把所有git库拉下来，但是并不会在本地库创建remote的分支\n为了解决这个问题，因此有如下方案\n普通clone，拉下来后脚本创建branch： git clone xxx.git cd xxx for branch in `git branch -r|grep -v ' -\u0026gt; '|cut -d\u0026quot;/\u0026quot; -f2`; do git checkout $branch; git fetch; done; 镜像方式clone，拉完后手动还原git repo mkdir my_repo_folder \u0026amp;\u0026amp; cd my_repo_folder git clone --mirror xxx.git .git git config --bool core.bare false git reset --hard ","id":56,"section":"posts","summary":"git reflog 当你提交或修改分支时，reflog 就会更新。 任何时间运行 git reflog 命令可以查看当前的状态 当你的commit是追加到上一次的commit，例如 git","tags":["git"],"title":"git有用的操作","uri":"https://blog.yoogo.top/2018/11/reflog/","year":"2018"},{"content":"在Golang里，import的作用是导入其他package，但是今天在看beego框架时看到了import 下划线，不知其意，故百度而解之。 import 下划线（如：import _ hello/imp）的作用：当导入一个包时，该包下的文件里所有init()函数都会被执行，然而，有些时候我们并不需要把整个包都导入进来，仅仅是是希望它执行init()函数而已。这个时候就可以使用 import _ 引用该包。即使用【import _ 包路径】只是引用该包，仅仅是为了调用init()函数，所以无法通过包名来调用包中的其他函数。\n","id":57,"section":"posts","summary":"在Golang里，import的作用是导入其他package，但是今天在看beego框架时看到了import 下划线，不知其意，故百度而解之。","tags":["go"],"title":"go-import 中下划线的作用","uri":"https://blog.yoogo.top/2018/11/go-import-%E4%B8%AD%E4%B8%8B%E5%88%92%E7%BA%BF%E7%9A%84%E4%BD%9C%E7%94%A8/","year":"2018"},{"content":"break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var var和const参考2.2Go语言基础里面的变量和常量申明 package和import已经有过短暂的接触 func 用于定义函数和方法 return 用于从函数返回 defer 用于类似析构函数 go 用于并发 select 用于选择不同类型的通讯 interface 用于定义接口，参考2.6小节 struct 用于定义抽象数据类型，参考2.5小节 break、case、continue、for、fallthrough、else、if、switch、goto、default这些参考2.3流程介绍里面 chan用于channel通讯 type用于声明自定义类型 map用于声明map类型数据 range用于读取slice、map、channel数据 ","id":58,"section":"posts","summary":"break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var var和const参考2.2Go语言基础里面的变量和常量申明 package和import已","tags":["go"],"title":"Go语言的一些语法","uri":"https://blog.yoogo.top/2018/11/go%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%AD%E6%B3%95/","year":"2018"},{"content":"namespace namespace是一种隔离机制，一个独立的namespace看上去拥有所有linux主机的资源，也拥有自己的0号进程（即系统初始化的进程）。一个namespace可以产生多个子namespace，通过设置clone系统调用的flag可以实现。事实上namespace是为了支持linux container（即linux容器）出现的，运用kernel中的namespace机制和cgroup机制（kernel的配额管理机制）可以实现轻量级的虚拟，即多个虚拟主机（容器）公用宿主机的kernel，彼此之间资源隔离。docker的部分技术也依赖于此。\nopt CPU占用最多的前10个进程： ps auxw|head -1;ps auxw|sort -rn -k3|head -10 内存消耗最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k4|head -10 虚拟内存使用最多的前10个进程\nps auxw|head -1;ps auxw|sort -rn -k5|head -10\n命令行关闭显示器 setterm --blank 1\n","id":59,"section":"posts","summary":"namespace namespace是一种隔离机制，一个独立的namespace看上去拥有所有linux主机的资源，也拥有自己的0号进程（即系统初始化的进程","tags":["linux"],"title":"Linux","uri":"https://blog.yoogo.top/2018/11/linux/","year":"2018"},{"content":"画普通矩阵，不带括号的\n\\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{matrix} 画带中括号的矩阵\n\\left[ \\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{matrix} \\right] 画带大括号的矩阵\n\\left\\{ \\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{matrix} \\right\\} 矩阵前加个参数\nA= \\left\\{ \\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{matrix} \\right\\} 矩阵中间有省略号\n\\cdots为水平方向的省略号 \\vdots为竖直方向的省略号 \\ddots为斜线方向的省略号 A= \\left\\{ \\begin{matrix} a \u0026amp; b \u0026amp; \\cdots \u0026amp; e\\\\ f \u0026amp; g \u0026amp; \\cdots \u0026amp; j \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ p \u0026amp; q \u0026amp; \\cdots \u0026amp; t \\end{matrix} \\right\\} 矩阵中间加根横线\narray必须为array {cccc|c}中的c表示矩阵元素，可以控制|的位置 A= \\left\\{ \\begin{array}{cccc|c} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{array} \\right\\} ","id":60,"section":"posts","summary":"画普通矩阵，不带括号的 \\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\ f \u0026amp; g \u0026amp; h \u0026amp; i \u0026amp; j \\\\ k \u0026amp; l \u0026amp; m \u0026amp; n \u0026amp; o \\\\ p \u0026amp; q \u0026amp; r \u0026amp; s \u0026amp; t \\end{matrix} 画带中括号的矩阵 \\left[ \\begin{matrix} a \u0026amp; b \u0026amp; c \u0026amp; d \u0026amp; e\\\\","tags":["markdown"],"title":"Markdown 画矩阵","uri":"https://blog.yoogo.top/2018/11/markdown-%E7%94%BB%E7%9F%A9%E9%98%B5/","year":"2018"},{"content":"在uploader中使用Minimagick对图片进行处理 include CarrierWave::MiniMagick\nclass InsurancePolicyUploader \u0026lt; BaseUploader process :watermark def store_dir \u0026quot;uploads/insurance_policy/#{model.id}\u0026quot; end def watermark manipulate! do |img| img.combine_options do |i| i.fill 'rgba(0,0,0,0.4)' # i.font \u0026quot;WenQuanYi-Zen-Hei\u0026quot; i.gravity \u0026quot;center\u0026quot; i.pointsize \u0026quot;150\u0026quot; i.draw \u0026quot;rotate 57 text 0,0 '仅限处理车辆事故及违章办理'\u0026quot; end img end end end 根据代码进行总结： rgba可以快速的将颜色透明化 ImageMagick本身并不支持中文的处理 convert -list font 查看i.font支持的字体 存在问题： 水印字体大小是死的，不能做到自适应图片大小来添加水印 注：一个令我恍然大悟的网站：rubblewebs\n","id":61,"section":"posts","summary":"在uploader中使用Minimagick对图片进行处理 include CarrierWave::MiniMagick class InsurancePolicyUploader \u0026lt; BaseUploader process :watermark def store_dir \u0026quot;uploads/insurance_policy/#{model.id}\u0026quot; end def watermark manipulate! do |img| img.combine_options do |i| i.fill 'rgba(0,0,0,0.4)' # i.font \u0026quot;WenQuanYi-Zen-Hei\u0026quot; i.gravity \u0026quot;center\u0026quot; i.pointsize \u0026quot;150\u0026quot; i.draw \u0026quot;rotate 57 text 0,0 '仅限处理车辆","tags":["rails"],"title":"minimagick水印","uri":"https://blog.yoogo.top/2018/11/minimagick/","year":"2018"},{"content":" py环境 apt-get install python-pip pip install --upgrade pip pip install setuptools pip install shadowsocks 配置文件 vim /etc/shadowsocks.json\n{ \u0026quot;server\u0026quot;:\u0026quot;0.0.0.0\u0026quot;, \u0026quot;server_port\u0026quot;:1024, \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;local_port\u0026quot;:1080, \u0026quot;password\u0026quot;:\u0026quot;mypassword\u0026quot;, \u0026quot;timeout\u0026quot;:300, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; } 后台运行/停止shadowsocks ssserver -c /etc/shadowsocks.json -d start/stop\n开机启动 vim /etc/rc.local\n在exit 0前面加上ss的启动命令 add_index \u0026ldquo;accidents\u0026rdquo;, [\u0026ldquo;handle_status\u0026rdquo;], :name =\u0026gt; \u0026ldquo;index_accidents_on_handle_status\u0026rdquo; 20180918083739\n","id":62,"section":"posts","summary":"py环境 apt-get install python-pip pip install --upgrade pip pip install setuptools pip install shadowsocks 配置文件 vim /etc/shadowsocks.json { \u0026quot;server\u0026quot;:\u0026quot;0.0.0.0\u0026quot;, \u0026quot;server_port\u0026quot;:1024, \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;local_port\u0026quot;:1080, \u0026quot;password\u0026quot;:\u0026quot;mypassword\u0026quot;, \u0026quot;timeout\u0026quot;:300, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; } 后台运行/停止shadowsocks ssserver -c /etc/shadowsocks.json -d start/stop 开机启动 vim /etc/rc.local 在exit 0前面","tags":["greatwall"],"title":"ss","uri":"https://blog.yoogo.top/2018/11/ss/","year":"2018"},{"content":"新增 submodule git submodule add git://github.com/majutsushi/tagbar.git .vim/bundle/tagbar 如果這個 repo 先前沒有用過 submodule 那麼 Git 會在目錄下建立一個叫做 .gitmodules 的目錄，這裡記錄了 remote repo 的 URL 和這個 submodule 在此專案的路徑。\n執行此命令後 submodule 和 .gitmodules 會自動 staged，這個時候可以 commit 和 push。\n更新 submodule 個別 repo 更新比較麻煩，必須到個別的目錄底下執行 git pull 去拉 upstream 的程式碼，可是這樣會比較安全；若要一次全部更新所有的 submodule 可以用這 foreach 指令：\ngit submodule foreach --recursive git pull origin master 刪除 submodule 本以為會有像是 git submodule rm 這樣的指令，結果竟然沒有，必須辛苦地一個一個手動移除，不知道不實作這個指令的考量是什麼，希望未來的版本能把它加上去。\n移除 submodule 有以下幾個步驟要做，先把 submodule 目錄從版本控制系統移除：\ngit rm --cached /path/to/files rm -rf /path/to/files 再來是修改 .gitmodules，把不用的 submodule 刪掉，例如：\n[submodule \u0026quot;.vim/bundle/vim-gitgutter\u0026quot;] path = .vim/bundle/vim-gitgutter url = git://github.com/airblade/vim-gitgutter.git -[submodule \u0026quot;.vim/bundle/vim-autoclose\u0026quot;] - path = .vim/bundle/vim-autoclose - url = git://github.com/Townk/vim-autoclose.git 還沒完喔！還要修改 .git/config 的內容，跟 .gitmodules 一樣，把需要移除的 submodule 刪掉，最後再 commit。\nclone 時把 submodule 一起抓下來 執行 git clone 時 Git 不會自動把 submodule 一起 clone 過來，必須加上 \u0026ndash;recursive 遞歸參數，這樣可以連帶 submodule 的 submodule 通通一起抓下來：\ngit clone \u0026ndash;recursive git@github.com:chinghanho/.dotfiles.git 如果已經抓下來才發現 submodule 是空的，可以用以下指令去抓，init 會在 _.git/config` 下註冊 remote repo 的 URL 和 local path：\ngit submodule init git submodule update --recursive 或是合併成一行 git submodule update --init --recursive 也可以，如果 upstream 有人改過 .gitmodules，那 local 端好像也是用這個方法 update。\n指令解釋 git submodule init：根據 .gitmodules 的名稱和 URL，將這些資訊註冊到 .git/config 內，可是把 .gitmodules 內不用的 submodule 移除，使用這個指令並沒辦法自動刪除 .git/config 的相關內容，必須手動刪除； git submodule update：根據已註冊（也就是 .git/config ）的 submodule 進行更新，例如 clone 遺失的 submodule，也就是上一段講的方法，所以執行這個指令前最好加上 \u0026ndash;init； git submodule sync：如果 submodule 的 remote URL 有變動，可以在 .gitmodules 修正 URL，然後執行這個指令，便會將 submodule 的 remote URL 更正。 ","id":63,"section":"posts","summary":"新增 submodule git submodule add git://github.com/majutsushi/tagbar.git .vim/bundle/tagbar 如果這個 repo 先前沒有用過 submodule 那麼 Git 會在目錄下建立一個叫做 .gitmodules 的目錄，這裡記錄了 remote repo 的 URL 和這個 submodule 在此專案的路徑。 執行此命令後 submodule 和 .gitmodules 會","tags":["git"],"title":"submodule","uri":"https://blog.yoogo.top/2018/11/submodule/","year":"2018"},{"content":"暴力关机损坏了 Ubuntu的图形系统配置，导致图形界面无法正常起来。所以就看到能够登录，却只有一片蓝色。\n先进入字符界面：Ctrl + Alt + F4\n然后安装相应服务，然后重置它！\nsudo apt-get install xserver-xorg-lts-utopic sudo dpkg-reconfigure xserver-xorg-lts-utopic reboot ","id":64,"section":"posts","summary":"暴力关机损坏了 Ubuntu的图形系统配置，导致图形界面无法正常起来。所以就看到能够登录，却只有一片蓝色。 先进入字符界面：Ctrl + Alt + F4 然后","tags":["linux"],"title":"Ubuntu绿屏怎么办","uri":"https://blog.yoogo.top/2018/11/ubuntu%E7%BB%BF%E5%B1%8F%E6%80%8E%E4%B9%88%E5%8A%9E/","year":"2018"},{"content":" 节点是红色或黑色。\n根节点是黑色。\n每个叶子节点都是黑色的空节点（NIL节点）。\n每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)\n从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。\n","id":65,"section":"posts","summary":"节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点（NIL节点）。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不","tags":["数据结构"],"title":"红黑树","uri":"https://blog.yoogo.top/2018/11/%E7%BA%A2%E9%BB%91%E6%A0%91/","year":"2018"},{"content":"集中式版本控制系统，版本库是集中存放在中央服务器的，而干活的时候，用的都是自己的电脑，所以要先从中央服务器取得最新的版本，然后开始干活，干完活了，再把自己的活推送给中央服务器。中央服务器就好比是一个图书馆，你要改一本书，必须先从图书馆借出来，然后回到家自己改，改完了，再放回图书馆。\n分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件A，你的同事也在他的电脑上改了文件A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。\n","id":66,"section":"posts","summary":"集中式版本控制系统，版本库是集中存放在中央服务器的，而干活的时候，用的都是自己的电脑，所以要先从中央服务器取得最新的版本，然后开始干活，干完","tags":["git"],"title":"集中式vs分布式","uri":"https://blog.yoogo.top/2018/11/%E9%9B%86%E4%B8%AD%E5%BC%8Fvs%E5%88%86%E5%B8%83%E5%BC%8F/","year":"2018"},{"content":"配置rails new 默认选项 使用 rails new --help 查看: Using --skip-bundle --skip-test -d postgresql from /Users/rocky/.railsrc Usage: rails new APP_PATH [options] Options: ... Description: The 'rails new' command creates a new Rails application with a default directory structure and configuration at the path you specify. You can specify extra command-line arguments to be used every time 'rails new' runs in the .railsrc configuration file in your home directory. Note that the arguments specified in the .railsrc file don't affect the defaults values shown above in this help message. Example: rails new ~/Code/Ruby/weblog This generates a skeletal Rails installation in ~/Code/Ruby/weblog. 可以发现 Description下有如下描述： You can specify extra command-line arguments to be used every time \u0026lsquo;rails new\u0026rsquo; runs in the .railsrc configuration file in your home directory.\n编辑.railsrc文件(vim ~/.railsrc)，我的配置: --skip bundle --skip test -d postgresql ","id":67,"section":"posts","summary":"配置rails new 默认选项 使用 rails new --help 查看: Using --skip-bundle --skip-test -d postgresql from /Users/rocky/.railsrc Usage: rails new APP_PATH [options] Options: ... Description: The 'rails new' command creates a new Rails application with a default directory structure and configuration at the path you specify. You can specify extra command-line arguments to be used every time 'rails new' runs","tags":["rails"],"title":"配置rails new","uri":"https://blog.yoogo.top/2018/11/%E9%85%8D%E7%BD%AErails-new/","year":"2018"}],"tags":[{"title":"cloudnative","uri":"https://blog.yoogo.top/tags/cloudnative/"},{"title":"coredns","uri":"https://blog.yoogo.top/tags/coredns/"},{"title":"devops","uri":"https://blog.yoogo.top/tags/devops/"},{"title":"dns","uri":"https://blog.yoogo.top/tags/dns/"},{"title":"editor","uri":"https://blog.yoogo.top/tags/editor/"},{"title":"git","uri":"https://blog.yoogo.top/tags/git/"},{"title":"go","uri":"https://blog.yoogo.top/tags/go/"},{"title":"greatwall","uri":"https://blog.yoogo.top/tags/greatwall/"},{"title":"hexo","uri":"https://blog.yoogo.top/tags/hexo/"},{"title":"iam","uri":"https://blog.yoogo.top/tags/iam/"},{"title":"java","uri":"https://blog.yoogo.top/tags/java/"},{"title":"k8s","uri":"https://blog.yoogo.top/tags/k8s/"},{"title":"linux","uri":"https://blog.yoogo.top/tags/linux/"},{"title":"mac","uri":"https://blog.yoogo.top/tags/mac/"},{"title":"markdown","uri":"https://blog.yoogo.top/tags/markdown/"},{"title":"network","uri":"https://blog.yoogo.top/tags/network/"},{"title":"os","uri":"https://blog.yoogo.top/tags/os/"},{"title":"rails","uri":"https://blog.yoogo.top/tags/rails/"},{"title":"体系结构","uri":"https://blog.yoogo.top/tags/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"},{"title":"工作流","uri":"https://blog.yoogo.top/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"title":"数据结构","uri":"https://blog.yoogo.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"正则","uri":"https://blog.yoogo.top/tags/%E6%AD%A3%E5%88%99/"},{"title":"计算思维","uri":"https://blog.yoogo.top/tags/%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4/"},{"title":"计算机网络","uri":"https://blog.yoogo.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]}